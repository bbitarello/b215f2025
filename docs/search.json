[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIOL B215: Biostatistics with R",
    "section": "",
    "text": "Instructor: Bárbara D. Bitarello (bbitarello [at] brynmawr.edu)\nTA: Nicole Cavalieri (ncavalieri [at] brynmawr.edu)\nLecture: Monday & Wednesday 11:40 AM (Park 264)\nLab: Wednesday 1:10 - 4 PM\nRemember to read the syllabus, listen to Snoop Dog.\nLecture notes:\nPiazza (for Qs)\nMoodle\nRstudio (Posit) Cloud\nRemember to read the Course Info, listen to SD.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#downloading-course-materials-using-git",
    "href": "index.html#downloading-course-materials-using-git",
    "title": "BST 260 Introduction to Data Science",
    "section": "Downloading course materials using Git",
    "text": "Downloading course materials using Git\nYou can download the quarto files used to create the course notes using Git. You can update files using git pull but you will not be able to change the course notes on the main repository. This means that if you edit the files and then try to update then using git pull you will encounter conflicts. For this reason recommend that you make a copy before editing files. We have edited the .gitignore file so that if you add the word notes to your filenames, git will not track the files. So we recommend that you before editing you make a copy of the file and notes to the filename. For example 01-unix.qmd to 01-unix-notes.qmd.\nYou can download the files using git clone like this:\n\nOpen a terminal and navigate to the directory you want to keep these notes in.\nType git clone  https://github.com/datasciencelabs/2023.git\n\nor using RStudio like this:\n\nGot to https://github.com/datasciencelabs/2023\nClick on the green “Clone or Download” on Github and copy the link.\nOpen RStudio, and go to File &gt; New Project &gt; Version Control &gt; Git, and paste in the link you just copied. Under “Create Project as Sub-directory of”, browse and select a folder where you want the course materials to go.\nPress “Create Project”. This will create a folder called 2023 in the folder you selected in step 3.\nNow, you can open this project using the projects tab in the upper right of RStudio, or going to File &gt; Open Project and then navigating to the 2023 folder and opening the .Rproj file.\n\nOnce you cloned the course repository and want to get updates, you must use git pull to get updates. You can do this in the terminal or on the RStudio’s Git pane.\n\nAssociating an existing directory\nIf you already cloned the repository outside of RStudio, you can associate the directory that was created in that step with RStudio. In RStudio, go to File &gt; New Project &gt; Existing Directory, and then navigate / click on the 2023 folder. Then click “Create Project”. Then you can follow step 5 above to open the project when you launch RStudio.\n\n\nForking the repository\nAn alternative more advanced way to cloning the directory is creating a fork. Forking a repository on GitHub allows you to create a copy of a project under your own GitHub account. This lets you make changes without affecting the original repository. Here’s how you can fork a repository on GitHub:\n\nLog In to GitHub:\n\nMake sure you are logged in to your GitHub account.\n\nNavigate to the Repository:\n\nGo to the main page of the repository you want to fork: https://github.com/datasciencelabs/2023\n\nClick the ‘Fork’ Button:\n\nIn the top-right corner of the repository’s page, you’ll find the “Fork” button. Click on it.\n\nChoose an Account:\n\nIf you are a member of any organizations, GitHub will ask you where you’d like to fork the repository. Choose your personal account unless you want to fork it to an organization.\n\nWait for the Forking Process to Complete:\n\nGitHub will then create a copy of the repository in your account. You’ll see an animation indicating the process, and once it’s done, you’ll be redirected to the forked repository under your account.\n\nClone Your Forked Repository:\n\nTo work with the forked repository on your local machine, you can clone it. Navigate to the main page of your forked repo, click on the green “Code” button, copy the URL, and then use the following command in your terminal or command prompt:\ngit clone [URL_you_copied]\n\n\nYou can continue to update the forked repository by doing the following:\n\nNavigate to Your Local Repository:\n\nOpen a terminal or command prompt.\nNavigate to the directory where you have your forked repository.\n\nAdd the Original Repository as an Upstream Remote:\n\nUse the following command to add the original repository as an upstream remote:\ngit remote add upstream [URL_of_original_repository]\nFor example, if the original repository’s URL is https://github.com/original-owner/original-repo.git, the command would be:\ngit remote add upstream https://github.com/original-owner/original-repo.git\n\nFetch Changes from the Upstream:\n\nUse the following command to fetch changes from the upstream:\ngit fetch upstream\n\nMerge Changes into Your Local Branch:\n\nFirst, ensure you are on the branch into which you want to merge the upstream changes, typically the main or master branch:\ngit checkout main\nThen, merge the changes from the upstream’s main or master branch:\ngit merge upstream/main\n\nPush Changes to Your Forked Repository on GitHub (if needed):\n\nIf you want these changes to reflect in your GitHub fork, push them:\ngit push origin main\n\n\nNow your fork is synchronized with the original repository. Whenever you want to pull in new changes from the original repository in the future, you just need to repeat steps 3-5.\nTo avoid conflicts you sill want to avoid editing the course notes files and instead make copies."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Course description",
    "section": "",
    "text": "This course introduces UNIX/Linux shell, version control with git and GitHub, R programming, data wrangling with dplyr and data.table, data visualization with ggplot2 and shiny, and reproducible document preparation with RStudio, knitr and markdown. We briefly introduce Monte Carlo simulations, statistical modeling, high-dimensional data techniques, and machine learning and how these are applied to real data. Throughout the course, we use motivating case studies and data analysis problem sets based on challenges similar to those you encounter in scientific research.\nLectures will be mostly live coding. We will go over exercises and challenges together but will pause 1-4 times per lectures so students can complete exercises on their own. The midterm questions will be selected from the exercises presented in class. Some time will be dedicated to answering problem set questions. Lectures will not be recorded.\nStudents are required to have a GitHub account and create a repository for the course.\nProblem sets are mostly composed of open ended questions. Submission should be in the form of a scientific report. Problem set submission need to be completely reproducible. Specifically, students are expected to upload a Quarto document to their GitHub class repository that graders can compile into a readable report."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Schedule\nThe schedule is subject to change.\n\n\n\n\n\nDate\n\n\nModule\n\n\nTopics\n\n\n\n\nMon, Aug 28\n\n\nProductivity Tools\n\n\n\n RStudio, RStudio Projects, Quarto, Unix\n\n\n\n\n\nWed, Aug 30\n\n\nProductivity Tools\n\n\n\nGit and GitHub      \n\n\n\n\n\nMon, Sep 4\n\n\n\nNo class\n\n\n\nLabor day\n\n\n\n\nWed, Sep 6\n\n\n\nR\n\n\n\n\n\nR Basics: The workspace, data types, coercing, lists, packages, namespaces, help, creating vectors, object oriented programming.\n\n\nVectorization: Vector arithmetics, sapply, split, cut, lapply, subsetting, sorting\n\n\n\n\n\n\nMon, Sep 11\n\n\nR \n\n\n\n\nIntroduction to Tidyverse: tidy data, mutate, select, filter, the pipe, summarize, group_by, sorting, and the purrr package\n\n\nDates and time: Date class and the lubridate package\n\n\n\n\n\n\nWed, Sep 13\n\n\nR\n\n\n\n\nImporting data\n\n\nFile types: binary, ascii, unicode\n\n\nLocalesImporting data\n\n\nDownloading files\n\n\n\n\nThe data.table package\n\n\n\n\n\n\nMon, Sep 18\n\n\nData visualization\n\n\n\n\nVisualizing Distributions: Summary statistics, distributions, histograms, smooth densities, the normal distribution, quantiles, percentiles, and boxplots.\n\n\nGrammar of graphics and the basics of the ggplot2 package\n\n\n\n\n\n\nWed, Sep 20\n\n\nData visualization\n\n\n\n\nData visualization principles\n\n\nggplot2 geometries\n\n\n\n\n\n\nFri, Sep 22\n\n\nProblem set 1 due\n\n\n\n\n\n\nMon, Sep 25\n\n\nData wrangling \n\n\n\n\nReshaping data\n\n\nJoining tables\n\n\n\n\n\n\nWed, Sep 27\n\n\nData wrangling \n\n\n\n\nWeb scraping\n\n\nString processing\n\n\nText mining\n\n\n\n\n\n\n\nMon, Oct 2\n\n\n\nProbability\n\n\n\n\nMonte Carlo simulations\n\n\nRandom Variables\n\n\nCentral Limit Theorem\n\n\nProbability case studies: Roulette, Poker, Birthday problem, Monte Hall, insurance\n\n\n\n\n\n\n\nWed, Oct 4\n\n\n\nInference\n\n\n\n\nPolls\n\n\nGuess the proportion of blue beads competition\n\n\nConfidence intervals\n\n\nData-driven models\n\n\n\n\n\n\n\nFri, Oct 6\n\n\n\nFinal project title due\n\n\n\n Submit title and a describe your plans to obtain data\n\n\n\n\n\n\nMon, Oct 9\n\n\n\nNo class\n\n\nIndigenous Peoples’ Day\n\n\n\n\n\nWed, Oct 11\n\n\n\nInference\n\n\n\n\nBayesian statistics\n\n\nHierarchical Models\n\n\nCase study: election forecasting\n\n\n\n\n\n\n\nMon, Oct 16\n\n\n\nMidterm 1\n\n\nIncludes all topics covered by October 11.\n\n\n\n\n\nWed, Oct 18\n\n\n\nLinear Models\n\n\n\n\nRegression and correlation\n\n\nCase study: is height hereditary?\n\n\nBivariate normal distribution, conditional expectations, least squares estimates\n\n\n\n\n\n\n\n\n\nMon, Oct 23\n\n\n\nLinear Models\n\n\n\n\nMultivariable regression\n\n\nCase study: build a baseball team\n\n\n\n\n\n\n\n\n\nWed, Oct 25\n\n\n\nLinear Models\n\n\n\n\nMeasurement error models\n\n\nTreatment effect models \n\n\nCase study: does a high-fat diet increase weight in mice?\n\n\n\n\n\n\n\nMon, Oct 30\n\n\n\nLinear Models\n\n\n\n\nAssociation tests\n\n\nCorrelation is not causation\n\n\n\n\n\n\n\nWed, Nov 1\n\n\n\nHigh dimensional data\n\n\n\n\nMatrices in R \n\n\nCase study: handwritten digits\n\n\n\n\n\n\n\nFri, Nov 3 \n\n\n\nProblem set 2 due\n\n\nOne paragraph description of projects that includes what dataset will be used.\n\n\n\n\n\nMon, Nov 6\n\n\n\nHigh dimensional data\n\n\n\nDimension reduction: Linear algebra, distance, PCA\n\n\n\n\n\n\nWed, Nov 8\n\n\n\nHigh dimensional data\n\n\n\n\nDimension reduction continued\n\n\nCase study: gene expression differences between ethnic groups.\n\n\n\n\n\n\n\nFri, Nov 10 \n\n\n\nProject description due\n\n\nOne paragraph description of projects that includes what dataset will be used.\n\n\n\n\n\nMon, Nov 13\n\n\n\n\nHigh dimensional data\n\n\n\n\n\nRegularization\n\n\nCase study: Recommendations systems in  movie ratings\n\n\n\n\nMatrix factorization\n\n\n\n\n\n\n\nWed, Nov 15\n\n\n\nMachine Learning \n\n\n\n\nIntroduction, definition of concepts, accuracy, test, training and validation sets\n\n\nEvaluation metrics: ROC curves, precision recall curves\n\n\n\n\n\n\n\nMon, Nov 20\n\n\n\nMidterm 2\n\n\nIncludes topics covered until Nov 15.\n\n\n\n\n\nWed, Nov 22\n\n\n\nNo class\n\n\n Thanksgiving\n\n\n\n\n\nMon Nov 27\n\n\n\nMachine Learning\n\n\n\n\nSmoothing\n\n\nCase study: Death rates after natural disasters\n\n\n\n\n\n\n\nWed, Nov 29\n\n\n\nMachine Learning\n\n\n\n\nCross-Validation\n\n\ncaret package\n\n\n\n\n\n\n\nMon, Dec 4\n\n\n\nMachine Learning\n\n\n\n Example of algorithms \n\n\n\nCase study: reading handwritten digits\n\n\n\n\n\n\n\nWed, Dec 6 \n\n\n\nOther topics\n\n\n\nPossible topcis (open to student requests)\n\n\n\nShiny\n\n\nInteractive graphics: plotly\n\n\nAdvanced Quarto\n\n\nResearch topics\n\n\nLarge language models\n\n\nDeep learning\n\n\n\n\n\n\n\nFri Dec 8\n\n\n\nProblem set 3 due\n\n\n\n\n\n\n\nMon, Dec 11\n\n\n\nHelp with project\n\n\n\n \n\n\n\n\n\n\nWed, Dec 13\n\n\n\nHelp with project\n\n\n\n\n\n\n\nWed, Dec 15\n\n\n\nFinal project due"
  },
  {
    "objectID": "01-quarto.html#r-and-rstudio",
    "href": "01-quarto.html#r-and-rstudio",
    "title": "1  Quarto",
    "section": "1.1 R and RStudio",
    "text": "1.1 R and RStudio\nBefore introducing Quarto we need R installed. We highly recommend using RStudio as an IDE for this course. We will be using it in lectures.\n\n1.1.1 Installation\n\nInstall the latest version (4.3.1) of R\nInstall RStudio\n\n\n\n\nrstudio\n\n\n\n\n1.1.2 Basics\nLet’s try a few things together:\n\nOpen a new R script file\nLearn tab complete\nRun commands while editing scripts\nRun the entire script\nMake a plot\nChange options to never save workspace.\n\n\n\n1.1.3 Projects\n\nStart new project in exciting directory.\nStart new project in new directory.\nChange projects."
  },
  {
    "objectID": "01-quarto.html#markdown",
    "href": "01-quarto.html#markdown",
    "title": "1  Quarto",
    "section": "1.2 Markdown",
    "text": "1.2 Markdown\nStart a new Quarto.\n\n1.2.1 Type of editor\n\nSource - See the actual code (WYSIWYG).\nVisual - Partial preview of final document.\n\n\n\n1.2.2 The header\nAt the top you see:\n---\ntitle: \"Untitled\"\n---\nThe things between the --- is the YAML header.\nYou will see it used throughout the Quarto guide.\n\n\n1.2.3 Text formating\nitalics, bold, bold italics\nstrikethrough\ncode\n\n\n1.2.4 Headings\n# Header 1\n## Header 2\n### Header 3\nand so on\n\n\n1.2.5 Links\nJust the link: https://quarto.org/docs/guide/\nLinked text: This is the link to Quarto Guide\n\n\n1.2.6 Images\n\n\n\nFirst week of data science\n\n\nThe image can also be a local file.\n\n\n1.2.7 Lists\nBullets:\n\nbullet 1\n\nsub-bullet 1\nsub-bullet 2\n\nbullet 2\n\nOrdered list\n\nItem 1\nItem 2\n\n\n\n1.2.8 Equations\nInline: \\(Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\)\nDisplay math:\n\\[\n\\mathbf{Y} = \\mathbf{X\\beta} + \\mathbf{\\varepsilon}\n\\]"
  },
  {
    "objectID": "01-quarto.html#computations",
    "href": "01-quarto.html#computations",
    "title": "1  Quarto",
    "section": "1.3 Computations",
    "text": "1.3 Computations\nThe main reason we use Quarto is because we can include code and execute the code when compiling the document. In R we refer to them as R chunks.\nTo add your own R chunks, you can type the characters above quickly with the key binding command-option-I on the Mac and Ctrl-Alt-I on Windows.\nThis applies to plots as well; the plot will be placed in that position. We can write something like this:\n\nx &lt;- 1\ny &lt;- 2\nx + y\n\n[1] 3\n\n\nBy default, the code will show up as well. To avoid having the code show up, you can use an argument, which are annotated with |# To avoid showing code in the final document, you can use the argument echo: FALSE. For example:\n\n\n[1] 3\n\n\nWe recommend getting into the habit of adding a label to the R code chunks. This will be very useful when debugging, among other situations. You do this by adding a descriptive word like this:\n\nx &lt;- 1\ny &lt;- 2\nx + y\n\n[1] 3\n\n\n\n1.3.1 Academic reports\nQuarto has many nice features that facilitates publishing academic reports in this guide\n\n\n1.3.2 Global execution options\nIf you want to apply an option globally, you can include in the header, under execute. For example adding the following line to the header make code not show up, by default:\nexecute:\n  echo: false\n\n\n1.3.3 More on markdown\nThere is a lot more you can do with R markdown. We highly recommend you continue learning as you gain more experience writing reports in R. There are many free resources on the internet including:\n\nRStudio’s tutorial: https://quarto.org/docs/get-started/hello/rstudio.html\nThe knitR book: https://yihui.name/knitr/\nPandoc’s Markdown in-depth documentation"
  },
  {
    "objectID": "01-quarto.html#sec-knitr",
    "href": "01-quarto.html#sec-knitr",
    "title": "1  Quarto",
    "section": "1.4 knitR",
    "text": "1.4 knitR\nWe use the knitR package to compile Quarto. The specific function used to compile is the knit function, which takes a file name as input. RStudio provides the Render button that makes it easier to compile the document.\nNote that the first time you click on the Render button, a dialog box may appear asking you to install packages you need. Once you have installed the packages, clicking Render will compile your Quarto file and the resulting document will pop up.\nThis particular example produces an html document which you can see in your working directory. To view it, open a terminal and list the files. You can open the file in a browser and use this to present your analysis. You can also produce a PDF or Microsoft document by changing:\nformat: html to format: pdf or format: docx. We can also produce documents that render on GitHub using format: gfm, which stands for GitHub flavored markdown, a convenient way to share your reports."
  },
  {
    "objectID": "01-quarto.html#exercises",
    "href": "01-quarto.html#exercises",
    "title": "1  Quarto",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\nWrite a Quarto document that defines variables \\(a=1, b=-1, c=-2\\) and print out the solutions to \\(f(x) = ax^2+bx+c=0\\). Do not report complex solutions, only real numbers.\nInclude a graph of \\(f(x)\\) versus \\(x\\) for \\(x \\in (-5,5)\\).\n\nThis is how you make a plot of a quadratic function:\n\na &lt;- 1 \nb &lt;- -1\nc &lt;- -2\nx &lt;- seq(-5, 5, length = 300)\nplot(x, a*x^2 + b*x + c, type = \"l\")\nabline(h = 0, lty = 2)\n\n\n\n\n\nGenerate a PDF report using knitr. Do not show the R code, only the solutions and explanations of what the reader is seeing.\nErase the PDF report and reproduce it but this time using \\(a=1, b=2, c=5\\).\nErase the PDF report and reproduce it but this time using \\(a=1, b=3, c=2\\).\nCreate an HTML page with the results for this last set of values, but this time showing the code."
  },
  {
    "objectID": "02-unix.html#naming-convention",
    "href": "02-unix.html#naming-convention",
    "title": "2  Unix",
    "section": "2.1 Naming convention",
    "text": "2.1 Naming convention\nIn general you want to name your files in a way that is related to their contents and specifies how they relate to other files. The Smithsonian Data Management Best Practices has “five precepts of file naming and organization” and they are:\n\n\n\nHave a distinctive, human-readable name that gives an indication of the content.\nFollow a consistent pattern that is machine-friendly.\nOrganize files into directories (when necessary) that follow a consistent pattern.\nAvoid repetition of semantic elements among file and directory names.\nHave a file extension that matches the file format (no changing extensions!)\n\n\n\nFor specific recommendations we highly recommend you follow The Tidyverse Style Guide1."
  },
  {
    "objectID": "02-unix.html#the-terminal",
    "href": "02-unix.html#the-terminal",
    "title": "2  Unix",
    "section": "2.2 The terminal",
    "text": "2.2 The terminal\n\necho \"Hello world\"\n\nHello world"
  },
  {
    "objectID": "02-unix.html#sec-filesystem",
    "href": "02-unix.html#sec-filesystem",
    "title": "2  Unix",
    "section": "2.3 The filesystem",
    "text": "2.3 The filesystem\n\n2.3.1 Directories and subdirectories\n\n\n\nfilesystem\n\n\n\n\n2.3.2 The home directory\n\n\n\n\n\n\nHome directory in Windows\n\n\n\n\n\n\n\nHome directory in MacOS\n\n\n\n\n\nThe structure on Windows looks something like this:\n\nAnd on MacOS something like this:"
  },
  {
    "objectID": "02-unix.html#working-directory",
    "href": "02-unix.html#working-directory",
    "title": "2  Unix",
    "section": "2.4 Working directory",
    "text": "2.4 Working directory\nThe working directory is the directly you are currently in. Later we will see that we can move to other directories using the command line. It’s similar to clicking on folders.\nYou can see your working directory like this:\n\npwd\n\n/Users/rafa/Documents/teaching/bst260/2023\n\n\nIn R we can use\n\ngetwd()\n\n[1] \"/Users/rafa/Documents/teaching/bst260/2023\""
  },
  {
    "objectID": "02-unix.html#sec-paths",
    "href": "02-unix.html#sec-paths",
    "title": "2  Unix",
    "section": "2.5 Paths",
    "text": "2.5 Paths\nThis string returned in previous command is full path to working directory.\nThe full path to your home directory is stored in an environment variable, discussed in more detail later:\n\necho $HOME\n\n/Users/rafa\n\n\nIn Unix, we use the shorthand ~ as a nickname for your home directory\nExample: the full path for docs (in image above) can be written like this ~/docs.\nMost terminals will show the path to your working directory right on the command line.\nExercise: Open a terminal window and see if the working directory is listed."
  },
  {
    "objectID": "02-unix.html#unix-commands",
    "href": "02-unix.html#unix-commands",
    "title": "2  Unix",
    "section": "2.6 Unix commands",
    "text": "2.6 Unix commands\n\n2.6.1 ls: Listing directory content\n\n\nls\n\n\n\n2.6.2 mkdir and rmdir: make and remove a directory\n\nmkdir projects\n\nIf you do this correctly, nothing will happen: no news is good news. If the directory already exists, you will get an error message and the existing directory will remain untouched.\nTo confirm that you created these directories, you can list the directories:\n\nls\n\nYou should see the directories we just created listed.\n\nmkdir docs teaching\n\nIf you made a mistake and need to remove the directory, you can use the command rmdir to remove it.\n\nmkdir junk\nrmdir junk\n\n\n\n2.6.3 cd: navigating the filesystem by changing directories\n\ncd projects\n\nTo check that the working directory changed, we can use a command we previously learned to see our location:\n\npwd"
  },
  {
    "objectID": "02-unix.html#autocomplete",
    "href": "02-unix.html#autocomplete",
    "title": "2  Unix",
    "section": "2.7 Autocomplete",
    "text": "2.7 Autocomplete\nIn Unix you can auto-complete by hitting tab. This means that we can type cd d then hit tab. Unix will either auto-complete if docs is the only directory/file starting with d or show you the options. Try it out! Using Unix without auto-complete will make it unbearable.\n\n2.7.1 cd continued\nGoing back one:\n\ncd ..\n\nGoing home:\n\ncd ~\n\nor simply:\n\ncd\n\nStating put (later we see why useful)\n\ncd .\n\nGoing far:\n\ncd /c/Users/yourusername/projects\n\nUsing relative paths:\n\ncd ../..\n\nGoing to previous working directory\n\ncd -"
  },
  {
    "objectID": "02-unix.html#practice",
    "href": "02-unix.html#practice",
    "title": "2  Unix",
    "section": "2.8 Practice",
    "text": "2.8 Practice\nLet’s explore some examples of navigating a filesystem using the command-line. Download and expand this file into a temporary directory and you will have the data struct in the following image.\n\n\n\nPractice file system\n\n\n\nSuppose our working directory is ~/projects, move to figs in project-1.\n\n\ncd project-1/figs\n\n\nNow suppose our working directory is ~/projects. Move to reports in docs in two different ways:\n\nThis is a relative path:\n\ncd ../docs/reports\n\nThe full path:\n\ncd ~/docs/reports ## assuming ~ is hometo\n\n\nSuppose we are in ~/projects/project-1/figs and want to change to ~/projects/project-2, show two different ways, one with relative path and one with full path.\n\nThis is with relative path\n\ncd ../../projects-2\n\nWith a full path\n\ncd ~/projects/proejcts-2 ## assuming home is ~"
  },
  {
    "objectID": "02-unix.html#more-unix-commands",
    "href": "02-unix.html#more-unix-commands",
    "title": "2  Unix",
    "section": "2.9 More Unix commands",
    "text": "2.9 More Unix commands\n\n2.9.1 mv: moving files\n\nmv path-to-file path-to-destination-directory\n\nFor example, if we want to move the file cv.tex from resumes to reports, you could use the full paths like this:\n\nmv ~/docs/resumes/cv.tex ~/docs/reports/\n\nYou can also use relative paths. So you could do this:\n\ncd ~/docs/resumes\nmv cv.tex ../reports/\n\nor this:\n\ncd ~/docs/reports/\nmv ../resumes/cv.tex ./\n\nWe can also use mv to change the name of a file.\n\ncd ~/docs/resumes\nmv cv.tex resume.tex\n\nWe can also combine the move and a rename. For example:\n\ncd ~/docs/resumes\nmv cv.tex ../reports/resume.tex\n\nAnd we can move entire directories. To move the resumes directory into reports, we do as follows:\n\nmv ~/docs/resumes ~/docs/reports/\n\nIt is important to add the last / to make it clear you do not want to rename the resumes directory to reports, but rather move it into the reports directory.\n\n\n2.9.2 cp: copying files\nThe command cp behaves similar to mv except instead of moving, we copy the file, meaning that the original file stays untouched.\n\n\n2.9.3 rm: removing files\nIn point-and-click systems, we remove files by dragging and dropping them into the trash or using a special click on the mouse. In Unix, we use the rm command.\n\n\n\n\n\n\nWarning\n\n\n\nUnlike throwing files into the trash, rm is permanent. Be careful!\n\n\nThe general way it works is as follows:\n\nrm filename\n\nYou can actually list files as well like this:\n\nrm filename-1 filename-2 filename-3\n\nYou can use full or relative paths. To remove directories, you will have to learn about arguments, which we do later.\n\n\n2.9.4 less: looking at a file\nOften you want to quickly look at the content of a file. If this file is a text file, the quickest way to do is by using the command less. To look a the file cv.tex, you do this:\n\ncd ~/docs/resumes\nless cv.tex \n\nTo exit the viewer, you type q. If the files are long, you can use the arrow keys to move up and down. There are many other keyboard commands you can use within less to, for example, search or jump pages."
  },
  {
    "objectID": "02-unix.html#sec-prep-project",
    "href": "02-unix.html#sec-prep-project",
    "title": "2  Unix",
    "section": "2.10 Preparing for a data science project",
    "text": "2.10 Preparing for a data science project\nWe are now ready to prepare a directory for a project. We will use the US murders project2 as an example.\nYou should start by creating a directory where you will keep all your projects. We recommend a directory called projects in your home directory. To do this you would type:\n\ncd ~\nmkdir projects\n\nOur project relates to gun violence murders so we will call the directory for our project murders. It will be a subdirectory in our projects directories. In the murders directory, we will create two subdirectories to hold the raw data and intermediate data. We will call these data and rda, respectively.\nOpen a terminal and make sure you are in the home directory:\n\ncd ~\n\nNow run the following commands to create the directory structure we want. At the end, we use ls and pwd to confirm we have generated the correct directories in the correct working directory:\n\ncd projects\nmkdir murders\ncd murders\nmkdir data rdas \nls\npwd\n\nNote that the full path of our murders dataset is ~/projects/murders.\nSo if we open a new terminal and want to navigate into that directory we type:\n\ncd projects/murders"
  },
  {
    "objectID": "02-unix.html#text-editors",
    "href": "02-unix.html#text-editors",
    "title": "2  Unix",
    "section": "2.11 Text editors",
    "text": "2.11 Text editors\nIn the course we will be using RStudio to edit files. But there will be situations in where this is not the most efficient approach. You might also need to write R code on a server that does not have RStudio installed. For this reason you need to learn to use a command-line text editors or terminal-based text editors. A key feature of these is that you can do everything you need on a terminal without the need for graphical interface. This is often necessary when using remote servers or computers you are not sitting in front off.\nCommand-line text editors are essential tools, especially for system administrators, developers, and other users who frequently work in a terminal environment. Here are some of the most popular command-line text editors:\n\nNano - Easy to use and beginner-friendly.\n\nFeatures: Simple interface, easy-to-use command prompts at the bottom of the screen, syntax highlighting.\n\nPico - Originally part of the Pine email client (Pico = PIne COmposer). It’s a simple editor and was widely used before Nano came around.\nVi or Vim - Vi is one of the oldest text editors and comes pre-installed on many UNIX systems. It is harder to use than Nano and Pico but is much more powerful. Vim is an enhanced version of Vi.\nEmacs - Another old and powerful text editor. It’s known for being extremely extensible.\n\nTo use these to edit a file you type, for example,\n\nnano filename"
  },
  {
    "objectID": "02-unix.html#advanced-unix",
    "href": "02-unix.html#advanced-unix",
    "title": "2  Unix",
    "section": "2.12 Advanced Unix",
    "text": "2.12 Advanced Unix\n\n2.12.1 Arguments\n\nrm -r directory-name\n\nall files, subdirectories, files in subdirectories, subdirectories in subdirectories, and so on, will be removed. This is equivalent to throwing a folder in the trash, except you can’t recover it. Once you remove it, it is deleted for good. Often, when you are removing directories, you will encounter files that are protected. In such cases, you can use the argument -f which stands for force.\nYou can also combine arguments. For instance, to remove a directory regardless of protected files, you type:\n\nrm -rf directory-name\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember that once you remove there is no going back, so use this command very carefully.\n\n\nA command that is often called with argument is ls. Here are some examples:\n\nls -a \n\n\nls -l \n\nIt is often useful to see files in chronological order. For that we use:\n\nls -t \n\nand to reverse the order of how files are shown you can use:\n\nls -r \n\nWe can combine all these arguments to show more information for all files in reverse chronological order:\n\nls -lart \n\nEach command has a different set of arguments. In the next section, we learn how to find out what they each do.\n\n\n2.12.2 Getting help\n\nman ls\n\nor\n\nls --help\n\n\n\n2.12.3 Pipes\n\nman ls | less\n\nor in Git Bash:\n\nls --help | less \n\nThis is also useful when listing files with many files. We can type:\n\nls -lart | less \n\n\n\n2.12.4 Wild cards\n\nls *.html\n\nTo remove all html files in a directory, we would type:\n\nrm *.html\n\nThe other useful wild card is the ? symbol.\n\nrm file-???.html\n\nThis will only remove files with that format.\nWe can combine wild cards. For example, to remove all files with the name file-001 regardless of suffix, we can type:\n\nrm file-001.* \n\n\n\n\n\n\n\nWarning\n\n\n\nCombining rm with the * wild card can be dangerous. There are combinations of these commands that will erase your entire filesystem without asking “are you sure?”. Make sure you understand how it works before using this wild card with the rm command.**\n\n\n\n\n2.12.5 Environment variables\nEarlier we saw this:\n\necho $HOME \n\nYou can see them all by typing:\n\nenv\n\nYou can change some of these environment variables. But their names vary across different shells. We describe shells in the next section.\n\n\n2.12.6 Shells\n\necho $SHELL\n\nThe most common one is bash.\nOnce you know the shell, you can change environmental variables. In Bash Shell, we do it using export variable value. To change the path, described in more detail soon, type: (Don’t actually run this command though!)\n\nexport PATH = /usr/bin/\n\n\n\n2.12.7 Executables\n\n\nwhich git\n\nThat directory is probably full of program files. The directory /usr/bin usually holds many program files. If you type:\n\nls /usr/bin\n\nin your terminal, you will see several executable files.\nThere are other directories that usually hold program files. The Application directory in the Mac or Program Files directory in Windows are examples.\nTo see where your system looks:\n\necho $PATH\n\nyou will see a list of directories separated by :. The directory /usr/bin is probably one of the first ones on the list.\nIf your command is called my-ls, you can type:\n\n./my-ls\n\nOnce you have mastered the basics of Unix, you should consider learning to write your own executables as they can help alleviate repetitive work.\n\n\n2.12.8 Permissions and file types\nIf you type:\n\nls -l\n\nAt the beginning, you will see a series of symbols like this -rw-r--r--. This string indicates the type of file: regular file -, directory d, or executable x. This string also indicates the permission of the file: is it readable? writable? executable? Can other users on the system read the file? Can other users on the system edit the file? Can other users execute if the file is executable? This is more advanced than what we cover here, but you can learn much more in a Unix reference book.\n\n\n2.12.9 Commands you should learn\n\ncurl - download data from the internet.\ntar - archive files and subdirectories of a directory into one file.\nssh - connect to another computer.\nfind - search for files by filename in your system.\ngrep - search for patterns in a file.\nawk/sed - These are two very powerful commands that permit you to find specific strings in files and change them.\nln - create a symbolic link. We do not recommend its use, but you should be familiar with it."
  },
  {
    "objectID": "02-unix.html#resources",
    "href": "02-unix.html#resources",
    "title": "2  Unix",
    "section": "2.13 Resources",
    "text": "2.13 Resources\nTo get started.\n\nhttps://www.codecademy.com/learn/learn-the-command-line\nhttps://www.edx.org/course/introduction-linux-linuxfoundationx-lfs101x-1\nhttps://www.coursera.org/learn/unix"
  },
  {
    "objectID": "02-unix.html#exercises",
    "href": "02-unix.html#exercises",
    "title": "2  Unix",
    "section": "2.14 Exercises",
    "text": "2.14 Exercises\nYou are not allowed to use RStudio or point and click for any of the exercises below. Open a text file called commands.txt using a text editor and keep a log of the commands you use in the exercises below. If you want to take notes, you can use # to distinguish notes from commands.\n\nDecide on a directory where you will save your class materials. Navigate into the directory using a full path.\nMake a directory called project-1 and cd into that directory.\nMake directors called data: data, rdas, code, and docs.\nUse curl or wget to download the file https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv and store it in rdas.\nCreate a R file in the code directory called code-1.R, write the following code in the file so that if the working directory is code it reads in the csv file you just downloaded. Use only relative paths.\n\n\nfilename &lt;- \"\"\ndat &lt;- read.csv(filename)\n\n\nAdd the following line to your R code so that it saves the file to the rdas directory. Use only relative paths.\n\n\nout &lt;- \"\"\ndat &lt;- save(dat, file = out)\n\n\nCreate a file code-2.R in the code directory. Use the following command to add a line to the file.\n\necho \"load('../rdas/murders.rda')\" &gt; code/code-2.R\nCheck to see if the line of code as added without opening a text editor.\n\nNavigate to the code directory and list all the files ending in .R.\nNavigate to the project-1 directory. Without navigating away, change the name of code-1.R to import.R, but keep the file in the same directory.\nChange the name of the project directory to murders. Describe what you have to change so the R script sill does the right thing and how this would be different if you had used full paths.\nBonus : Navigate to the murders directory. Read the man page for the find function. Use find to list all the files ending in .R."
  },
  {
    "objectID": "02-unix.html#footnotes",
    "href": "02-unix.html#footnotes",
    "title": "2  Unix",
    "section": "",
    "text": "https://style.tidyverse.org/↩︎\nhttps://github.com/rairizarry/murders↩︎"
  },
  {
    "objectID": "03-git.html#why-use-git-and-github",
    "href": "03-git.html#why-use-git-and-github",
    "title": "3  Git and GitHub",
    "section": "3.1 Why use Git and GitHub?",
    "text": "3.1 Why use Git and GitHub?\n\nSharing.\nCollaborating.\nVersion control.\n\nWe focus on the sharing aspects of Git and GitHub, but introduce some of the basics that permit you to collaborate and version control."
  },
  {
    "objectID": "03-git.html#what-is-git",
    "href": "03-git.html#what-is-git",
    "title": "3  Git and GitHub",
    "section": "3.2 What is Git?",
    "text": "3.2 What is Git?\n\n\n\nArt by: Allison Horst"
  },
  {
    "objectID": "03-git.html#what-is-github",
    "href": "03-git.html#what-is-github",
    "title": "3  Git and GitHub",
    "section": "3.3 What is GitHub?",
    "text": "3.3 What is GitHub?\nBasically, it’s a service that hosts the remote repository (repo) on the web. This facilitates collaboration and sharing greatly.\nThere many other features such as\n\na recognition system: reward, badges and stars, for example.\nhosting web pages, like the class notes for example.\nforks and pull requests,\nissue tracking\nautomation tools\n\nIt has been describes a social network for software developers.\nThe main tool behind GitHub, is Git.\nSimilar to how to how main tool behind RStudio, is R."
  },
  {
    "objectID": "03-git.html#github-accounts",
    "href": "03-git.html#github-accounts",
    "title": "3  Git and GitHub",
    "section": "3.4 GitHub accounts",
    "text": "3.4 GitHub accounts\nOnce you have a GitHub account, you are ready to connect Git and RStudio to this account.\nA first step is to let Git know who we are. This will make it easier to connect with GitHub. We start by opening a terminal window in RStudio (remember you can get one through Tools in the menu bar). Now we use the git config command to tell Git who we are. We will type the following two commands in our terminal window:\ngit config --global user.name \"Your Name\"\ngit config --global user.mail \"your@email.com\"\nConsider adding a profile README.md. Instructions are here\nLooks like this"
  },
  {
    "objectID": "03-git.html#repositories",
    "href": "03-git.html#repositories",
    "title": "3  Git and GitHub",
    "section": "3.5 Repositories",
    "text": "3.5 Repositories\nYou are now ready to create a GitHub repository (repo). This will be your remote repo.\nThe general idea is that you will have at least two copies of your code: one on your computer and one on GitHub. If you add collaborators to this repo, then each will have a copy on their computer. The GitHub copy is usually considered the main (previously called master) copy that each collaborator syncs to. Git will help you keep all the different copies synced.\nLet’s go make one on GitHub…\nThen create a directory on your computer, this will be the local repo, and connect it to the Github repository.\nFirst copy and paste the location of your git repository\nIt should look something like this:\nhttps://github.com/your-username/your-repo-name.git\ngit init\ngit remote add origin &lt;remote-url&gt;\nNow the two are connected."
  },
  {
    "objectID": "03-git.html#sec-git-overview",
    "href": "03-git.html#sec-git-overview",
    "title": "3  Git and GitHub",
    "section": "3.6 Overview of Git",
    "text": "3.6 Overview of Git\nThe main actions in Git are to:\n\npull changes from the remote repo, in this case the GitHub repo\nadd files, or as we say in the Git lingo stage files\ncommit changes to the local repo\npush changes to the remote repo, in our case the GitHub repo\n\n\n\n\nFrom Meme Git Compilation by Lulu Ilmaknun Qurotaini\n\n\n\n3.6.1 The four areas of Git\n\n\n\n3.6.2 Status\n\ngit status filename\n\n\n3.6.3 Add\nUse git add to move put file to staging area.\n\ngit add &lt;filename&gt;\ngit status &lt;filename&gt;\n\n\n3.6.4 Commit\nUse\ngit commit -m \"must add comment\"\nto move all the added files to the local repository. This file is now tracked and a copy of this version is kept going forward… this is like adding V1 to your filename.\nYou can commit files directly without using add by explicitely writing the files at the end of the commit:\ngit commit -m \"must add comment\" &lt;filename&gt;\n\n\n\n3.6.5 Push\nTo move to upstream repo we use\ngit push -u origin main\nThe -u flag sets the upstream, so in the future, you can simply use git push to push changes. So going forward we can just type:\ngit push\nHere we need to be careful as if collaborating this will affect the work of others. It might also create a conflict.\n\n\n\n3.6.6 Fetch\nTo update our local repository to the remote one we use\ngit fetch\n\n\n\n3.6.7 Merge\nOnce we are sure this is good, we can merge with our local files\ngit merge\n\n\n\n3.6.8 Pull\nIt is common to want to just skip the fetch step and just update everything. For this we use\ngit pull\n\n\n\n3.6.9 Checkout\n If you want to pull down a specific file you from the remote repo you can use:\ngit checkout filename\nBut if you have a newer version in your local repository this will create a conflict. If you are sure you want to get rid of your local copy you can remove and then checkout.\nYou can also use checkout to pull older version:\ngit checkout &lt;commit-id&gt; &lt;filename&gt;\nYou can get the commit-id either on the GitHub webpage or using\ngit log filename\n\n\n\n\n\n\nNote\n\n\n\nIf you are asked for passwords when connecting or pushing things to you want to read this and avoid this. It will be impossible to use if you have to enter a password each time you push."
  },
  {
    "objectID": "03-git.html#branches",
    "href": "03-git.html#branches",
    "title": "3  Git and GitHub",
    "section": "3.7 Branches",
    "text": "3.7 Branches\nGit can be even more complex. We can have several branches. These are useful for working in parallel or testing stuff out that might not make the main repo.\n\n\n\nArt by: Allison Horst\n\n\nWe wont go over this. But you should at least now these three commands\ngit remote -v\ngit brach"
  },
  {
    "objectID": "03-git.html#clone",
    "href": "03-git.html#clone",
    "title": "3  Git and GitHub",
    "section": "3.8 Clone",
    "text": "3.8 Clone\n\nIf you\ngit clone &lt;repo-url&gt;\npwd\nmkdir git-example\ncd git-example\ngit clone https://github.com/rairizarry/murders.git\ncd murders"
  },
  {
    "objectID": "03-git.html#using-git-in-rstudio",
    "href": "03-git.html#using-git-in-rstudio",
    "title": "3  Git and GitHub",
    "section": "3.9 Using Git in RStudio",
    "text": "3.9 Using Git in RStudio\nGo to file, new project, version control, and follow the instructions. Then notice the Git tab.\n For more memes see Meme Git Compilation by Lulu Ilmaknun"
  },
  {
    "objectID": "04-r-basics.html#packages",
    "href": "04-r-basics.html#packages",
    "title": "4  R Basics",
    "section": "4.1 Packages",
    "text": "4.1 Packages\nUse install.packages to install the dslabs package.\nTryout the following functions: sessionInfo, installed.packages"
  },
  {
    "objectID": "04-r-basics.html#prebuilt-functions",
    "href": "04-r-basics.html#prebuilt-functions",
    "title": "4  R Basics",
    "section": "4.2 Prebuilt functions",
    "text": "4.2 Prebuilt functions\nMuch of what we do in R is called prebuilt functions. Today we are using: ls, rm, library, search, factor, list, exists, str, typeof, class and maybe more.\nYou can see the code for a function by typing it without the parenthesis:\nTry this:\n\nls"
  },
  {
    "objectID": "04-r-basics.html#help-system",
    "href": "04-r-basics.html#help-system",
    "title": "4  R Basics",
    "section": "4.3 Help system",
    "text": "4.3 Help system\nIn R you can use ? or help to learn more about functions.\nYou can learn about function using\nhelp(\"ls\")\nor\n?ls"
  },
  {
    "objectID": "04-r-basics.html#the-workspace",
    "href": "04-r-basics.html#the-workspace",
    "title": "4  R Basics",
    "section": "4.4 The workspace",
    "text": "4.4 The workspace\nDefine a variable.\nUse ls to see if it’s there. Also take a look at the Environment tab in RStudio.\nUse rm to remove the variable you defined."
  },
  {
    "objectID": "04-r-basics.html#variable-name-convention",
    "href": "04-r-basics.html#variable-name-convention",
    "title": "4  R Basics",
    "section": "4.5 Variable name convention",
    "text": "4.5 Variable name convention\nA nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces.\nFor more we recommend this guide."
  },
  {
    "objectID": "04-r-basics.html#data-types",
    "href": "04-r-basics.html#data-types",
    "title": "4  R Basics",
    "section": "4.6 Data types",
    "text": "4.6 Data types\nThe main data types in R are:\n\nOne dimensional vectors: numeric, integer, logical, complex, characters.\nFactors\nLists: this includes data frames\nArrays: Matrices are the most widely used\nDate and time\ntibble\nS4 objects\n\nMany errors in R come from confusing data types. Let’s learn what these data types are and useful tools to help us.\nstr stands for structure, gives us information about an object.\ntypeof gives you the basic data type of the object. It reveals the lower-level, more fundamental type of an object in R’s memory.\nclass This function returns the class attribute of an object. The class of an object is essentially type_of at a higher, often user-facing level.\n\nlibrary(dslabs)\n\n\ntypeof(murders)\n\n[1] \"list\"\n\n\n\nclass(murders)\n\n[1] \"data.frame\"\n\n\n\nstr(murders)\n\n'data.frame':   51 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ abb       : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n $ population: num  4779736 710231 6392017 2915918 37253956 ...\n $ total     : num  135 19 232 93 1257 ..."
  },
  {
    "objectID": "04-r-basics.html#data-frames",
    "href": "04-r-basics.html#data-frames",
    "title": "4  R Basics",
    "section": "4.7 Data frames",
    "text": "4.7 Data frames\nDate frames are the most common class used in data analysis. It is like a spreadsheet. Rows represents observations and columns variables. Each variable can be a different data type.\nYou can add columns like this:\n\nmurders$pop_rank &lt;- rank(murders$population)\nhead(murders)\n\n       state abb region population total pop_rank\n1    Alabama  AL  South    4779736   135       29\n2     Alaska  AK   West     710231    19        5\n3    Arizona  AZ   West    6392017   232       36\n4   Arkansas  AR  South    2915918    93       20\n5 California  CA   West   37253956  1257       51\n6   Colorado  CO   West    5029196    65       30\n\n\nYou can access columns with the $\n\nmurders$population\n\n [1]  4779736   710231  6392017  2915918 37253956  5029196  3574097   897934\n [9]   601723 19687653  9920000  1360301  1567582 12830632  6483802  3046355\n[17]  2853118  4339367  4533372  1328361  5773552  6547629  9883640  5303925\n[25]  2967297  5988927   989415  1826341  2700551  1316470  8791894  2059179\n[33] 19378102  9535483   672591 11536504  3751351  3831074 12702379  1052567\n[41]  4625364   814180  6346105 25145561  2763885   625741  8001024  6724540\n[49]  1852994  5686986   563626\n\n\nbut also []\n\nmurders[1:5,]\n\n       state abb region population total pop_rank\n1    Alabama  AL  South    4779736   135       29\n2     Alaska  AK   West     710231    19        5\n3    Arizona  AZ   West    6392017   232       36\n4   Arkansas  AR  South    2915918    93       20\n5 California  CA   West   37253956  1257       51\n\n\n\nmurders[1:5, 1:2]\n\n       state abb\n1    Alabama  AL\n2     Alaska  AK\n3    Arizona  AZ\n4   Arkansas  AR\n5 California  CA\n\n\n\nmurders[1:5, c(\"state\", \"abb\")]\n\n       state abb\n1    Alabama  AL\n2     Alaska  AK\n3    Arizona  AZ\n4   Arkansas  AR\n5 California  CA"
  },
  {
    "objectID": "04-r-basics.html#with",
    "href": "04-r-basics.html#with",
    "title": "4  R Basics",
    "section": "4.8 with",
    "text": "4.8 with\nThe function with let’s us use the column names as objects:\n\nwith(murders, length(state))\n\n[1] 51"
  },
  {
    "objectID": "04-r-basics.html#vectors",
    "href": "04-r-basics.html#vectors",
    "title": "4  R Basics",
    "section": "4.9 Vectors",
    "text": "4.9 Vectors\nThe columns of data frames are one dimensional (atomic) vectors.\nHere is an example:\n\nlength(murders$population)\n\n[1] 51\n\n\nHow to create vectors:\n\nx &lt;- c(\"b\", \"s\", \"t\", \" \", \"2\", \"6\", \"0\")\n\nSequences are particularly useful:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nseq(1, 9, 2)\n\n[1] 1 3 5 7 9\n\n\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nseq_along(x)\n\n[1] 1 2 3 4 5 6 7"
  },
  {
    "objectID": "04-r-basics.html#factors",
    "href": "04-r-basics.html#factors",
    "title": "4  R Basics",
    "section": "4.10 Factors",
    "text": "4.10 Factors\nOne key data type distinction is factors versus characters:\n\ntypeof(murders$state)\n\n[1] \"character\"\n\ntypeof(murders$region)\n\n[1] \"integer\"\n\n\nFactors store levels and then the label of each level. They are very useful for categorical data.\n\nx &lt;- murders$region\nlevels(x)\n\n[1] \"Northeast\"     \"South\"         \"North Central\" \"West\"         \n\n\n\n4.10.1 Categories based on strata\nThe function cut is useful for converting numbers into categories\n\nwith(murders, cut(population, \n                  c(0, 10^6, 10^7, Inf)))\n\n [1] (1e+06,1e+07] (0,1e+06]     (1e+06,1e+07] (1e+06,1e+07] (1e+07,Inf]  \n [6] (1e+06,1e+07] (1e+06,1e+07] (0,1e+06]     (0,1e+06]     (1e+07,Inf]  \n[11] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07] (1e+07,Inf]   (1e+06,1e+07]\n[16] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07]\n[21] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07]\n[26] (1e+06,1e+07] (0,1e+06]     (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07]\n[31] (1e+06,1e+07] (1e+06,1e+07] (1e+07,Inf]   (1e+06,1e+07] (0,1e+06]    \n[36] (1e+07,Inf]   (1e+06,1e+07] (1e+06,1e+07] (1e+07,Inf]   (1e+06,1e+07]\n[41] (1e+06,1e+07] (0,1e+06]     (1e+06,1e+07] (1e+07,Inf]   (1e+06,1e+07]\n[46] (0,1e+06]     (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07]\n[51] (0,1e+06]    \nLevels: (0,1e+06] (1e+06,1e+07] (1e+07,Inf]\n\n\n\nmurders$size &lt;- cut(murders$population, c(0, 10^6, 10^7, Inf), \n            labels = c(\"small\", \"medium\", \"large\"))\nmurders[1:6,c(\"state\", \"size\")]\n\n       state   size\n1    Alabama medium\n2     Alaska  small\n3    Arizona medium\n4   Arkansas medium\n5 California  large\n6   Colorado medium\n\n\n\n\n4.10.2 changing levels\nYou can change the levels (this will come in handy when we learn linear models)\nOrder levels alphabetically:\n\nfactor(x, levels = sort(levels(murders$region)))\n\n [1] South         West          West          South         West         \n [6] West          Northeast     South         South         South        \n[11] South         West          West          North Central North Central\n[16] North Central North Central South         South         Northeast    \n[21] South         Northeast     North Central North Central South        \n[26] North Central West          North Central West          Northeast    \n[31] Northeast     West          Northeast     South         North Central\n[36] North Central South         West          Northeast     Northeast    \n[41] South         North Central South         South         West         \n[46] Northeast     South         West          South         North Central\n[51] West         \nLevels: North Central Northeast South West\n\n\nMake west the first level:\n\nx &lt;- relevel(x, ref = \"West\")\n\nOrder levels by population size:\n\nx &lt;- reorder(murders$region, murders$population, sum)\n\nFactors are more efficient:\n\nx &lt;- sample(murders$state[c(5,33,44)], 10^7, replace = TRUE)\ny &lt;- factor(x)\nobject.size(x)\n\n80000232 bytes\n\nobject.size(y)\n\n40000648 bytes\n\n\n\nsystem.time({x &lt;- tolower(x)})\n\n   user  system elapsed \n  1.451   0.009   1.460 \n\n\nExercise: How can we make this go much faster?\n\nsystem.time({levels(y) &lt;- tolower(levels(y))})\n\n   user  system elapsed \n  0.019   0.003   0.022 \n\n\nFactors can be confusing:\n\nx &lt;- factor(c(\"3\",\"2\",\"1\"), levels = c(\"3\",\"2\",\"1\"))\nas.numeric(x)\n\n[1] 1 2 3\n\n\n\nx[1]\n\n[1] 3\nLevels: 3 2 1\n\nlevels(x[1])\n\n[1] \"3\" \"2\" \"1\"\n\ntable(x[1])\n\n\n3 2 1 \n1 0 0 \n\nz &lt;- x[1]\nz &lt;- droplevels(z)\n\n\nx[1] &lt;- \"4\"\n\nWarning in `[&lt;-.factor`(`*tmp*`, 1, value = \"4\"): invalid factor level, NA\ngenerated\n\nx\n\n[1] &lt;NA&gt; 2    1   \nLevels: 3 2 1"
  },
  {
    "objectID": "04-r-basics.html#nas",
    "href": "04-r-basics.html#nas",
    "title": "4  R Basics",
    "section": "4.11 NAs",
    "text": "4.11 NAs\nNA stands for not available. We will see many NAs if we analyze data generally.\n\nx &lt;- as.numeric(\"a\")\n\nWarning: NAs introduced by coercion\n\n\n\nis.na(x)\n\n[1] TRUE\n\n\n\nis.na(\"a\")\n\n[1] FALSE\n\n\n\n1 + 2 + NA\n\n[1] NA\n\n\nWhen used with logicals behaves like FALSE\n\nTRUE & NA\n\n[1] NA\n\nTRUE | NA\n\n[1] TRUE\n\n\nBut is is not FALSE. Try this:\n\nif (NA) print(1) else print(0)\n\nA related constant is NaN which stands for not a number. It is a numeric that is not a number.\n\nclass(0/0)\n\n[1] \"numeric\"\n\nsqrt(-1)\n\nWarning in sqrt(-1): NaNs produced\n\n\n[1] NaN\n\nlog(-1)\n\nWarning in log(-1): NaNs produced\n\n\n[1] NaN\n\n0/0\n\n[1] NaN"
  },
  {
    "objectID": "04-r-basics.html#coercing",
    "href": "04-r-basics.html#coercing",
    "title": "4  R Basics",
    "section": "4.12 coercing",
    "text": "4.12 coercing\nWhen you do something nonsensical with data types, R tries to figure out what you mean. This can cause confusion and unnoticed errors. So it’s important to understand how and when it happens. Here are some examples:\n\ntypeof(1L)\n\n[1] \"integer\"\n\ntypeof(1)\n\n[1] \"double\"\n\ntypeof(1 + 1L)\n\n[1] \"double\"\n\n\n\nc(\"a\", 1, 2)\n\n[1] \"a\" \"1\" \"2\"\n\n\n\nTRUE + FALSE\n\n[1] 1\n\n\n\nfactor(\"a\") == \"a\"\n\n[1] TRUE\n\nidentical(factor(\"a\"), \"a\")\n\n[1] FALSE\n\n\nYou want to avoid automatic coercion and instead explicitly do it. Most coercion functions start with as.\n\nx &lt;- factor(c(\"a\",\"b\",\"b\",\"c\"))\nas.character(x)\n\n[1] \"a\" \"b\" \"b\" \"c\"\n\nas.numeric(x)\n\n[1] 1 2 2 3\n\n\n\nx &lt;- c(\"12323\", \"12,323\")\nas.numeric(x)\n\nWarning: NAs introduced by coercion\n\n\n[1] 12323    NA\n\nreadr::parse_guess(x)\n\n[1] 12323 12323"
  },
  {
    "objectID": "04-r-basics.html#lists",
    "href": "04-r-basics.html#lists",
    "title": "4  R Basics",
    "section": "4.13 lists",
    "text": "4.13 lists\nData frames are a type of list. List permit components of different types and, unlike data frames, length\n\nx &lt;- list(name = \"John\", id = 112, grades = c(95, 87, 92))\n\nYou can access components in different ways:\n\nx$name\n\n[1] \"John\"\n\nx[[1]]\n\n[1] \"John\"\n\nx[[\"name\"]]\n\n[1] \"John\""
  },
  {
    "objectID": "04-r-basics.html#matrics",
    "href": "04-r-basics.html#matrics",
    "title": "4  R Basics",
    "section": "4.14 matrics",
    "text": "4.14 matrics\nMatrices are another widely used data type. They are similar to data frames except all entries need to be of the same type.\nWe will learn more about matrices in the High Dimensional data Analysis part of the class."
  },
  {
    "objectID": "04-r-basics.html#functions",
    "href": "04-r-basics.html#functions",
    "title": "4  R Basics",
    "section": "4.15 functions",
    "text": "4.15 functions\nYou can define your own function. The form is like this:\n\nf &lt;- function(x, y, z = 0){\n  ### do calculations with x, y, z to compute object\n  ## return(object)\n}\n\nHere is an example of a function that sums \\(1,2,\\dots,n\\)\n\ns &lt;- function(n){\n   return(sum(1:n))\n}"
  },
  {
    "objectID": "04-r-basics.html#lexical-scope",
    "href": "04-r-basics.html#lexical-scope",
    "title": "4  R Basics",
    "section": "4.16 Lexical scope",
    "text": "4.16 Lexical scope\n\nf &lt;- function(x){\n  cat(\"y is\", y,\"\\n\")\n  y &lt;- x\n  cat(\"y is\", y,\"\\n\")\n  return(y)\n}\ny &lt;- 2\nf(3)\n\ny is 2 \ny is 3 \n\n\n[1] 3\n\ny &lt;- f(3)\n\ny is 2 \ny is 3 \n\ny\n\n[1] 3"
  },
  {
    "objectID": "04-r-basics.html#namespaces",
    "href": "04-r-basics.html#namespaces",
    "title": "4  R Basics",
    "section": "4.17 Namespaces",
    "text": "4.17 Namespaces\nLook at this function.\n\nfilter\nlibrary(dplyr)\nfilter\n\nNote this is just the Global Environment.\nUse search to see other environments.\nNote all the functions in stats\nYou can explicitly say which you want:\n\nstats::filter\ndplyr::filter\n\nTry to understand this example:\n\nexists(\"murders\")\n\n[1] TRUE\n\nlibrary(dslabs)\nexists(\"murders\")\n\n[1] TRUE\n\nmurders &lt;- murders\nmurders2 &lt;- murders\nrm(murders)\nexists(\"murders\")\n\n[1] TRUE\n\ndetach(\"package:dslabs\")\nexists(\"murders\")\n\n[1] FALSE\n\nexists(\"murders2\")\n\n[1] TRUE"
  },
  {
    "objectID": "04-r-basics.html#object-oriented-programming",
    "href": "04-r-basics.html#object-oriented-programming",
    "title": "4  R Basics",
    "section": "4.18 object oriented programming",
    "text": "4.18 object oriented programming\nR uses object oriented programming. It uses to approaches referred to as S3 and S4. The original S3 is more common.\nWhat does this mean?\n\nclass(co2)\n\n[1] \"ts\"\n\nplot(co2)\n\n\n\n\n\nplot(as.numeric(co2))\n\n\n\n\nSee the difference? The first one actually calls the function\n\nplot.ts\n\nNotice all the plot functions that start with plot.\nThe function plot will call different functions depending on the class of the arguments:\n\nplot\n\nfunction (x, y, ...) \nUseMethod(\"plot\")\n&lt;bytecode: 0x1157ebb90&gt;\n&lt;environment: namespace:base&gt;"
  },
  {
    "objectID": "04-r-basics.html#exercises",
    "href": "04-r-basics.html#exercises",
    "title": "4  R Basics",
    "section": "4.19 Exercises",
    "text": "4.19 Exercises\n\nWhat is the sum of the first 100 positive integers? The formula for the sum of integers \\(1\\) through \\(n\\) is \\(n(n+1)/2\\). Define \\(n=100\\) and then use R to compute the sum of \\(1\\) through \\(100\\) using the formula. What is the sum?\n\n\nn &lt;- 100\nn*(n + 1) / 2\n\n[1] 5050\n\n\n\nNow use the same formula to compute the sum of the integers from 1 through 1,000.\n\n\nn &lt;- 1000\nn*(n + 1) / 2\n\n[1] 500500\n\n\n\nNow use the functions seq and sum to compute the sum with R for any n, rather than a formula.\n\n\nn &lt;- 100\nx &lt;- seq(1, 100)\nsum(x)\n\n[1] 5050\n\n\n\nIn math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type sqrt(4), we evaluate the sqrt function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.\n\n\nlog(sqrt(100), base = 10)\n\n[1] 1\n\nlog10(sqrt(100))\n\n[1] 1\n\n\n\nMake sure the US murders dataset is loaded. Use the function str to examine the structure of the murders object. What are the column names used by the data frame for these five variables?\n\n\nlibrary(dslabs)\nstr(murders)\n\n'data.frame':   51 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ abb       : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n $ population: num  4779736 710231 6392017 2915918 37253956 ...\n $ total     : num  135 19 232 93 1257 ..."
  },
  {
    "objectID": "05-vectorization.html#arithmetics",
    "href": "05-vectorization.html#arithmetics",
    "title": "5  Vectorization",
    "section": "5.1 Arithmetics",
    "text": "5.1 Arithmetics\n\nheights &lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)\n\nConvert to meters:\n\nheights * 2.54 / 100\n\n [1] 1.7526 1.5748 1.6764 1.7780 1.7780 1.8542 1.7018 1.8542 1.7018 1.7780\n\n\nDifference from the average:\n\navg &lt;- mean(heights)\nheights - avg \n\n [1]  0.3 -6.7 -2.7  1.3  1.3  4.3 -1.7  4.3 -1.7  1.3\n\n\nExercise: compute the height in standardized units\n\ns &lt;- sd(heights)\n(heights - avg) / s\n\n [1]  0.08995503 -2.00899575 -0.80959530  0.38980515  0.38980515  1.28935548\n [7] -0.50974519  1.28935548 -0.50974519  0.38980515\n\n# can also use scale(heights)\n\nIf it’s two vectors, it does it component wise:\n\nheights &lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)\nerror &lt;- rnorm(length(heights), 0, 0.1)\nheights + error\n\n [1] 68.76366 61.95831 66.23369 70.01296 70.00050 73.08083 66.91846 73.23657\n [9] 66.98593 69.99302\n\n\nExercise:\nAdd a column to the murders dataset with the murder rate in per 100,000.\n\nlibrary(dslabs)\nmurders$rate &lt;- with(murders, total / population * 10^5)"
  },
  {
    "objectID": "05-vectorization.html#functions-that-vectorize",
    "href": "05-vectorization.html#functions-that-vectorize",
    "title": "5  Vectorization",
    "section": "5.2 Functions that vectorize",
    "text": "5.2 Functions that vectorize\nMost arithmetic functions work on vectors\n\nx &lt;- 1:10\nsqrt(x)\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\nlog(x)\n\n [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n [8] 2.0794415 2.1972246 2.3025851\n\n2^x\n\n [1]    2    4    8   16   32   64  128  256  512 1024\n\n\nNote that the conditional function if-else does not vectorize. A particularly useful function is a vectorized version ifelse. Here is an example:\n\na &lt;- c(0, 1, 2, -4, 5)\nifelse(a &gt; 0, 1/a, NA)\n\n[1]  NA 1.0 0.5  NA 0.2\n\n\nOther conditional functions, such as any and all, do vectorize."
  },
  {
    "objectID": "05-vectorization.html#indexing",
    "href": "05-vectorization.html#indexing",
    "title": "5  Vectorization",
    "section": "5.3 Indexing",
    "text": "5.3 Indexing\nVectorization also works for logical relationships:\n\nind &lt;- murders$population &lt; 10^6\n\nYou can subset a vector using these:\n\nmurders$state[ind]\n\n[1] \"Alaska\"               \"Delaware\"             \"District of Columbia\"\n[4] \"Montana\"              \"North Dakota\"         \"South Dakota\"        \n[7] \"Vermont\"              \"Wyoming\"             \n\n\nYou can also use vectorization to apply logical operators:\n\nind &lt;- murders$population &lt; 10^6 & murders$region == \"West\"\nmurders$state[ind]\n\n[1] \"Alaska\"  \"Montana\" \"Wyoming\""
  },
  {
    "objectID": "05-vectorization.html#split",
    "href": "05-vectorization.html#split",
    "title": "5  Vectorization",
    "section": "5.4 split",
    "text": "5.4 split\nSplit is a useful function to get indexes using a factor.\n\ninds &lt;- with(murders, split(seq_along(region), region))\nmurders$state[inds$West]\n\n [1] \"Alaska\"     \"Arizona\"    \"California\" \"Colorado\"   \"Hawaii\"    \n [6] \"Idaho\"      \"Montana\"    \"Nevada\"     \"New Mexico\" \"Oregon\"    \n[11] \"Utah\"       \"Washington\" \"Wyoming\""
  },
  {
    "objectID": "05-vectorization.html#functions-for-subsetting",
    "href": "05-vectorization.html#functions-for-subsetting",
    "title": "5  Vectorization",
    "section": "5.5 Functions for subsetting",
    "text": "5.5 Functions for subsetting\nThe functions which, match and the operator %in% are useful for sub-setting\nHere are some examples:\n\nind &lt;- which(murders$state == \"California\")\nind\n\n[1] 5\n\nmurders[ind,]\n\n       state abb region population total     rate\n5 California  CA   West   37253956  1257 3.374138\n\n\n\nind &lt;- match(c(\"New York\", \"Florida\", \"Texas\"), murders$state)\nind\n\n[1] 33 10 44\n\n\n\nc(\"Boston\", \"Dakota\", \"Washington\") %in% murders$state\n\n[1] FALSE FALSE  TRUE"
  },
  {
    "objectID": "05-vectorization.html#sapply",
    "href": "05-vectorization.html#sapply",
    "title": "5  Vectorization",
    "section": "5.6 sapply",
    "text": "5.6 sapply\nYou can apply functions that don’t vectorize. Like this one:\n\ns &lt;- function(n){\n   return(sum(1:n))\n}\n\nTry it on a vector:\n\nns &lt;- c(25, 100, 1000)\ns(ns)\n\nWarning in 1:n: numerical expression has 3 elements: only the first used\n\n\n[1] 325\n\n\nWe can use sapply\n\nsapply(ns, s)\n\n[1]    325   5050 500500\n\n\nsapply will work on any vector, including lists."
  },
  {
    "objectID": "05-vectorization.html#exercises",
    "href": "05-vectorization.html#exercises",
    "title": "5  Vectorization",
    "section": "5.7 Exercises",
    "text": "5.7 Exercises\nNow we are ready to help your friend. Let’s give them options of places with low murders rates, mountains, and not too small.\nFor the following exercises do no load any packages other than dslabs.\n\nShow the subset of murders showing states with less than 1 per 100,000 deaths. Show all variables.\n\n\nif (exists(\"murders\")) rm(murders)\nlibrary(dslabs)\n\nmurders$rate &lt;- with(murders, total/population*10^5)\nmurders[murders$rate &lt; 1,]\n\n           state abb        region population total      rate\n12        Hawaii  HI          West    1360301     7 0.5145920\n13         Idaho  ID          West    1567582    12 0.7655102\n16          Iowa  IA North Central    3046355    21 0.6893484\n20         Maine  ME     Northeast    1328361    11 0.8280881\n24     Minnesota  MN North Central    5303925    53 0.9992600\n30 New Hampshire  NH     Northeast    1316470     5 0.3798036\n35  North Dakota  ND North Central     672591     4 0.5947151\n38        Oregon  OR          West    3831074    36 0.9396843\n42  South Dakota  SD North Central     814180     8 0.9825837\n45          Utah  UT          West    2763885    22 0.7959810\n46       Vermont  VT     Northeast     625741     2 0.3196211\n51       Wyoming  WY          West     563626     5 0.8871131\n\n\n\nShow the subset of murders showing states with less than 1 per 100,000 deaths and in the West of the US. Don’t show the region variable.\n\n\nmurders[murders$rate &lt; 1 & murders$region == \"West\",]\n\n     state abb region population total      rate\n12  Hawaii  HI   West    1360301     7 0.5145920\n13   Idaho  ID   West    1567582    12 0.7655102\n38  Oregon  OR   West    3831074    36 0.9396843\n45    Utah  UT   West    2763885    22 0.7959810\n51 Wyoming  WY   West     563626     5 0.8871131\n\n\n\nShow the largest state with a rate less than 1 per 100,000.\n\n\ndat &lt;- murders[murders$rate &lt; 1,]\ndat[which.max(dat$population),]\n\n       state abb        region population total    rate\n24 Minnesota  MN North Central    5303925    53 0.99926\n\n\n\nShow the state with a population of more than 10 million with the lowest rate.\n\n\ndat &lt;- murders[murders$population &gt;= 10^7,]\ndat[which.min(dat$rate),]\n\n      state abb    region population total    rate\n33 New York  NY Northeast   19378102   517 2.66796\n\n\n\nCompute the rate for each region of the US.\n\n\nindexes &lt;- split(1:nrow(murders), murders$region)\nsapply(indexes, function(ind) {\n  sum(murders$total[ind])/sum(murders$population[ind])*10^5\n})\n\n    Northeast         South North Central          West \n     2.655592      3.626558      2.731334      2.656175 \n\n\nMore practice exercises:\n\nCreate a vector of numbers that starts at 6, does not pass 55, and adds numbers in increments of 4/7: 6, 6 + 4/7, 6 + 8/7, and so on. How many numbers does the list have? Hint: use seq and length.\nMake this data frame:\n\n\ntemp &lt;- c(35, 88, 42, 84, 81, 30)\ncity &lt;- c(\"Beijing\", \"Lagos\", \"Paris\", \"Rio de Janeiro\", \n          \"San Juan\", \"Toronto\")\ncity_temps &lt;- data.frame(name = city, temperature = temp)\n\nConvert the temperatures to Celsius.\n\nCompute the following sum\n\n\\[\nS_n = 1+1/2^2 + 1/3^2 + \\dots 1/n^2\n\\]\nShow that as \\(n\\) gets bigger we get closer \\(\\pi^2/6\\).\n\nUse the %in% operator and the predefined object state.abb to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?\nExtend the code you used in the previous exercise to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and viceversa, then which to obtain an index.\nShow all variables for New York, California, and Texas, in that order."
  },
  {
    "objectID": "06-tidyverse.html#tidy-data",
    "href": "06-tidyverse.html#tidy-data",
    "title": "6  Tidyverse",
    "section": "6.1 Tidy data",
    "text": "6.1 Tidy data\nThis is tidy:\n\n\n      country year fertility\n1     Germany 1960      2.41\n2 South Korea 1960      6.16\n3     Germany 1961      2.44\n4 South Korea 1961      5.99\n5     Germany 1962      2.47\n6 South Korea 1962      5.79\n\n\nOriginally, the data was in the following format:\n\n\n      country 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n1     Germany 2.41 2.44 2.47 2.49 2.49 2.48 2.44 2.37 2.28 2.17 2.04\n2 South Korea 6.16 5.99 5.79 5.57 5.36 5.16 4.99 4.85 4.73 4.62 4.53\n\n\nNot tidy.\nPart of what we learn in the data wrangling part of the class is to make data tidy."
  },
  {
    "objectID": "06-tidyverse.html#adding-a-column-with-mutate",
    "href": "06-tidyverse.html#adding-a-column-with-mutate",
    "title": "6  Tidyverse",
    "section": "6.2 Adding a column with mutate",
    "text": "6.2 Adding a column with mutate\n\nmurders &lt;- mutate(murders, rate = total/population*100000)\n\nNotice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error? This is non-standard evaluation where the context is used to know what variable names means."
  },
  {
    "objectID": "06-tidyverse.html#subsetting-with-filter",
    "href": "06-tidyverse.html#subsetting-with-filter",
    "title": "6  Tidyverse",
    "section": "6.3 Subsetting with filter",
    "text": "6.3 Subsetting with filter\n\nfilter(murders, rate &lt;= 0.71)\n\n          state abb        region population total      rate\n1        Hawaii  HI          West    1360301     7 0.5145920\n2          Iowa  IA North Central    3046355    21 0.6893484\n3 New Hampshire  NH     Northeast    1316470     5 0.3798036\n4  North Dakota  ND North Central     672591     4 0.5947151\n5       Vermont  VT     Northeast     625741     2 0.3196211"
  },
  {
    "objectID": "06-tidyverse.html#selecting-columns-with-select",
    "href": "06-tidyverse.html#selecting-columns-with-select",
    "title": "6  Tidyverse",
    "section": "6.4 Selecting columns with select",
    "text": "6.4 Selecting columns with select\n\nnew_table &lt;- select(murders, state, region, rate)\nfilter(new_table, rate &lt;= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211"
  },
  {
    "objectID": "06-tidyverse.html#the-pipe-or",
    "href": "06-tidyverse.html#the-pipe-or",
    "title": "6  Tidyverse",
    "section": "6.5 The pipe: |> or %>%",
    "text": "6.5 The pipe: |&gt; or %&gt;%\nWe use the pipe to chain a series of operations… for example if we want to select columns and then filter rows we chain like this:\n\\[ \\mbox{original data }\n\\rightarrow \\mbox{ select }\n\\rightarrow \\mbox{ filter } \\]\nThe code looks like this:\n\nmurders |&gt; select(state, region, rate) |&gt; filter(rate &lt;= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nThe object on the left of the pipe is used as the first argument for the function on the right.\nThe second argument becomes the first, the third the second, and so on…\n\n16 |&gt; sqrt() |&gt; log(base = 2)\n\n[1] 2"
  },
  {
    "objectID": "06-tidyverse.html#summarizing-data",
    "href": "06-tidyverse.html#summarizing-data",
    "title": "6  Tidyverse",
    "section": "6.6 Summarizing data",
    "text": "6.6 Summarizing data\nHere is how it works:\n\nmurders |&gt; summarize(avg = mean(rate))\n\n       avg\n1 2.779125\n\n\nLet’s compute murder rate for the US. Is the above it?\nNo the rate is NOT the average of rates.\n\nmurders |&gt; summarize(rate = sum(total)/sum(population)*100000)\n\n      rate\n1 3.034555\n\n\n\n6.6.1 Multiple summaries\nWe want the median, minimum and max population size:\n\nmurders |&gt; summarize(median = median(population), min = min(population), max = max(population))\n\n   median    min      max\n1 4339367 563626 37253956\n\n\nWhy don’t we use quantiles?\n\nmurders |&gt; summarize(quantiles = quantile(population, c(0.5, 0, 1)))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n  quantiles\n1   4339367\n2    563626\n3  37253956\n\n\nFor multiple summaries we use reframe\n\nmurders |&gt; reframe(quantiles = quantile(population, c(0.5, 0, 1)))\n\n  quantiles\n1   4339367\n2    563626\n3  37253956\n\n\nHowever, if we want a column per summary, as the summarize call above, we have to define a function that returns a data frame like this:\n\nmedian_min_max &lt;- function(x){\n  qs &lt;- quantile(x, c(0.5, 0, 1))\n  data.frame(median = qs[1], min = qs[2], max = qs[3])\n}\n\nThen we can call summarize as above:\n\nmurders |&gt; summarize(median_min_max(population))\n\n   median    min      max\n1 4339367 563626 37253956\n\n\n\n\n6.6.2 Group then summarize with group_by\nLet’s compute murder rate by region.\nTake a close look at this output? How is it different than the original?\n\nmurders |&gt; group_by(region)\n\n# A tibble: 51 × 6\n# Groups:   region [4]\n   state                abb   region    population total  rate\n   &lt;chr&gt;                &lt;chr&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Alabama              AL    South        4779736   135  2.82\n 2 Alaska               AK    West          710231    19  2.68\n 3 Arizona              AZ    West         6392017   232  3.63\n 4 Arkansas             AR    South        2915918    93  3.19\n 5 California           CA    West        37253956  1257  3.37\n 6 Colorado             CO    West         5029196    65  1.29\n 7 Connecticut          CT    Northeast    3574097    97  2.71\n 8 Delaware             DE    South         897934    38  4.23\n 9 District of Columbia DC    South         601723    99 16.5 \n10 Florida              FL    South       19687653   669  3.40\n# ℹ 41 more rows\n\n\nNote the Groups: region [4] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame, and dplyr functions, in particular summarize, will behave differently when acting on this object.\n\nmurders |&gt; group_by(region) |&gt; summarize(rate = sum(total) / sum(population) * 100000)\n\n# A tibble: 4 × 2\n  region         rate\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Northeast      2.66\n2 South          3.63\n3 North Central  2.73\n4 West           2.66\n\n\nThe summarize function applies the summarization to each group separately.\nFor another example, let’s compute the median, minimum, and maximum population in the four regions of the country using the median_min_max defined above:\n\nmurders |&gt; group_by(region) |&gt; summarize(median_min_max(population))\n\n# A tibble: 4 × 4\n  region          median    min      max\n  &lt;fct&gt;            &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 Northeast     3574097  625741 19378102\n2 South         4625364  601723 25145561\n3 North Central 5495456. 672591 12830632\n4 West          2700551  563626 37253956"
  },
  {
    "objectID": "06-tidyverse.html#ungroup",
    "href": "06-tidyverse.html#ungroup",
    "title": "6  Tidyverse",
    "section": "6.7 ungroup",
    "text": "6.7 ungroup\nYou can also summarize a variable but not collapse the dataset. We use mutate instead of summarize. Here is an example where we add a column with the population in each region and the number of states in the region, shown for each state. When we do this, we usually want to ungroup before continuing our analysis.\n\nmurders |&gt; group_by(region) |&gt; \n  mutate(region_pop = sum(population), n = n()) |&gt;\n  ungroup()\n\n# A tibble: 51 × 8\n   state                abb   region    population total  rate region_pop     n\n   &lt;chr&gt;                &lt;chr&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n 1 Alabama              AL    South        4779736   135  2.82  115674434    17\n 2 Alaska               AK    West          710231    19  2.68   71945553    13\n 3 Arizona              AZ    West         6392017   232  3.63   71945553    13\n 4 Arkansas             AR    South        2915918    93  3.19  115674434    17\n 5 California           CA    West        37253956  1257  3.37   71945553    13\n 6 Colorado             CO    West         5029196    65  1.29   71945553    13\n 7 Connecticut          CT    Northeast    3574097    97  2.71   55317240     9\n 8 Delaware             DE    South         897934    38  4.23  115674434    17\n 9 District of Columbia DC    South         601723    99 16.5   115674434    17\n10 Florida              FL    South       19687653   669  3.40  115674434    17\n# ℹ 41 more rows\n\n\n\n6.7.1 pull\nTidyverse function always returns a data frame. Even if its just one number.\n\nmurders |&gt; \n  summarize(rate = sum(total)/sum(population)*100000) |&gt;\n  class()\n\n[1] \"data.frame\"\n\n\nTo get a number use pull\n\nmurders |&gt; \n  summarize(rate = sum(total)/sum(population)*100000) |&gt;\n  pull(rate) \n\n[1] 3.034555"
  },
  {
    "objectID": "06-tidyverse.html#sorting-data-frames",
    "href": "06-tidyverse.html#sorting-data-frames",
    "title": "6  Tidyverse",
    "section": "6.8 Sorting data frames",
    "text": "6.8 Sorting data frames\nStates order by rate\n\nmurders |&gt; arrange(rate) |&gt; head()\n\n          state abb        region population total      rate\n1       Vermont  VT     Northeast     625741     2 0.3196211\n2 New Hampshire  NH     Northeast    1316470     5 0.3798036\n3        Hawaii  HI          West    1360301     7 0.5145920\n4  North Dakota  ND North Central     672591     4 0.5947151\n5          Iowa  IA North Central    3046355    21 0.6893484\n6         Idaho  ID          West    1567582    12 0.7655102\n\n\nIf we want decreasing we can either use the negative or, for more readability, use desc:\n\nmurders |&gt; arrange(desc(rate)) |&gt; head()\n\n                 state abb        region population total      rate\n1 District of Columbia  DC         South     601723    99 16.452753\n2            Louisiana  LA         South    4533372   351  7.742581\n3             Missouri  MO North Central    5988927   321  5.359892\n4             Maryland  MD         South    5773552   293  5.074866\n5       South Carolina  SC         South    4625364   207  4.475323\n6             Delaware  DE         South     897934    38  4.231937\n\n\nWe can use two variables as well:\n\nmurders |&gt; arrange(region, desc(rate)) |&gt; head(11)\n\n                  state abb    region population total       rate\n1          Pennsylvania  PA Northeast   12702379   457  3.5977513\n2            New Jersey  NJ Northeast    8791894   246  2.7980319\n3           Connecticut  CT Northeast    3574097    97  2.7139722\n4              New York  NY Northeast   19378102   517  2.6679599\n5         Massachusetts  MA Northeast    6547629   118  1.8021791\n6          Rhode Island  RI Northeast    1052567    16  1.5200933\n7                 Maine  ME Northeast    1328361    11  0.8280881\n8         New Hampshire  NH Northeast    1316470     5  0.3798036\n9               Vermont  VT Northeast     625741     2  0.3196211\n10 District of Columbia  DC     South     601723    99 16.4527532\n11            Louisiana  LA     South    4533372   351  7.7425810"
  },
  {
    "objectID": "06-tidyverse.html#exercises",
    "href": "06-tidyverse.html#exercises",
    "title": "6  Tidyverse",
    "section": "6.9 Exercises",
    "text": "6.9 Exercises"
  },
  {
    "objectID": "06-tidyverse.html#exercises-1",
    "href": "06-tidyverse.html#exercises-1",
    "title": "6  Tidyverse",
    "section": "6.10 Exercises",
    "text": "6.10 Exercises\nLet’s redo the exercises from previous chapter but now with tidyverse:\n\nShow the subset of murders showing states with less than 1 per 100,000 deaths. Show all variables.\n\n\nmurders &lt;- mutate(murders, rate = total/population*10^5)\nfilter(murders, rate &lt; 1)\n\n           state abb        region population total      rate\n1         Hawaii  HI          West    1360301     7 0.5145920\n2          Idaho  ID          West    1567582    12 0.7655102\n3           Iowa  IA North Central    3046355    21 0.6893484\n4          Maine  ME     Northeast    1328361    11 0.8280881\n5      Minnesota  MN North Central    5303925    53 0.9992600\n6  New Hampshire  NH     Northeast    1316470     5 0.3798036\n7   North Dakota  ND North Central     672591     4 0.5947151\n8         Oregon  OR          West    3831074    36 0.9396843\n9   South Dakota  SD North Central     814180     8 0.9825837\n10          Utah  UT          West    2763885    22 0.7959810\n11       Vermont  VT     Northeast     625741     2 0.3196211\n12       Wyoming  WY          West     563626     5 0.8871131\n\n\n\nShow the subset of murders showing states with less than 1 per 100,000 deaths and in the West of the US. Don’t show the region variable.\n\n\nfilter(murders, rate &lt; 1 & region == \"West\")\n\n    state abb region population total      rate\n1  Hawaii  HI   West    1360301     7 0.5145920\n2   Idaho  ID   West    1567582    12 0.7655102\n3  Oregon  OR   West    3831074    36 0.9396843\n4    Utah  UT   West    2763885    22 0.7959810\n5 Wyoming  WY   West     563626     5 0.8871131\n\n\n\nShow the largest state with a rate less than 1 per 100,000.\n\n\nmurders |&gt; filter(rate &lt; 1) |&gt; slice_max(population)\n\n      state abb        region population total    rate\n1 Minnesota  MN North Central    5303925    53 0.99926\n\n\n\nShow the state with a population of more than 10 million with the lowest rate.\n\n\nmurders |&gt; filter(population &gt; 10^7) |&gt; slice_min(rate)\n\n     state abb    region population total    rate\n1 New York  NY Northeast   19378102   517 2.66796\n\n\n\nCompute the rate for each region of the US.\n\n\nmurders |&gt; group_by(region) |&gt; summarize(rate = sum(total)/sum(population)*10^5)\n\n# A tibble: 4 × 2\n  region         rate\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Northeast      2.66\n2 South          3.63\n3 North Central  2.73\n4 West           2.66\n\n\nFor the next exercises we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package. Once you install the NHANES package, you can load the data like this:\n\nlibrary(NHANES)\n\n\nCheck for consistency between Race1 and Race3. Do any rows have different entries?\n\n\nNHANES |&gt; filter(!is.na(Race1) & !is.na(Race3)) |&gt; \n  filter(as.character(Race1) != as.character(Race3)) |&gt;\n  count(Race1, Race3)\n\n# A tibble: 1 × 3\n  Race1 Race3     n\n  &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n1 Other Asian   288\n\n\n\nDefine a new race variable that has as few NA and Other as possible.\n\n\ndat &lt;- NHANES %&gt;% mutate(Race = Race3) |&gt;\n  mutate(Race = if_else(is.na(Race), Race1, Race))\n\n\nCompute proportion of individuals that smoked at the time of the survey, by race category and gender. Keep track of how many people answered the question. Order the result by the number that answered. Read the help file for NHANES carefully before doing this one. To be clear: what proportion of people who have smoked at any time in their life are smoking now.\n\n\ndat |&gt; group_by(Gender, Race) |&gt; \n  summarize(n = sum(!is.na(Smoke100)), \n            smoke = sum(SmokeNow == \"Yes\", na.rm = TRUE)) |&gt;\n  mutate(smoke = smoke/n) |&gt;\n  arrange(desc(n))\n\n`summarise()` has grouped output by 'Gender'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 12 × 4\n# Groups:   Gender [2]\n   Gender Race         n  smoke\n   &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;  &lt;dbl&gt;\n 1 female White     2472 0.185 \n 2 male   White     2377 0.219 \n 3 female Black      442 0.204 \n 4 male   Black      379 0.301 \n 5 male   Mexican    339 0.212 \n 6 female Mexican    262 0.111 \n 7 female Hispanic   218 0.0963\n 8 male   Hispanic   198 0.278 \n 9 female Other      177 0.203 \n10 male   Other      162 0.309 \n11 female Asian      112 0.0357\n12 male   Asian       97 0.175 \n\n\n\nCreate a new dataset that combines the Mexican and Hispanic, and removes the Other category. Hint: use the function forcats::fct_collapse().\n\n\ndat &lt;- dat |&gt; \n  mutate(Race = forcats::fct_collapse(Race, Hispanic = c(\"Hispanic\", \"Mexican\"))) |&gt;\n  filter(Race != \"Other\") |&gt;\n  mutate(Race = droplevels(Race))\n\n\nRecompute proportion of individuals that smoke now by race category and gender. Order by rate of smokers.\n\n\ndat |&gt; group_by(Gender, Race) |&gt; \n  summarize(n = sum(!is.na(Smoke100)), smoke = sum(SmokeNow == \"Yes\", na.rm = TRUE)) |&gt;\n  mutate(smoke = smoke/n) |&gt;\n  arrange(desc(smoke))\n\n`summarise()` has grouped output by 'Gender'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   Gender [2]\n  Gender Race         n  smoke\n  &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 male   Black      379 0.301 \n2 male   Hispanic   537 0.236 \n3 male   White     2377 0.219 \n4 female Black      442 0.204 \n5 female White     2472 0.185 \n6 male   Asian       97 0.175 \n7 female Hispanic   480 0.104 \n8 female Asian      112 0.0357\n\n\n\nCompute the median age by race category and gender order by Age.\n\n\ndat |&gt; group_by(Gender, Race) |&gt;\n  summarize(Age = median(Age)) |&gt;\n  arrange(Gender, Age)\n\n`summarise()` has grouped output by 'Gender'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   Gender [2]\n  Gender Race       Age\n  &lt;fct&gt;  &lt;fct&gt;    &lt;dbl&gt;\n1 female Hispanic    27\n2 female Black       33\n3 female Asian       37\n4 female White       42\n5 male   Hispanic    28\n6 male   Black       30\n7 male   Asian       31\n8 male   White       40\n\n\n\nNow redo the smoking rate calculation by age group. But first, remove individuals with no group and remove any age groups for which less than 10 people answered the question. Within each age group and Gender order by percent that smokes.\n\n\nres &lt;- dat |&gt; \n  filter(!is.na(AgeDecade)) |&gt;\n  group_by(AgeDecade) |&gt; \n  mutate(n = sum(!is.na(Smoke100))) |&gt; \n  ungroup() |&gt;\n  filter(n &gt;= 10) |&gt;\n  group_by(AgeDecade, Gender, Race) |&gt;\n  summarize(n = sum(!is.na(Smoke100)), \n            smoke = sum(SmokeNow == \"Yes\", na.rm = TRUE), \n            .groups = \"drop\") |&gt; ## This is similar to running ungroup() in a next step\n  mutate(smoke = smoke/n) |&gt;\n  arrange(AgeDecade, Gender, desc(smoke))\n\n## Bonus: a plot\nres |&gt; ggplot(aes(AgeDecade, smoke, color = Race)) +\n  geom_point() + \n  geom_line() + \n  facet_wrap(~Gender)\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?"
  },
  {
    "objectID": "07-dates-and-times.html#the-date-data-type",
    "href": "07-dates-and-times.html#the-date-data-type",
    "title": "7  Dates and times",
    "section": "7.1 The date data type",
    "text": "7.1 The date data type\nWe have described three main types of vectors: numeric, character, and logical. When analyzing data, we often encounter variables that are dates. Although we can represent a date with a string, for example December 03, 2023`, once we pick a reference day, referred to as the epoch by computer programmers, they can be converted to numbers by calculating the number of days since the epoch. In R and Unix, the epoch is defined as January 1, 1970. So, for example, January 2, 1970 is day 1, December 31, 1969 is day -1, and so on.\n\nx &lt;- as.Date(\"1970-01-01\")\ntypeof(x)\n\n[1] \"double\"\n\nclass(x)\n\n[1] \"Date\"\n\nas.numeric(x)\n\n[1] 0\n\n\n\nx &lt;- Sys.Date()\nas.numeric(x)\n\n[1] 19675\n\n\nThe date class let’s R know that it is date so you can extract year, months, days of the week etc…\nYou can make them look good using the format function:\n\nformat(x, \"%B %d, %Y\")\n\n[1] \"November 14, 2023\"\n\n\nThere are many formats:\n\nformat(x, \"%b %d %y\")\n\n[1] \"Nov 14 23\"\n\n\nTo see all the possibilities you can consult the data and time formats cheat sheet"
  },
  {
    "objectID": "07-dates-and-times.html#predefined-objects",
    "href": "07-dates-and-times.html#predefined-objects",
    "title": "7  Dates and times",
    "section": "7.2 Predefined objects",
    "text": "7.2 Predefined objects\n\nmonth.name\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\nmonth.abb\n\n [1] \"Jan\" \"Feb\" \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\""
  },
  {
    "objectID": "07-dates-and-times.html#sec-lubridate",
    "href": "07-dates-and-times.html#sec-lubridate",
    "title": "7  Dates and times",
    "section": "7.3 The lubridate package",
    "text": "7.3 The lubridate package\nThe lubridate package provides tools to work with date and times.\n\nlibrary(lubridate)\n\nAn example of the many useful functions is as_date\n\nas_date(0)\n\n[1] \"1970-01-01\"\n\n\nAnother one is\n\ntoday()\n\n[1] \"2023-11-14\"\n\n\nWe can generate random dates like this:\n\nset.seed(2013 - 9 - 10)\nn &lt;- 10 \ndates &lt;- as_date(sample(0:as.numeric(today()), n, replace = TRUE))\n\nThe functions year, month and day extract those values:\n\ndata.frame(date = dates, month = month(dates), day = day(dates), year = year(dates))\n\n         date month day year\n1  1987-10-06    10   6 1987\n2  2020-10-02    10   2 2020\n3  1990-05-23     5  23 1990\n4  2009-03-04     3   4 2009\n5  1977-01-30     1  30 1977\n6  1986-03-06     3   6 1986\n7  2022-02-02     2   2 2022\n8  1977-07-29     7  29 1977\n9  2022-10-11    10  11 2022\n10 2007-11-08    11   8 2007\n\n\nWe can also extract the month labels:\n\nmonth(dates, label = TRUE)\n\n [1] Oct Oct May Mar Jan Mar Feb Jul Oct Nov\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\n\nAnother useful set of functions are the parsers that convert strings into dates. The function ymd assumes the dates are in the format YYYY-MM-DD and tries to parse as well as possible.\n\nx &lt;- c(20090101, \"2009-01-02\", \"2009 01 03\", \"2009-1-4\",\n       \"2009-1, 5\", \"Created on 2009 1 6\", \"200901 !!! 07\")\nymd(x)\n\n[1] \"2009-01-01\" \"2009-01-02\" \"2009-01-03\" \"2009-01-04\" \"2009-01-05\"\n[6] \"2009-01-06\" \"2009-01-07\"\n\n\nA further complication comes from the fact that dates often come in different formats in which the order of year, month, and day are different. The preferred format is to show year (with all four digits), month (two digits), and then day, or what is called the ISO 8601. Specifically we use YYYY-MM-DD so that if we order the string, it will be ordered by date. You can see the function ymd returns them in this format.\nBut, what if you encounter dates such as “09/01/02”? This could be September 1, 2002 or January 2, 2009 or January 9, 2002. lubridate provides options:\n\nx &lt;- \"09/01/02\"\nymd(x)\n\n[1] \"2009-01-02\"\n\nmdy(x)\n\n[1] \"2002-09-01\"\n\ndmy(x)\n\n[1] \"2002-01-09\"\n\n\nThe lubridate package is also useful for dealing with times:\n\nnow()\n\n[1] \"2023-11-14 20:47:49 EST\"\n\n\nYou can provide time zones too:\n\nnow(\"GMT\")\n\n[1] \"2023-11-15 01:47:49 GMT\"\n\n\nYou can see all the available time zones with OlsonNames() function.\nWe can extract hours, minutes, and seconds:\n\nnow() |&gt; hour()\n\n[1] 20\n\nnow() |&gt; minute()\n\n[1] 47\n\nnow() |&gt; second()\n\n[1] 49.24163\n\n\nThe package also includes a function to parse strings into times as well as parsers for time objects that include dates:\n\nx &lt;- c(\"12:34:56\")\nhms(x)\n\n[1] \"12H 34M 56S\"\n\nx &lt;- \"Nov/2/2012 12:34:56\"\nmdy_hms(x)\n\n[1] \"2012-11-02 12:34:56 UTC\""
  },
  {
    "objectID": "07-dates-and-times.html#sequences",
    "href": "07-dates-and-times.html#sequences",
    "title": "7  Dates and times",
    "section": "7.4 Sequences",
    "text": "7.4 Sequences\n\nx &lt;- seq(today(), today() + 7, by = \"days\")"
  },
  {
    "objectID": "07-dates-and-times.html#rounding",
    "href": "07-dates-and-times.html#rounding",
    "title": "7  Dates and times",
    "section": "7.5 Rounding",
    "text": "7.5 Rounding\n\nx &lt;- seq(today() - 365 + 1, today(), by = \"days\")\ntable(floor_date(x, unit = \"week\"))\n\n\n2022-11-13 2022-11-20 2022-11-27 2022-12-04 2022-12-11 2022-12-18 2022-12-25 \n         5          7          7          7          7          7          7 \n2023-01-01 2023-01-08 2023-01-15 2023-01-22 2023-01-29 2023-02-05 2023-02-12 \n         7          7          7          7          7          7          7 \n2023-02-19 2023-02-26 2023-03-05 2023-03-12 2023-03-19 2023-03-26 2023-04-02 \n         7          7          7          7          7          7          7 \n2023-04-09 2023-04-16 2023-04-23 2023-04-30 2023-05-07 2023-05-14 2023-05-21 \n         7          7          7          7          7          7          7 \n2023-05-28 2023-06-04 2023-06-11 2023-06-18 2023-06-25 2023-07-02 2023-07-09 \n         7          7          7          7          7          7          7 \n2023-07-16 2023-07-23 2023-07-30 2023-08-06 2023-08-13 2023-08-20 2023-08-27 \n         7          7          7          7          7          7          7 \n2023-09-03 2023-09-10 2023-09-17 2023-09-24 2023-10-01 2023-10-08 2023-10-15 \n         7          7          7          7          7          7          7 \n2023-10-22 2023-10-29 2023-11-05 2023-11-12 \n         7          7          7          3 \n\ntable(floor_date(x, unit = \"year\"))\n\n\n2022-01-01 2023-01-01 \n        47        318 \n\n\nWhat if I want to start counting on Mondays?\n\nx &lt;- seq(today() - weeks(1) + 1, today(), by = \"days\")\nwday(x)\n\n[1] 4 5 6 7 1 2 3\n\ndata.frame(day = x, week = floor_date(x, unit = \"week\", week_start = \"Sun\"))\n\n         day       week\n1 2023-11-08 2023-11-05\n2 2023-11-09 2023-11-05\n3 2023-11-10 2023-11-05\n4 2023-11-11 2023-11-05\n5 2023-11-12 2023-11-12\n6 2023-11-13 2023-11-12\n7 2023-11-14 2023-11-12"
  },
  {
    "objectID": "07-dates-and-times.html#day-of-the-year-or-month",
    "href": "07-dates-and-times.html#day-of-the-year-or-month",
    "title": "7  Dates and times",
    "section": "7.6 day of the year or month",
    "text": "7.6 day of the year or month\n\nyday(x)\n\n[1] 312 313 314 315 316 317 318\n\nmday(x)\n\n[1]  8  9 10 11 12 13 14"
  },
  {
    "objectID": "07-dates-and-times.html#exercises",
    "href": "07-dates-and-times.html#exercises",
    "title": "7  Dates and times",
    "section": "7.7 Exercises",
    "text": "7.7 Exercises\nIn the previous exercise section, we wrangled data from a PDF file containing vital statistics from Puerto Rico. We did this for the month of September. Below we include code that does it for all 12 months.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr   1.1.1     ✔ readr   2.1.4\n✔ forcats 1.0.0     ✔ stringr 1.5.0\n✔ ggplot2 3.4.2     ✔ tibble  3.2.1\n✔ purrr   1.0.1     ✔ tidyr   1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(purrr)\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nlibrary(dslabs)\n\nfn &lt;- system.file(\"extdata\", \"RD-Mortality-Report_2015-18-180531.pdf\",\n                  package=\"dslabs\")\ndat &lt;- map_df(str_split(pdf_text(fn), \"\\n\"), function(s){\n  s &lt;- str_trim(s)\n  header_index &lt;- str_which(s, \"2015\")[1]\n  tmp &lt;- str_split(s[header_index], \"\\\\s+\", simplify = TRUE)\n  month &lt;- tmp[1]\n  header &lt;- tmp[-1]\n  tail_index  &lt;- str_which(s, \"Total\")\n  n &lt;- str_count(s, \"\\\\d+\")\n  out &lt;- c(1:header_index, which(n == 1), \n           which(n &gt;= 28), tail_index:length(s))\n  res &lt;- s[-out] |&gt;  str_remove_all(\"[^\\\\d\\\\s]\") |&gt; str_trim() |&gt;\n    str_split_fixed(\"\\\\s+\", n = 6) \n  res &lt;- data.frame(res[,1:5]) |&gt; as_tibble() |&gt; \n    setNames(c(\"day\", header)) |&gt;\n    mutate(month = month, day = as.numeric(day)) |&gt;\n    pivot_longer(-c(day, month), names_to = \"year\", values_to = \"deaths\") |&gt;\n    mutate(deaths = as.numeric(deaths)) |&gt;\n    mutate(month = str_to_title(month)) |&gt;\n    mutate(month = if_else(month==\"Ago\", \"Aug\", month))\n}) \n\n\nMake sure that year is a number.\n\n\ndat &lt;- mutate(dat, year = as.numeric(year))\n\n\nWe want to make a plot of death counts versus date. A first step is to convert the month variable from characters to numbers. Hint: use month.abb.\n\n\ndat &lt;- dat |&gt; mutate(month = match(month, month.abb))\n\n\nCreate a new column date with the date for each observation. Hint: use the make_date function.\n\n\ndat &lt;- dat |&gt; mutate(date = make_date(year, month, day))\n\n\nPlot deaths versus date. Hint: the plot function can take dates for either axis.\n\n\nwith(dat, plot(date, deaths))\n\n\n\n\n\nNote that after May 31, 2018, the deaths are all 0. The data is probably not entered yet. We also see a drop off starting around May 1. Redefine dat to exclude observations taken on or after May 1, 2018. Then, remake the plot.\n\n\ndat &lt;- dat |&gt; filter(date &lt; make_date(2018, 5, 1))\nwith(dat, plot(date, deaths))\n\n\n\n\n\nRepeat the plot but use the day of the year on the x-axis instead of date and different colors for the different year. Hint: Use the col argument in plot.\n\n\nwith(dat, plot(yday(date), deaths, col = year - min(year) + 1))\n\n\n\n\n\nCompute the number deaths per day by month.\n\n\nres &lt;- dat |&gt; group_by(date = floor_date(date, unit = \"month\")) |&gt;\n  summarize(mean(deaths))\n\n\nShow the deaths per day for July and for September. What do you notice?\n\n\nres |&gt; filter(month(date) %in% c(7,9)) |&gt;\n  mutate(month = month(date), year = year(date)) |&gt;\n  arrange(month, year)\n\n# A tibble: 6 × 4\n  date       `mean(deaths)` month  year\n  &lt;date&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2015-07-01           76.8     7  2015\n2 2016-07-01           79.2     7  2016\n3 2017-07-01           76.4     7  2017\n4 2015-09-01           75.1     9  2015\n5 2016-09-01           78.8     9  2016\n6 2017-09-01           98.4     9  2017\n\n\nSeptember 2017 is an outlier.\n\nCompute deaths per week and make a plot.\n\n\nres &lt;- dat |&gt; group_by(date = floor_date(date, unit = \"week\")) |&gt;\n  summarize(deaths = mean(deaths))\nwith(res, plot(date, deaths))"
  },
  {
    "objectID": "08-importing-data.html#r-base-functions",
    "href": "08-importing-data.html#r-base-functions",
    "title": "8  Importing data",
    "section": "8.1 R base functions",
    "text": "8.1 R base functions\nWe include example data files for practice in the dslabs package. They are stored here:\n\ndir &lt;- system.file(\"extdata\", package = \"dslabs\")\n\nTake a look:\n\nlist.files(dir)\n\n [1] \"2010_bigfive_regents.xls\"                               \n [2] \"calificaciones.csv\"                                     \n [3] \"carbon_emissions.csv\"                                   \n [4] \"fertility-two-countries-example.csv\"                    \n [5] \"HRlist2.txt\"                                            \n [6] \"life-expectancy-and-fertility-two-countries-example.csv\"\n [7] \"murders.csv\"                                            \n [8] \"olive.csv\"                                              \n [9] \"RD-Mortality-Report_2015-18-180531.pdf\"                 \n[10] \"ssa-death-probability.csv\"                              \n\n\nCopy one of them to your working directory:\n\nfile_path &lt;- file.path(dir, \"murders.csv\")\nfile.copy(file_path, \"murders.csv\")\n\n[1] TRUE\n\n\nThe file.path function combines characters to form a complete path, ensuring compatibility with the respective operating system. Linux and Mac use forward slashes /, while Windows uses backslashes \\, to separate directories. This function is useful because often you want to define paths using a variable.\nThe file.copy function copies a file and returns TRUE if succesful. If the file exists it will not copy.\nWhat kind of file is it? Although the suffix usually tells us what type of file it is, there is no guarantee that these always match.\n\nreadLines(\"murders.csv\", n = 3)\n\n[1] \"state,abb,region,population,total\" \"Alabama,AL,South,4779736,135\"     \n[3] \"Alaska,AK,West,710231,19\"         \n\n\nIt is comma delimited and has a header. You can import it like this:\n\ndat &lt;- read.csv(\"murders.csv\")\n\nThere are other importing function in base R: read.table, read.csv and read.delim, for example."
  },
  {
    "objectID": "08-importing-data.html#the-readr-and-readxl-packages",
    "href": "08-importing-data.html#the-readr-and-readxl-packages",
    "title": "8  Importing data",
    "section": "8.2 The readr and readxl packages",
    "text": "8.2 The readr and readxl packages\nTidyverse has improved versions of functions for importing data.\n\n8.2.1 readr\nThe readr package includes functions for reading data stored in text file spreadsheets into R. readr is part of the tidyverse package, but you can load it directly using:\n\nlibrary(readr)\n\nThe following functions are available to read-in spreadsheets:\n\n\n\n\n\n\n\n\nFunction\nFormat\nTypical suffix\n\n\n\n\nread_table\nwhite space separated values\ntxt\n\n\nread_csv\ncomma separated values\ncsv\n\n\nread_csv2\nsemicolon separated values\ncsv\n\n\nread_tsv\ntab delimited separated values\ntsv\n\n\nread_delim\ngeneral text file format, must define delimiter\ntxt\n\n\n\nthe readr equivalent of readLines is read_lines:\n\nread_lines(\"murders.csv\", n_max = 3)\n\n[1] \"state,abb,region,population,total\" \"Alabama,AL,South,4779736,135\"     \n[3] \"Alaska,AK,West,710231,19\"         \n\n\nFrom the .csv suffix and the peek at the file, we know to use read_csv:\n\ndat &lt;- read_csv(\"murders.csv\")\n\nRows: 51 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): state, abb, region\ndbl (2): population, total\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote that we receive a message letting us know what data types were used for each column. Also note that dat is a tibble, not just a data frame. This is because read_csv is a tidyverse parser.\nA powerful added feature of read_csv is the col_type arguments that let’s you specify the data type of each column before reading. This can help with parsing dates or not letting an error like a letter in a column of numbers turn everything into a character.\n\n\n8.2.2 readxl\nMany spreadsheets are saved in Microsoft Excel format. For this we use parsers in the readxl package:\n\nlibrary(readxl)\n\nThe package provides functions to read-in Microsoft Excel formats:\n\n\n\n\n\n\n\n\nFunction\nFormat\nTypical suffix\n\n\n\n\nread_excel\nauto detect the format\nxls, xlsx\n\n\nread_xls\noriginal format\nxls\n\n\nread_xlsx\nnew format\nxlsx\n\n\n\nThe Microsoft Excel formats permit you to have more than one spreadsheet in one file. These are referred to as sheets. The functions listed above read the first sheet by default, but we can also read the others. The excel_sheets function gives us the names of all the sheets in an Excel file. These names can then be passed to the sheet argument in the three functions above to read sheets other than the first."
  },
  {
    "objectID": "08-importing-data.html#downloading-files",
    "href": "08-importing-data.html#downloading-files",
    "title": "8  Importing data",
    "section": "8.3 Downloading files",
    "text": "8.3 Downloading files\nA common place for data to reside is on the internet. When these data are in files, we can download them and then import them or even read them directly from the web.\n\nurl &lt;- \n  \"https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv\"\n\nThe read_csv file can read these files directly:\n\ndat &lt;- read_csv(url)\n\nYou can also download the file first using download.file or the Unix commands curl or wget.\n\ntmp_filename &lt;- tempfile()\ndownload.file(url, tmp_filename)\ndat &lt;- read_csv(tmp_filename)\n\nRows: 51 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): state, abb, region\ndbl (2): population, total\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfile.remove(tmp_filename)\n\n[1] TRUE"
  },
  {
    "objectID": "08-importing-data.html#encoding",
    "href": "08-importing-data.html#encoding",
    "title": "8  Importing data",
    "section": "8.4 Encoding",
    "text": "8.4 Encoding\nRStudio assumes the Unicode encoding. A common pitfall in data analysis is assuming a file is Unicode when, in fact, it is something else.\nTo understand encoding, remember that everything on a computer needs to eventually be converted to 0s and 1s. ASCII is an encoding that maps characters to numbers. ASCII uses 7 bits (0s and 1s) which results in \\(2^7 = 128\\) unique items, enough to encode all the characters on an English language keyboard. However, other languages use characters not included in this encoding. For example, the é in México is not encoded by ASCII. For this reason, a new encoding, using more than 7 bits, was defined: Unicode. When using Unicode, one can chose between 8, 16, and 32 bits abbreviated UTF-8, UTF-16, and UTF-32 respectively. RStudio defaults to UTF-8 encoding. ASCII is a subset of UTF-8.\nTry reading in this file:\n\nurl &lt;- \"https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/calificaciones.csv\"\nreadLines(url, n = 2)\n\n[1] \"\\\"nombre\\\",\\\"f.n.\\\",\\\"estampa\\\",\\\"puntuaci\\xf3n\\\"\"                       \n[2] \"\\\"Beyonc\\xe9\\\",\\\"04 de septiembre de 1981\\\",2023-09-22 02:11:02,\\\"87,5\\\"\"\n\n\nWhen you see these weird characters the problem is almost always that you are assuming the wrong encoding. You need to be a hacker to figure out, readr has a function that tries:\n\nguess_encoding(url)\n\n# A tibble: 3 × 2\n  encoding   confidence\n  &lt;chr&gt;           &lt;dbl&gt;\n1 ISO-8859-1       0.92\n2 ISO-8859-2       0.72\n3 ISO-8859-9       0.53\n\n\nThe first guess makes sense as Spanish is often saved using Latin-1 encoding, also known as ISO-8859 encoding because it was the first to include accents and other characters used in Spanish. Once we figure this out we can read in the file correctly:\n\nread_csv(url, locale = locale(encoding = \"ISO-8859-1\", decimal_mark = \",\"))\n\nRows: 7 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): nombre, f.n.\ndbl  (1): puntuación\ndttm (1): estampa\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 7 × 4\n  nombre   f.n.                     estampa             puntuación\n  &lt;chr&gt;    &lt;chr&gt;                    &lt;dttm&gt;                   &lt;dbl&gt;\n1 Beyoncé  04 de septiembre de 1981 2023-09-22 02:11:02       87.5\n2 Blümchen 20 de abril de 1980      2023-09-22 03:23:05       99  \n3 João     10 de junio de 1931      2023-09-21 22:43:28       98.9\n4 López    24 de julio de 1969      2023-09-22 01:06:59       88.7\n5 Ñengo    15 de diciembre de 1981  2023-09-21 23:35:37       93.1\n6 Plácido  24 de enero de 1941      2023-09-21 23:17:21       88.7\n7 Thalía   26 de agosto de 1971     2023-09-21 23:08:02       83  \n\n\n\n\n[1] TRUE"
  },
  {
    "objectID": "08-importing-data.html#exercises",
    "href": "08-importing-data.html#exercises",
    "title": "8  Importing data",
    "section": "8.5 Exercises",
    "text": "8.5 Exercises\n\nUse the read_csv function to read each of the csv files that the following code saves in the files object. Hint: use the pattern in list.files to keep only the csv files.\n\n\nlibrary(readr)\npath &lt;- system.file(\"extdata\", package = \"dslabs\")\nfiles &lt;- list.files(path, pattern = \".csv\")\nres &lt;- lapply(files, function(fn) \n  read_csv(file.path(path, fn), show_col_types = FALSE))\n\nNew names:\n• `` -&gt; `...1`\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n\nNote that you get a warning. To see which one you can run it one-by-one in a loop:\n\n\nfor (i in seq_along(files)) {\n  print(files[i])\n  read_csv(file.path(path, files[i]), show_col_types = FALSE)\n}\n\n[1] \"calificaciones.csv\"\n[1] \"carbon_emissions.csv\"\n[1] \"fertility-two-countries-example.csv\"\n[1] \"life-expectancy-and-fertility-two-countries-example.csv\"\n[1] \"murders.csv\"\n[1] \"olive.csv\"\n\n\nNew names:\n• `` -&gt; `...1`\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n[1] \"ssa-death-probability.csv\"\n\n\nolive.csv gives us a New names warning. This is because the first line of the file is missing the header for the first column.\n\nread_lines(file.path(path, \"olive.csv\"), n_max = 2)\n\n[1] \",Region,Area,palmitic,palmitoleic,stearic,oleic,linoleic,linolenic,arachidic,eicosenoic\"\n[2] \"1,North-Apulia,1,1,1075,75,226,7823,672,36,60,29\"                                       \n\n\nRead the help file for read_csv to figure out how to read in the file without reading this header. If you skip the header, you should not get this warning. Save the result to an object called dat.\n\nread_csv(file.path(path, \"olive.csv\"), col_names = FALSE, skip = 1)\n\nRows: 572 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): X2\ndbl (11): X1, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 572 × 12\n      X1 X2             X3    X4    X5    X6    X7    X8    X9   X10   X11   X12\n   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 North-Apul…     1     1  1075    75   226  7823   672    36    60    29\n 2     2 North-Apul…     1     1  1088    73   224  7709   781    31    61    29\n 3     3 North-Apul…     1     1   911    54   246  8113   549    31    63    29\n 4     4 North-Apul…     1     1   966    57   240  7952   619    50    78    35\n 5     5 North-Apul…     1     1  1051    67   259  7771   672    50    80    46\n 6     6 North-Apul…     1     1   911    49   268  7924   678    51    70    44\n 7     7 North-Apul…     1     1   922    66   264  7990   618    49    56    29\n 8     8 North-Apul…     1     1  1100    61   235  7728   734    39    64    35\n 9     9 North-Apul…     1     1  1082    60   239  7745   709    46    83    33\n10    10 North-Apul…     1     1  1037    55   213  7944   633    26    52    30\n# ℹ 562 more rows\n\n\n\nA problem with the previous approach is that we don’t know what the columns represent. Type names(dat) to see that the names are not informative. Use the read_lines with argument n_max=1 to read just the first line.\n\n\nread_lines(file.path(path, \"olive.csv\"), n_max = 1)\n\n[1] \",Region,Area,palmitic,palmitoleic,stearic,oleic,linoleic,linolenic,arachidic,eicosenoic\"\n\n\nNotice that you can use this to assign names to the data frame.\n\ncolnames &lt;- read_lines(file.path(path, \"olive.csv\"), n_max = 1) \ncolnames &lt;- strsplit(colnames, \",\") |&gt; unlist()\ncolnames[1] &lt;- \"row_number\"\nnames(dat) &lt;- colnames\n\nWarning: The `value` argument of `names&lt;-` must have the same length as `x` as of tibble\n3.0.0."
  },
  {
    "objectID": "09-data-table.html#manipulating-data-tables",
    "href": "09-data-table.html#manipulating-data-tables",
    "title": "9  data.table",
    "section": "9.1 Manipulating data tables",
    "text": "9.1 Manipulating data tables\ndata.table is a separate package that needs to be installed. Once installed, we then need to load it:\n\nlibrary(data.table)\n\nWe will provide example code showing the data.table approaches to dplyr’s mutate, filter, select, group_by, and summarize shown in the tidyverse chapter. As in that chapter, we will use the murders dataset:\nThe first step when using data.table is to convert the data frame into a data.table object using the as.data.table function:\n\nmurders_dt &lt;- as.data.table(murders)\n\nWithout this initial step, most of the approaches shown below will not work.\n\n9.1.1 Selecting\nSelecting with data.table is done in a similar way to subsetting matrices. While with dplyr we write:\n\nselect(murders, state, region)\n\nIn data.table, we use notation similar to what is used with matrices:\n\nmurders_dt[, c(\"state\", \"region\")] |&gt; head()\n\n        state region\n1:    Alabama  South\n2:     Alaska   West\n3:    Arizona   West\n4:   Arkansas  South\n5: California   West\n6:   Colorado   West\n\n\nWe can also use the .() data.table notation to alert R that variables inside the parenthesis are column names, not objects in the R environment. So the above can also be written like this:\n\nmurders_dt[, .(state, region)] |&gt; head()\n\n        state region\n1:    Alabama  South\n2:     Alaska   West\n3:    Arizona   West\n4:   Arkansas  South\n5: California   West\n6:   Colorado   West\n\n\n\n\n9.1.2 Adding a column or changing columns\nWe learned to use the dplyr mutate function with this example:\n\nmurders &lt;- mutate(murders, rate = total / population * 100000)\n\ndata.table uses an approach that avoids a new assignment (update by reference). This can help with large datasets that take up most of your computer’s memory. The data.table :=` function permits us to do this:\n\nmurders_dt[, rate := total / population * 100000]\n\nThis adds a new column, rate, to the table. Notice that, as in dplyr, we used total and population without quotes.\nWe can see that the new column is added:\n\nhead(murders_dt)\n\n        state abb region population total     rate\n1:    Alabama  AL  South    4779736   135 2.824424\n2:     Alaska  AK   West     710231    19 2.675186\n3:    Arizona  AZ   West    6392017   232 3.629527\n4:   Arkansas  AR  South    2915918    93 3.189390\n5: California  CA   West   37253956  1257 3.374138\n6:   Colorado  CO   West    5029196    65 1.292453\n\n\nTo define new multiple columns, we can use the := function with multiple arguments:\n\nmurders_dt[, \":=\"(rate = total / population * 100000, rank = rank(population))]\n\n\n\n9.1.3 Technical detail: reference versus copy\nThe data.table package is designed to avoid wasting memory. So if you make a copy of a table, like this:\n\nx &lt;- data.table(a = 1)\ny &lt;- x\n\ny is actually referencing x, it is not an new opject: it’s just another name for x. Until you change y, a new object will not be made. However, the := function changes by reference so if you change x, a new object is not made and y continues to be just another name for x:\n\nx[,a := 2]\ny\n\n   a\n1: 2\n\n\nYou can also change x like this:\n\ny[,a := 1]\nx\n\n   a\n1: 1\n\n\nTo avoid this, you can use the copy function which forces the creation of an actual copy:\n\nx &lt;- data.table(a = 1)\ny &lt;- copy(x)\nx[,a := 2]\ny\n\n   a\n1: 1\n\n\nNote that the function as.data.table creates a copy of the data frame being converted. However, if working with a large data frames it is helpful to avoid this, and you can do this by using setDT.\n\nx &lt;- data.frame(a = 1)\nsetDT(x)\n\nHowever, note that because no copy is being made, be aware that the following code does not create a new object:\n\nx &lt;- data.frame(a = 1)\ny &lt;- setDT(x)\n\nThe objects x and y are referencing the same data table:\n\nx[,a := 2]\ny\n\n   a\n1: 2\n\n\n\n\n9.1.4 Subsetting\nWith dplyr, we filtered like this:\n\nfilter(murders, rate &lt;= 0.7)\n\nWith data.table, we again use an approach similar to subsetting matrices, except data.table knows that rate refers to a column name and not an object in the R environment:\n\nmurders_dt[rate &lt;= 0.7]\n\n           state abb        region population total      rate rank\n1:        Hawaii  HI          West    1360301     7 0.5145920   12\n2:          Iowa  IA North Central    3046355    21 0.6893484   22\n3: New Hampshire  NH     Northeast    1316470     5 0.3798036   10\n4:  North Dakota  ND North Central     672591     4 0.5947151    4\n5:       Vermont  VT     Northeast     625741     2 0.3196211    3\n\n\nNotice that we can combine the filter and select into one succint command. Here are the state names and rates for those with rates below 0.7.\n\nmurders_dt[rate &lt;= 0.7, .(state, rate)]\n\n           state      rate\n1:        Hawaii 0.5145920\n2:          Iowa 0.6893484\n3: New Hampshire 0.3798036\n4:  North Dakota 0.5947151\n5:       Vermont 0.3196211\n\n\nCompare to the dplyr approach:\n\nmurders |&gt; filter(rate &lt;= 0.7) |&gt; select(state, rate)"
  },
  {
    "objectID": "09-data-table.html#summarizing-data",
    "href": "09-data-table.html#summarizing-data",
    "title": "9  data.table",
    "section": "9.2 Summarizing data",
    "text": "9.2 Summarizing data\nAs an example, we will use the heights dataset:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:data.table':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(dslabs)\nheights_dt &lt;- as.data.table(heights)\n\nIn data.table, we can call functions inside .() and they will be applied to rows. So the equivalent of:\n\ns &lt;- heights |&gt; \n  summarize(average = mean(height), standard_deviation = sd(height))\n\nin dplyr is the following in data.table:\n\ns &lt;- heights_dt[, .(average = mean(height), standard_deviation = sd(height))]\n\nNote that this permits a compact way of subsetting and then summarizing. Instead of:\n\ns &lt;- heights |&gt; \n  filter(sex == \"Female\") |&gt;\n  summarize(average = mean(height), standard_deviation = sd(height))\n\nwe can write:\n\ns &lt;- heights_dt[sex == \"Female\", .(average = mean(height), standard_deviation = sd(height))]\n\n\n9.2.1 Multiple summaries\nWe previously defined the function:\n\nmedian_min_max &lt;- function(x){\n  qs &lt;- quantile(x, c(0.5, 0, 1))\n  data.frame(median = qs[1], minimum = qs[2], maximum = qs[3])\n}\n\nSimilar to dplyr, we can call this function within .() to obtain the three number summary:\n\nheights_dt[, .(median_min_max(height))]\n\n   median minimum  maximum\n1:   68.5      50 82.67717\n\n\n\n\n9.2.2 Group then summarize\nThe group_by followed by summarize in dplyr is performed in one line in data.table. We simply add the by argument to split the data into groups based on the values in categorical variable:\n\nheights_dt[, .(average = mean(height), standard_deviation = sd(height)), by = sex]\n\n      sex  average standard_deviation\n1:   Male 69.31475           3.611024\n2: Female 64.93942           3.760656"
  },
  {
    "objectID": "09-data-table.html#sorting-data-frames",
    "href": "09-data-table.html#sorting-data-frames",
    "title": "9  data.table",
    "section": "9.3 Sorting data frames",
    "text": "9.3 Sorting data frames\nWe can order rows using the same approach we use for filter. Here are the states ordered by murder rate:\n\nmurders_dt[order(population)] |&gt; head()\n\n                  state abb        region population total       rate rank\n1:              Wyoming  WY          West     563626     5  0.8871131    1\n2: District of Columbia  DC         South     601723    99 16.4527532    2\n3:              Vermont  VT     Northeast     625741     2  0.3196211    3\n4:         North Dakota  ND North Central     672591     4  0.5947151    4\n5:               Alaska  AK          West     710231    19  2.6751860    5\n6:         South Dakota  SD North Central     814180     8  0.9825837    6\n\n\nN To sort the table in descending order, we can order by the negative of population or use the decreasing argument:\n\nmurders_dt[order(population, decreasing = TRUE)] \n\n\n9.3.1 Nested sorting\nSimilarly, we can perform nested ordering by including more than one variable in order\n\nmurders_dt[order(region, rate)]"
  },
  {
    "objectID": "09-data-table.html#optional-exercises-will-not-be-included-in-the-midterms",
    "href": "09-data-table.html#optional-exercises-will-not-be-included-in-the-midterms",
    "title": "9  data.table",
    "section": "9.4 Optional exercises (will not be included in the midterms)",
    "text": "9.4 Optional exercises (will not be included in the midterms)\n\nLoad the data.table package and the murders dataset and convert it to data.table object:\n\n\nlibrary(data.table)\nlibrary(dslabs)\nmurders_dt &lt;- as.data.table(murders)\n\nRemember you can add columns like this:\n\nmurders_dt[, population_in_millions := population / 10^6]\n\nAdd a murders column named rate with the per 100,000 murder rate as in the example code above.\n\nAdd a column rank containing the rank, from highest to lowest murder rate.\nIf we want to only show the states and population sizes, we can use:\n\n\nmurders_dt[, .(state, population)] \n\nShow the state names and abbreviations in murders.\n\nYou can show just the New York row like this:\n\n\nmurders_dt[state == \"New York\"]\n\nYou can use other logical vectors to filter rows.\nShow the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\n\nWe can remove rows using the != operator. For example, to remove Florida, we would do this:\n\n\nno_florida &lt;- murders_dt[state != \"Florida\"]\n\nCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\n\nWe can also use %in% to filter. You can therefore see the data from New York and Texas as follows:\n\n\nmurders_dt[state %in% c(\"New York\", \"Texas\")]\n\nCreate a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\n\nSuppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.\n\n\nmurders_dt[population &lt; 5000000 & region == \"Northeast\"]\n\nMake sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: they are in the Northeast or West and the murder rate is less than 1. Show only the state name, the rate, and the rank."
  },
  {
    "objectID": "pset1.html#measles",
    "href": "pset1.html#measles",
    "title": "Problem Set 1",
    "section": "Measles",
    "text": "Measles\n\nLoad the dslabs package and figure out what is in the us_contagious_diseases dataset. Create a data frame, call it avg, that has a column for year, and a rate column containing the cases of Measles per 10,000 people per year in the US. Because we start in 1928, exclude Alaska and Hawaii. Make sure to take into account the number of weeks reporting each year. If a week was not report, it should not be included in the calculation of the rate.\n\n\nlibrary(tidyverse)\nlibrary(dslabs)\navg &lt;- us_contagious_diseases |&gt;\n  filter(!state %in% c(\"Hawaii\",\"Alaska\") & disease == \"Measles\" & weeks_reporting &gt; 0) |&gt;\n  group_by(year) |&gt;\n  summarize(rate = sum(count*52/weeks_reporting, na.rm = TRUE)/sum(population) * 10000)\n\nCriteria (5 points):\n\nFiltered correctly: removed HI and AK, kept only Measels, removed rows with no weeks reporting = 2 points.\nGrouped by year = 1 point.\nAdjusted for week reportin = 1 point.\nSummarized correctly: sum of counts divided by sum of popuation and multiploed by 10^5 = 1 point.\n\n\nUse the data frame avg to make a trend plot showing the cases rate for Measles per year. Add a vertical line showing the year the Measles vaccines was introduced. Write a short paragraph describing the graph to someone you are urging to take the Measles vaccines.\n\n\navg |&gt; \n  ggplot(aes(year, rate)) +\n  geom_line() + \n  labs(x = \"Year\", y = \"Cases per 10,000\", title = \"Measles cases per year in the US\") +\n  geom_vline(xintercept = 1963, col = \"blue\")\n\n\n\n\nCriteria (5 points):\n\nPlotted rate versus year = 3 points\nVertical line added correctly = 1 point.\nLabels and title = 1 point.\n\n\nIs the pattern observed above the same for each state? Add a grey trend line for each state to the plot above. Use a transformation that keeps the high rates from dominating the figure.\n\n\nus_contagious_diseases |&gt;\n  filter(!state %in% c(\"Hawaii\",\"Alaska\") & disease == \"Measles\" & weeks_reporting &gt; 0) |&gt;\n  filter(weeks_reporting &gt; 0) |&gt;\n  mutate(rate = count/population*10000*52/weeks_reporting) |&gt; \n  ggplot(aes(year, rate)) +\n  geom_line(aes(group = state), color = \"grey\") + \n  geom_line(data = avg) +\n  scale_y_continuous(trans = \"sqrt\") +\n  labs(x = \"Year\", y = \"Cases per 10,000\", title = \"Measles cases per year in the US\") +\n  geom_vline(xintercept = 1963, col = \"blue\")\n\n\n\n\nCriteria (5 points):\n\nIncludes curves for each state with no lengend = 2 points.\nAverage curve can be distinguished = 1 point.\nTransformation was used = 1 point.\nLabels and title = 1 point.\n\n\nIn the plot above we can’t tell which state is which curve. Using color would be challenging as it is hard if not impossible to find 48 colors we humans can distinguish. To make a plot where you can compare states knowing which is which, use one of the axis for state and the other for year, and then use hue or intensity as a visual cue for rates. Use a sqrt transformation to avoid the higher rates taking up all the color scale. Use grey to denote missing data. Order the states based on their highest peak.\n\n\nreds &lt;- RColorBrewer::brewer.pal(9, \"Reds\")\nus_contagious_diseases |&gt;\n  filter(!state %in% c(\"Hawaii\",\"Alaska\") & disease == \"Measles\") |&gt;\n  mutate(rate = count/population*10000*52/weeks_reporting) |&gt;\n  mutate(state = reorder(state, rate, max, na.rm = TRUE)) |&gt;\n  ggplot(aes(year, state, fill = rate)) +\n  geom_tile(color = \"grey\") +\n  scale_x_continuous(expand = c(0,0)) + ## to remove extra space on sides\n  scale_fill_gradientn(colors = reds, trans = \"sqrt\") +\n  geom_vline(xintercept = 1963, col = \"blue\") +\n  theme_minimal() +  \n  theme(panel.grid = element_blank(), \n        legend.position = \"bottom\", \n        text = element_text(size = 8)) +\n  labs(title = \"Measles cases per year in the US\", x = \"\", y = \"\")\n\n\n\n\nCriteria (8 points):\n\nTile plot used = 2 points.\nRate computed correctly = 1 point.\nSequential palette used = 1 point.\nNAs shown in grey = 1 point.\nTransformation used = 1 point.\nStates ordered by peak, title and labels included = 2 point."
  },
  {
    "objectID": "pset1.html#covid-19",
    "href": "pset1.html#covid-19",
    "title": "Problem Set 1",
    "section": "COVID-19",
    "text": "COVID-19\n\nThe csv file shared here includes weekly data on SARS-CoV-2 reported cases, tests, COVID-19 hospitalizations and deaths, and vaccination rates by state.\n\n\nImport the file into R without making a copy on your computer.\nExamine the dataset.\nWrite a sentence describing each variable in the dataset.\n\n\nurl &lt;- \"https://raw.githubusercontent.com/datasciencelabs/2023/main/data/covid19-data.csv\"\ndat &lt;- read_csv(url) \n\nRows: 9853 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): state, state_name\ndbl (13): population, region, mmwr_year, mmwr_week, cases, tests, hosp, deat...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nstate - State abbreviation.\nstate_name - State name.\npopulation - State population in 2020 census.\nregion - Department of Health and Human Services regions.\nmmwr_year - Morbidity and Mortality Weekly Report year.\nmmwr_week - Morbidity and Mortality Weekly Report week.\ncases - SARS-CoV-2 cases reported to CDC that week.\ntests - Accumulated tests reported according to JHU dashboard.\nhosp - COVID-19 Hospitalization reported to CDC.\ndeaths_underlying_cause - Deaths reported to CDC with COVID-19 as underlying cause of death.\ndeaths_multiple_causes - Deaths reported to CDC with COVID-19 as one of the causes of death.\nprov_death - Deaths provisionally reported to CDC as COVID-19 related.\nseries_complete - number of people in state with first vaccine series complete by the end of the week.\nbooster - number of people in state with booster by the end of the week.\nbivalent - number of people in state with bivalent vaccine by the end of the week.\n\nCriteria (5 points)\n\nData read correctly = 2 points.\nVariables described correctly = 3 points.\n\n\nOne of these columns could benefit from being a factor. Identify the column and convert it to factor.\n\n\ndat &lt;- mutate(dat, region = factor(region))\n\nCriteria (2 points)\n\nRegion identified as variable = 1 point.\nCorrectly changed to a factor = 1 point.\n\n\nRather than providing a date, the dataset provides MMWR year and week. Read this document and write a function to convert these to the start of the MMWR week in ISO-8601.\n\n\nlibrary(lubridate)\nmmwr_to_date &lt;- function(mmwr_year, mmwr_week) {\n  first_day &lt;- floor_date(make_date(mmwr_year, 1, 4) , unit = \"week\")\n  date &lt;- first_day + weeks(mmwr_week - 1) \n  return(date)\n}\n\nCriteria (5 points):\n\nA function was defined with two variables = 2 points\nFirst day identified correctly = 1 point.\nNumber of weeks added correctly = 2 points.\n\n\nAdd a columns start_date and end_date with the start and end of the MMWR week. Confirm that it worked by computing the MMWR week and year for both start and end date and comparing it to the MMWR week and year provided.\n\n\ndat &lt;- dat |&gt; mutate(start_date = mmwr_to_date(mmwr_year, mmwr_week),\n                   end_date = start_date + days(6))\n## check : these should all be TRUE\ndat |&gt; summarize(w1 = all(epiweek(start_date) == mmwr_week),\n                 y1 = all(epiyear(start_date) == mmwr_year),\n                 w2 = all(epiweek(end_date) == mmwr_week),\n                 y2 = all(epiyear(end_date) == mmwr_year))\n\n# A tibble: 1 × 4\n  w1    y1    w2    y2   \n  &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n1 TRUE  TRUE  TRUE  TRUE \n\n\nCriteria (3 points)\n\nTwo columns added correctly = 1 point.\nCheck performed correctly = 2 point.\n\n\nMake a trend plot similar to the one we made for Measles:\n\n\nInclude a trend line for the US cases rate. Use per 100,000 person per week as the unit.\nAdd a trend for each state, with color representing region.\nUse the end of the week date for your x-axis.\nAdd a vertical dashed line on the day COVID-19 vaccination started being delivered in the US.\n\nWrite a paragraph describing the COVID-19 pandemic by describing the plot.\n\ndat |&gt; \n  filter(!is.na(cases)) |&gt;\n  group_by(end_date) |&gt;\n  mutate(avg = sum(cases)/sum(population)*10^5) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(end_date, cases/population*10^5)) +\n  geom_line(aes(group = state, color = region), alpha = 0.5) +\n  geom_line(aes(y = avg)) +\n  scale_y_continuous(trans = \"sqrt\") + \n  geom_vline(xintercept = make_date(2020, 12, 14), lty = 2) +\n  labs(title = \"SARS-Cov-2 cases per 100,000 in the US\", x = \"Date\", y = \"Cases per 100,000\", \n       caption = \"Colors represent HHS regions\")\n\n\n\n\nCriteria (5 points):\n\nIncludes curves for each state with colors and lengend showing region= 2 points.\nAverage curve can be distinguished = 1 point.\nVertical line in correct location = 1 point.\nLabels and title = 1 point.\n\n\nThe number of cases depends on testing capacity. Note that during the start of the pandemic, when we know many people died, there are not that many cases reported. Also notice somewhat large variability across states that might not be consistent with actual prevalence. The tests columns provides the cumulative number of tests performed by the data represented by the week. This data is not official CDC data. It was provided by Johns Hopkins Coronavirus Resource Center. Before using the data, explore the data for each state carefully looking for potential problems.\n\nFor each state compute and plot the number of tests perforemd each week. Look at the plot for each state and look for possible problems. No need to make this plot pretty since we are just using it for data exploration. Report any inconsistencies if any.\n\ntmp &lt;- dat |&gt; \n  group_by(state) |&gt;\n  arrange(end_date) |&gt;\n  mutate(tests = diff(c(0, tests))) |&gt;\n  ungroup() |&gt;\n  filter(!is.na(tests)) \n\ntmp |&gt;\n  ggplot(aes(end_date, tests)) +\n  geom_line() +\n  facet_wrap(~state, scales = \"free_y\") +\n  labs(x = \"Date\", y = \"Number of tests\")\n\n\n\nstates &lt;- tmp |&gt; filter(tests &lt; 0) |&gt; pull(state_name) |&gt; unique()\n\nFor we see negative numbers which is must be an error as the number of tests must be non negative.\nCriteria (5 points):\n\nDifference computed correctly after group by = 1 point.\nPlot using facet wrap = 1 point.\nFree y-scale = 1 point.\nCorrect states identified = 2 points.\n\n\nTo see if the inconsistencies seen in the previous plot are a problem if we are only going to look at the total number of tests at the end, plot the cumulative tests for each of the states with inconsistencies and see if the results are sensible. Explain your answer in 1-2 sentences.\n\n\ndat |&gt; filter(state_name %in% states) |&gt;\n  filter(!is.na(tests)) |&gt;\n  ggplot(aes(end_date, tests/10^6)) +\n  geom_line() +\n  facet_wrap(~state, scales = \"free_y\") +\n  labs(x = \"Date\", y = \"Cumulative number of tests (in millions)\") \n\n\n\n\nCriteria (5 points):\n\nOnly correct states shown and cumulative values shown = 1\nPlot using facet wrap with free y-scale = 1 point.\nExplanation includes observation of dips that are likely corrections, and that it does not affect final values = 3 points.\n\n\nJHU stopped reporting some time in 2021. What was that date? Show the day in the format September 18, 2022.\n\n\nlast_day &lt;- dat |&gt; filter(!is.na(tests)) |&gt; pull(end_date) |&gt; max()\nformat(last_day, \"%B %d, %Y\")\n\n[1] \"December 18, 2021\"\n\n\nCriteria (3 points):\n\nCorrect date identied = 1 point.\nCorrect format used = 2 point.\n\n\nCompute the number of tests per capita for the last day JHU reported these statistics. Make a boxplot of these values for each region and include the state level data with the state abbreviation as a label. Write a sentences describing these differences you see and how this could affect our interpretation of differences in cases rates across states.\n\n\ndat |&gt; filter(end_date == last_day) |&gt;\n  mutate(region = factor(region)) |&gt;\n  ggplot(aes(region, tests/population)) +\n  geom_boxplot() +\n  geom_text(aes(label = state)) +\n  labs(x = \"Region\", y = \"Test per capita\", title = \"SARS-COV2 tests per person\")\n\n\n\n\nCriteria (5 points):\n\nCorrect boplot shown = 1 points.\nLabels and titles = 1 point.\nDescription of state to state variation provided: 3 points.\n\n\nAlthough JHU stopped collecting testing data from the states, the CDC collected data from a few laboratories. We provide these date in this url.\n\n\nImport the data into R without downloading the file.\nMake sure that you create a data frame with a column with dates in Dates format and tests as numbers.\n\n\nurl &lt;- \"https://raw.githubusercontent.com/datasciencelabs/2023/main/data/covid19-tests.txt\"\ntests &lt;- read_delim(url, delim = \" \") \n\nRows: 183 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (2): date, tests\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntests &lt;- tests |&gt; mutate(date = mdy(date), tests = parse_number(tests))\n## alternatively we can re-read the file using the appropriate column types\ntests &lt;- read_delim(url, delim = \" \", \n                    col_types = cols(date = col_date(format = \"%m-%d-%y\"),\n                                     tests = col_number()))\n\nCriteria 3 points):\n\nDates is date format = 2 point\nTests are numeric = 1 point.\n\n\nPlot the tests per week to see the trend in time. Write a sentence of the pattern you see.\n\n\ntests |&gt; ggplot(aes(date, tests)) + geom_line() + labs(title = \"Tests per week\", x = \"Date\", y = \"Number of tests\")\n\n\n\n\nAt the start of the pandemic there were few tests conducted and now we are also seeing a drop.\nCriteria (3 points):\n\nCorrect trend plot made = 1 points.\nObservation that the number of tests are much lower now included = 2 points.\n\n\nThe analysis on tests points to cases as not being a good measure of the state of the pandemic. Remake the trend plot but using death rates instead of cases rates. Write a sentence on how this plot better shows about the start of the pandemic that the cases plot did not.\n\n\ndat |&gt; \n  filter(!is.na(deaths_prov)) |&gt;\n  group_by(end_date) |&gt;\n  mutate(avg = sum(deaths_prov)/sum(population)*10^5) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(end_date, deaths_prov/population*10^5)) +\n  geom_line(aes(group = state, color = region), alpha = 0.5) +\n  geom_line(aes(y = avg)) +\n  scale_y_continuous(trans = \"sqrt\") + \n  geom_vline(xintercept = make_date(2020, 12, 14), lty = 2) +\n  labs(title = \"COVID-19 deaths per 100,000 in the US\", x = \"Date\", y = \"Deaths per 100,000\", \n       caption = \"Colors represent HHS regions\") \n\n\n\n\nCriteria (3 points):\n\nSame code as before but using deaths_prov = 1 points.\nThe fact that we now see that the highest death rates were actually at the start of the pandemic and that is was particularly bad in the northeast is noted = 2 points.\n\n\nWe want to examine the percent of the population that completed the first series of vaccines, received the booster, and received the bivalent booster, respectively. First run this line of code and examine what it does.\n\n\ntmp &lt;- dat |&gt; \n  pivot_longer(c(series_complete, booster, bivalent), names_to = \"series\", values_to = \"percent\") |&gt;\n  select(state, region, population, end_date, series, percent) |&gt;\n  filter(!is.na(percent)) |&gt;\n  mutate(percent = percent/population,\n         series = factor(series, c(\"series_complete\", \"booster\", \"bivalent\"))) \n\nThen make a plot showing the percent of population vaccination for each state. Use color to represent region.\n\nShow the dates on the x axis with the month abbreviation and year.\nPlace the three plots vertically, on top of each other.\nShow percentages on the y axis. Hint: use scales::percent.\n\n\nlevels(tmp$series) &lt;- c(\"First vaccine\", \"Booster\", \"Bivalent\")\ntmp |&gt;  \n  rename(Region = region) |&gt;\n  ggplot(aes(end_date, percent, group = state, color = Region)) +\n  geom_line() +\n  scale_y_continuous(labels = scales::percent) +\n  scale_x_date(date_labels = \"%b %y\", date_breaks = \"6 month\") + \n  facet_wrap(~series, nrow = 3) +\n  labs(title = \"Percent of population vaccinated by state\", x = \"Date\", y = \"Percent vaccinated\")\n\n\n\n\nCriteria (5 points):\n\nCummulative values shown = 1 point.\nRegions shown with color and legend included = 1 point.\nFacet wrap used correctly with 3 rows = 2 points.\nClear labels and titles = 1 point.\n\n\nFor each period used to make the trend plot for the three series, make a boxplot showing the maximum percentage reached by every state stratified by region. Let the range of the y axis adapt to the range of each group.\n\n\ntmp |&gt; \n  group_by(state, region, series) |&gt;\n  summarize(percent = max(percent), .groups = \"drop\") |&gt;\n  ggplot(aes(region, percent)) +\n  scale_y_continuous(labels = scales::percent) +\n  geom_boxplot() +\n  geom_text(aes(label = state)) +\n  facet_wrap(~series, scales = \"free_y\", nrow = 3) +\n  labs(title = \"Percent vaccinated by region\", x = \"Region\", y = \"Percent\")\n\n\n\n\nCriteria (5 points)\n\nMaximum computed correctly = 2 points\nBoxplot showing points by region = 2 points\nFaceting and labels = 1 point.\n\n\nUse the plot above to define four periods: No vaccine, First vaccine, Booster, and Bivalent. Get the dates when each vaccine series starts (these don’t have to be exact). Create a version of your data frame with a column called period that keeps the period associated with the week contained in each row.\n\n\nperiod_dates &lt;- c(make_date(2020, 1,1), make_date(2021, 1, 1), make_date(2021, 9, 1), make_date(2022, 9, 1), today())\n\nlabels &lt;- c(\"No vaccine\", \"First vaccine\", \"Booster\", \"Bivalent\")\ntmp &lt;- dat |&gt; mutate(period = cut(end_date, period_dates, labels = labels))\n\nCriteria (3 points):\n\nApproriate choice of dates selecteed: 2 points\nused cut correctly: 1 point\n\n\nDefine a new variable that stores the maximum vaccination percentage reached during each period. But for the first vaccine period use series_complete, for the booster period use the booster column, and for the bivalent period use the bivalent percentages. Remove data from the no vaccine period. The make a plot comparing the COVID-19 death rate to the percent vaccinated. Use color to denote region. You should produce three plots, each with it’s own axes range. Put the three plots in three entries of a 2x2 layout. Comment on what you observe.\n\n\ntmp |&gt; filter(!period %in% c(\"No vaccine\")) |&gt;\n  mutate(period = droplevels(period)) |&gt;\n  group_by(state) |&gt;\n   mutate(vax = case_when(\n     period == \"First vaccine\" ~ max(series_complete, na.rm = TRUE),\n     period == \"Booster\" ~ max(booster, na.rm = TRUE),\n     period == \"Bivalent\" ~ max(bivalent, na.rm = TRUE))) |&gt;\n  ungroup() |&gt;\n  group_by(state, region, period) |&gt;\n  summarize(deaths = mean(deaths_prov, na.rm = TRUE) / population[1] * 100000,\n            vax = vax[1]/population[1], .groups = \"drop\") |&gt; \n  rename(Region = region) |&gt;\n  ggplot(aes(vax, deaths, label = state, color = Region)) + \n  geom_point(size = 0) + \n  geom_text(show.legend = FALSE) + \n  scale_x_continuous(labels = scales::percent) +\n  guides(colour = guide_legend(override.aes = list(size = 1))) + \n  facet_wrap(~period, scales = \"free\", ncol = 2) + \n  labs(\"Death rate versus vaccination rate for states\", x = \"% population vaccinated\", y = \"Per 100,000 per week death rate\")\n\n\n\n\nCriteria (8 points)\n\nCorrectly defines the three periods = 3 points\nCorrectly computes the death rates and vaccination percentages in each period = 3 points\nUses facet_wrap to show three plots = 2 point\nClear labels and titles = 1 point.\n\n\nDoes population density have an effect on infections? Use the state.area predefined variable to add compute population density. Make a histogram and density plot of state densities. Note that you will have to add the geographical area for Puerto Rico and DC as it is not included in state.area\n\n\nmy.state.abb &lt;- c(state.abb, \"PR\", \"DC\")\nmy.state.area &lt;- c(state.area, 5325, 69)\npopdens &lt;- dat |&gt; filter(end_date == min(end_date)) |&gt; \n  select(state, population) |&gt;\n  mutate(area = my.state.area[match(state, my.state.abb)]) |&gt; \n  mutate(popdens = population / area) \n  \npopdens |&gt; ggplot(aes(popdens)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 25, color = \"black\") +\n  geom_density() +\n  scale_x_log10() +\n  labs(title = \"Distribution of poplation density across states\", x = \"Population density\", y = \"Density\")\n\n\n\n\nCriteria (5 points):\n\nAdds PR and DC = 1 point\nAdd population density column = 1 point\nHistogram shown = 1 point\nDensity shown = 1 point\nTitle, labels and smoothing parameter and bin size choice = 1 point\n\n\nPlot death rates versus density for the four periods defined above.\n\n\ntmp |&gt; filter(!is.na(deaths_prov)) |&gt;\n  mutate(area = my.state.area[match(state, my.state.abb)]) |&gt; \n  mutate(popdens = population / area) |&gt;\n  group_by(state, period) |&gt;\n  summarize(rate = mean(deaths_prov) / population[1]*100000, popdens = popdens[1], .groups = \"drop\") |&gt;\n  ggplot(aes(popdens, rate, label = state)) +\n  geom_text() +\n  scale_x_log10() +\n  facet_wrap(~period) + \n  labs(title = \"Death rate versus density\", x = \"Population density\", y = \"Deaths per 100,000 per week\")\n\n\n\n\nCriteria (5 points)\n\nComputes rate and population density correctly = 2 points\nFour plots shown made with facet_wrap = 2 points\nLabels and titles = 1 point"
  },
  {
    "objectID": "10-distributions.html#case-study-describing-student-heights",
    "href": "10-distributions.html#case-study-describing-student-heights",
    "title": "10  Distributions",
    "section": "10.1 Case study: describing student heights",
    "text": "10.1 Case study: describing student heights\nWe will study self reported heights from studnets from past classes:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nhead(heights)\n\n     sex height\n1   Male     75\n2   Male     70\n3   Male     68\n4   Male     74\n5   Male     61\n6 Female     65"
  },
  {
    "objectID": "10-distributions.html#distributions",
    "href": "10-distributions.html#distributions",
    "title": "10  Distributions",
    "section": "10.2 Distributions",
    "text": "10.2 Distributions\nThe most basic statistical summary of a list of objects or numbers is its distribution.\n\nprop.table(table(heights$sex))\n\n\n   Female      Male \n0.2266667 0.7733333 \n\n\nHere is the distribution for the regions in the murders dataset:\n\n\n\n\n\n\n10.2.1 Histograms\nCumulative distributions function shows everything you need to know the distribution.\n\n\n\n\n\nHistograms lose a bit fo information but are easier to read:\n\n\n\n\n\n\n\n10.2.2 Smoothed density\nSmooth density plots relay the same information as a histogram but are aesthetically more appealing. Here is what a smooth density plot looks like for our heights data:\n\n\n\n\n\nAn advantage is that it is easy to show more than one:\n\n\n\n\n\nWith the right argument, ggplot automatically shades the intersecting region with a different color.\n\n\n10.2.3 The normal distribution\nThe normal distribution, also known as the bell curve and as the Gaussian distribution. Here is what the normal distribution looks like:\n\n\n\n\n\nA useful characteristic of the normal distribution is that it is defined by just two numbers: the average (also called mean) and the standard deviation.\nSo for the male height data we can define the average of standard deviation like this:\n\nindex &lt;- heights$sex == \"Male\"\nx &lt;- heights$height[index]\nm &lt;- sum(x) / length(x)\ns &lt;- sqrt(sum((x - mu)^2)/length(x))\n\nThe pre-built functions mean and sd can be used here: :::\n\nm &lt;- mean(x)\ns &lt;- sd(x)\nc(average = m, sd = s)\n\n  average        sd \n69.314755  3.611024 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe pre-built functions mean and sd (note that, for reasons explained in statistics textbooks,sd divides by length(x)-1 rather than length(x)) can be used here:\n\n\nHere is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:"
  },
  {
    "objectID": "10-distributions.html#boxplots",
    "href": "10-distributions.html#boxplots",
    "title": "10  Distributions",
    "section": "10.3 Boxplots",
    "text": "10.3 Boxplots\nBoxplots provide a five number summary (and shows outliers):\n\n\n\n\n\nIn this case, the histogram above or a smooth density plot would serve as a relatively succinct summary."
  },
  {
    "objectID": "10-distributions.html#sec-dataviz-stratification",
    "href": "10-distributions.html#sec-dataviz-stratification",
    "title": "10  Distributions",
    "section": "10.4 Stratification",
    "text": "10.4 Stratification\nShowing conditional distributions is often very informative\n\n\n\n\n\nWe also see the normal approximation might not be useful for females:\n\n\n\n\n\nRegarding the five smallest values, note that these values are:\n\nheights |&gt; filter(sex == \"Female\") |&gt; \n  top_n(5, desc(height)) |&gt;\n  pull(height)\n\n[1] 51 53 55 52 52\n\n\nBecause these are reported heights, a possibility is that the student meant to enter 5'1\", 5'2\", 5'3\" or 5'5\"."
  },
  {
    "objectID": "10-distributions.html#exercises",
    "href": "10-distributions.html#exercises",
    "title": "10  Distributions",
    "section": "10.5 Exercises",
    "text": "10.5 Exercises\n\nSuppose we can’t make a plot and want to compare the distributions side by side. We can’t just list all the numbers. Instead, we will look at the percentiles. Create a five row table showing female_percentiles and male_percentiles with the 10th, 30th, 50th, 70th, & 90th percentiles for each sex. Then create a data frame with these two as columns.\n\n\nlibrary(dslabs)\n## Here is an R-base solution\nqs &lt;- seq(10,90,20)\nwith(heights,\n     data.frame(\n       quantile(height[sex == \"Male\"], qs/100),\n       quantile(height[sex == \"Female\"], qs/100)\n)) |&gt; setNames(c(\"female_percentiles\", \"male_percentiles\"))\n\n    female_percentiles male_percentiles\n10%           65.00000         61.00000\n30%           68.00000         63.00000\n50%           69.00000         64.98031\n70%           71.00000         66.46417\n90%           73.22751         69.00000\n\n## Here is the solution using pivot_wider, which we learn later\nlibrary(dplyr)\nqs &lt;- seq(10,90,20)\nheights |&gt; group_by(sex) |&gt;\n  reframe(quantile = paste0(qs, \"%\"), value = quantile(height, qs/100)) |&gt;\n  pivot_wider(names_from = sex) |&gt;\n  rename(female_percentiles = Female, male_percentiles = Male)\n\n# A tibble: 5 × 3\n  quantile female_percentiles male_percentiles\n  &lt;chr&gt;                 &lt;dbl&gt;            &lt;dbl&gt;\n1 10%                    61               65  \n2 30%                    63               68  \n3 50%                    65.0             69  \n4 70%                    66.5             71  \n5 90%                    69               73.2\n\n\n\nStudy the following boxplots showing population sizes by country:\n\n\n\n\n\n\nWhich continent has the country with the biggest population size?\n\nWhat continent has the largest median population size?\nWhat is median population size for Africa to the nearest million?\nWhat proportion of countries in Europe have populations below 14 million?\n\n\n0.99\n0.75\n0.50\n0.25\n\n\nWhen using the log transformation, which continent shown above has the largest interquartile range?\n\n\n## We can see that it is Americas visually, but just in case here it is:\ntab |&gt; group_by(continent) |&gt;\n  summarize(diff(quantile(log10(population), c(.25,.75))))\n\n# A tibble: 5 × 2\n  continent `diff(quantile(log10(population), c(0.25, 0.75)))`\n  &lt;fct&gt;                                                  &lt;dbl&gt;\n1 Africa                                                 0.908\n2 Americas                                               1.51 \n3 Asia                                                   1.04 \n4 Europe                                                 0.595\n5 Oceania                                                0.905\n\n\n\nLoad the height data set and create a vector x with just the male heights:\n\n\nlibrary(dslabs)\nx &lt;- heights$height[heights$sex==\"Male\"]\n\nWhat proportion of the data is between 69 and 72 inches (taller than 69, but shorter or equal to 72)? Hint: use a logical operator and mean."
  },
  {
    "objectID": "11-ggplot2.html#the-components-of-a-graph",
    "href": "11-ggplot2.html#the-components-of-a-graph",
    "title": "11  ggplot2",
    "section": "11.1 The components of a graph",
    "text": "11.1 The components of a graph\nWe will construct a graph that summarizes the US murders dataset that looks like this:\n\n\n\n\n\nThe first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the ggplot2 terminology. The main three components to note are:\n\nData: The US murders data table is being summarized. We refer to this as the data component.\nGeometry: The plot above is a scatterplot. This is referred to as the geometry component. Other possible geometries are barplot, histogram, smooth densities, qqplot, and boxplot. We will learn more about these in the Data Visualization part of the book.\nAesthetic mapping: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we map data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the aesthetic mapping component. How we define the mapping depends on what geometry we are using.\n\nWe also note that:\n\nThe points are labeled with the state abbreviations.\nThe range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales.\nThere are labels, a title, a legend, and we use the style of The Economist magazine.\n\nWe will now construct the plot piece by piece."
  },
  {
    "objectID": "11-ggplot2.html#ggplot-objects",
    "href": "11-ggplot2.html#ggplot-objects",
    "title": "11  ggplot2",
    "section": "11.2 ggplot objects",
    "text": "11.2 ggplot objects\nStart by defining the dataset:\n\nggplot(data = murders)\n\nWe can also use the pipe:\n\nmurders |&gt; ggplot()\n\n\n\n\nWe call aslo assign the output to a variabel\n\np &lt;- ggplot(data = murders)\nclass(p)\n\n[1] \"gg\"     \"ggplot\"\n\n\nTo see the plot we can print it:\n\nprint(p)\np"
  },
  {
    "objectID": "11-ggplot2.html#geometries",
    "href": "11-ggplot2.html#geometries",
    "title": "11  ggplot2",
    "section": "11.3 Geometries",
    "text": "11.3 Geometries\nIn ggplot2 we create graphs by adding layers. Layers can define geometries, compute summary statistics, define what scales to use, or even change styles. To add layers, we use the symbol +. In general, a line of code will look like this:\n\n\nDATA |&gt; ggplot() + LAYER 1 + LAYER 2 + … + LAYER N\n\n\nUsually, the first added layer defines the geometry. We want to make a scatterplot. What geometry do we use?\nLet’s look at the cheat sheet: https://rstudio.github.io/cheatsheets/data-visualization.pdf"
  },
  {
    "objectID": "11-ggplot2.html#aesthetic-mappings",
    "href": "11-ggplot2.html#aesthetic-mappings",
    "title": "11  ggplot2",
    "section": "11.4 Aesthetic mappings",
    "text": "11.4 Aesthetic mappings\nTo make a scatter plot we use geom_points. Take a look at the help file and learn that this is how we use it:\n\nmurders |&gt; ggplot() + \n  geom_point(aes(x = population/10^6, y = total))\n\nSince we defined p above, we can add a layer like this:\n\np + geom_point(aes(population/10^6, total))"
  },
  {
    "objectID": "11-ggplot2.html#layers",
    "href": "11-ggplot2.html#layers",
    "title": "11  ggplot2",
    "section": "11.5 Layers",
    "text": "11.5 Layers\nTo add text we use geom_text:\n\np + geom_point(aes(population/10^6, total)) +\n  geom_text(aes(population/10^6, total, label = abb))\n\n\n\n\nAs an example of the unique behavior of aes mentioned above, note that this call:\n\np_test &lt;- p + geom_text(aes(population/10^6, total, label = abb))\n\nis fine, whereas this call:\n\np_test &lt;- p + geom_text(aes(population/10^6, total), label = abb) \n\nwill give you an error since abb is not found because it is outside of the aes function. The layer geom_text does not know where to find abb since it is a column name and not a global variable.\n\n11.5.1 Tinkering with arguments\n\np + geom_point(aes(population/10^6, total), size = 3) +\n  geom_text(aes(population/10^6, total, label = abb))\n\n\n\n\n\np + geom_point(aes(population/10^6, total), size = 3) +\n  geom_text(aes(population/10^6, total, label = abb), nudge_x = 1.5)"
  },
  {
    "objectID": "11-ggplot2.html#global-versus-local-aesthetic-mappings",
    "href": "11-ggplot2.html#global-versus-local-aesthetic-mappings",
    "title": "11  ggplot2",
    "section": "11.6 Global versus local aesthetic mappings",
    "text": "11.6 Global versus local aesthetic mappings\n\nargs(ggplot)\n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \nNULL\n\n\nWe can define a global aes in the ggplot function. All the layers will assume this mapping unless we explicitly define another one:\n\np &lt;- murders |&gt; ggplot(aes(population/10^6, total, label = abb))\np + geom_point(size = 3) + \n  geom_text(nudge_x = 1.5)\n\n\n\n\nWe can overide the global aes by defining one in the geometry functions:\n\np + geom_point(size = 3) +  \n  geom_text(aes(x = 10, y = 800, label = \"Hello there!\"))"
  },
  {
    "objectID": "11-ggplot2.html#scales",
    "href": "11-ggplot2.html#scales",
    "title": "11  ggplot2",
    "section": "11.7 Scales",
    "text": "11.7 Scales\n\np + geom_point(size = 3) +  \n  geom_text(nudge_x = 0.05) + \n  scale_x_continuous(trans = \"log10\") +\n  scale_y_continuous(trans = \"log10\") \n\n\n\n\nThis particular transformation is so common that ggplot2 provides the specialized functions scale_x_log10and scale_y_log10, which we can use to rewrite the code like this:\n\np + geom_point(size = 3) +  \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10()"
  },
  {
    "objectID": "11-ggplot2.html#labels-and-titles",
    "href": "11-ggplot2.html#labels-and-titles",
    "title": "11  ggplot2",
    "section": "11.8 Labels and titles",
    "text": "11.8 Labels and titles\n\np + geom_point(size = 3) +  \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Populations in millions (log scale)\") + \n  ylab(\"Total number of murders (log scale)\") +\n  ggtitle(\"US Gun Murders in 2010\")\n\n\n\n\nWe can also use the labs function:\n\np + geom_point(size = 3) +  \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10() +\n  labs(x = \"Populations in millions (log scale)\", \n       y = \"Total number of murders (log scale)\", \n       title = \"US Gun Murders in 2010\")\n\nWe are almost there! All we have left to do is add color, a legend, and optional changes to the style."
  },
  {
    "objectID": "11-ggplot2.html#categories-as-colors",
    "href": "11-ggplot2.html#categories-as-colors",
    "title": "11  ggplot2",
    "section": "11.9 Categories as colors",
    "text": "11.9 Categories as colors\nLet’s redefine p so we can test layers easilty:\n\np &lt;-  murders |&gt; ggplot(aes(population/10^6, total, label = abb)) +   \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Populations in millions (log scale)\") + \n  ylab(\"Total number of murders (log scale)\") +\n  ggtitle(\"US Gun Murders in 2010\")\n\nHere is an exmaple of adding color:\n\np + geom_point(size = 3, color = \"blue\")\n\n\n\n\nBut if we want the color to relate to a variable, we need to include it in the map:\n\np + geom_point(aes(col = region), size = 3)"
  },
  {
    "objectID": "11-ggplot2.html#annotation-shapes-and-adjustments",
    "href": "11-ggplot2.html#annotation-shapes-and-adjustments",
    "title": "11  ggplot2",
    "section": "11.10 Annotation, shapes, and adjustments",
    "text": "11.10 Annotation, shapes, and adjustments\nWe want to add a line with intercept the us rate. So lets comput that\n\nr &lt;- murders |&gt; \n  summarize(rate = sum(total) /  sum(population) * 10^6) |&gt; \n  pull(rate)\n\nNow we can use the geom_abline function.\n\np + geom_point(aes(col = region), size = 3) + \n  geom_abline(intercept = log10(r))\n\n\n\n\nWe are very close to the goal. Let’s redefine p so we can easily add the finishing touches:\n\np &lt;- p + geom_abline(intercept = log10(r), lty = 2, color = \"darkgrey\") +\n  geom_point(aes(col=region), size = 3)  \n\nFor example, this is how we change the name of the legend:\n\np &lt;- p + scale_color_discrete(name = \"Region\") \np"
  },
  {
    "objectID": "11-ggplot2.html#sec-add-on-packages",
    "href": "11-ggplot2.html#sec-add-on-packages",
    "title": "11  ggplot2",
    "section": "11.11 Add-on packages",
    "text": "11.11 Add-on packages\nThe dslabs package can define the look used in the textbook:\n\nds_theme_set()\n\nMany other themes are added by the package ggthemes. Among those are the theme_economist theme that we used. After installing the package, you can change the style by adding a layer like this:\n\nlibrary(ggthemes)\np + theme_economist()\n\n\n\n\nYou can see how some of the other themes look by simply changing the function. For instance, you might try the theme_fivethirtyeight() theme instead.\n\nlibrary(ggthemes)\np + theme_fivethirtyeight()\n\n\n\n\nAnd if you want to ruin the plot, give it the excel theme:\n\np + theme_excel()\n\n\n\n\nFor more fun themes:\n\nlibrary(ThemePark)\np + theme_starwars()\n\n\n\n\n\np + theme_zelda()"
  },
  {
    "objectID": "11-ggplot2.html#putting-it-all-together",
    "href": "11-ggplot2.html#putting-it-all-together",
    "title": "11  ggplot2",
    "section": "11.12 Putting it all together",
    "text": "11.12 Putting it all together\nNow that we are done testing, we can write one piece of code that produces our desired plot from scratch.\n\nlibrary(ggthemes)\nlibrary(ggrepel)\n\nr &lt;- murders |&gt; \n  summarize(rate = sum(total) /  sum(population) * 10^6) |&gt;\n  pull(rate)\n\nmurders |&gt; ggplot(aes(population/10^6, total, label = abb)) +   \n  geom_abline(intercept = log10(r), lty = 2, color = \"darkgrey\") +\n  geom_point(aes(col = region), size = 3) +\n  geom_text_repel() + \n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Populations in millions (log scale)\") + \n  ylab(\"Total number of murders (log scale)\") +\n  ggtitle(\"US Gun Murders in 2010\") + \n  scale_color_discrete(name = \"Region\") +\n  theme_economist()"
  },
  {
    "objectID": "11-ggplot2.html#grids-of-plots",
    "href": "11-ggplot2.html#grids-of-plots",
    "title": "11  ggplot2",
    "section": "11.13 Grids of plots",
    "text": "11.13 Grids of plots\nThere are often reasons to graph plots next to each other. The gridExtra package permits us to do that:\n\nlibrary(gridExtra)\np1 &lt;- murders |&gt; ggplot(aes(log10(population))) + geom_histogram()\np2 &lt;- murders |&gt; ggplot(aes(log10(population), log10(total))) + geom_point()\ngrid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "11-ggplot2.html#sec-other-geometries",
    "href": "11-ggplot2.html#sec-other-geometries",
    "title": "11  ggplot2",
    "section": "11.14 ggplot2 geometries",
    "text": "11.14 ggplot2 geometries\nWe previously introduced the ggplot2 package for data visualization. Here we demonstrate how to generate plots related to distributions, specifically the plots shown earlier in this chapter.\n\n11.14.1 Barplots\nTo generate a barplot we can use the geom_bar geometry. The default is to count the number of each category and draw a bar. Here is the plot for the regions of the US.\n\nmurders |&gt; ggplot(aes(region)) + geom_bar()\n\n\n\n\nWe often already have a table with a distribution that we want to present as a barplot. Here is an example of such a table:\n\ntab &lt;- murders |&gt; \n  count(region) |&gt; \n  mutate(proportion = n/sum(n))\ntab\n\n         region  n proportion\n1     Northeast  9  0.1764706\n2         South 17  0.3333333\n3 North Central 12  0.2352941\n4          West 13  0.2549020\n\n\nWe no longer want geom_bar to count, but rather just plot a bar to the height provided by the proportion variable. For this we need to provide x (the categories) and y (the values) and use the stat=\"identity\" option.\n\ntab |&gt; ggplot(aes(region, proportion)) + geom_bar(stat = \"identity\")\n\n\n\n\n\n\n11.14.2 Histograms\nTo generate histograms we use geom_histogram. By looking at the help file for this function, we learn that the only required argument is x, the variable for which we will construct a histogram. We dropped the x because we know it is the first argument. The code looks like this:\n\nheights |&gt; \n  filter(sex == \"Female\") |&gt; \n  ggplot(aes(height)) + \n  geom_histogram()\n\nIf we run the code above, it gives us a message:\n\nstat_bin() using bins = 30. Pick better value with binwidth.\n\nWe previously used a bin size of 1 inch, so the code looks like this:\n\nheights |&gt; \n  filter(sex == \"Female\") |&gt; \n  ggplot(aes(height)) + \n  geom_histogram(binwidth = 1)\n\nFinally, if for aesthetic reasons we want to add color, we use the arguments described in the help file. We also add labels and a title:\n\nheights |&gt; \n  filter(sex == \"Female\") |&gt; \n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", col = \"black\") +\n  xlab(\"Female heights in inches\") + \n  ggtitle(\"Histogram\")\n\n\n\n\n\n\n11.14.3 Density plots\nTo create a smooth density, we use the geom_density. To make a smooth density plot with the data previously shown as a histogram we can use this code:\n\nheights |&gt; \n  filter(sex == \"Female\") |&gt;\n  ggplot(aes(height)) +\n  geom_density()\n\nTo fill in with color, we can use the fill argument.\n\nheights |&gt; \n  filter(sex == \"Female\") |&gt;\n  ggplot(aes(height)) +\n  geom_density(fill = \"blue\")\n\n\n\n\nTo change the smoothness of the density, we use the adjust argument to multiply the default value by that adjust. For example, if we want the bandwidth to be twice as big we use:\n\nheights |&gt; \n  filter(sex == \"Female\") |&gt;\n  ggplot(aes(height)) +\n  geom_density(fill = \"blue\", adjust = 2)\n\n\n\n11.14.4 Boxplots\nThe geometry for boxplot is geom_boxplot. As discussed, boxplots are useful for comparing distributions. For example, below are the previously shown heights for women, but compared to men. For this geometry, we need arguments x as the categories, and y as the values.\n\n\n\n\n\n\n\n11.14.5 QQ-plots\nFor qq-plots we use the geom_qq geometry. From the help file, we learn that we need to specify the sample (we will learn about samples in a later chapter). Here is the qqplot for men heights.\n\nheights |&gt; filter(sex == \"Male\") |&gt;\n  ggplot(aes(sample = height)) +\n  geom_qq()\n\n\n\n\nBy default, the sample variable is compared to a normal distribution with average 0 and standard deviation 1. To change this, we use the dparams arguments based on the help file. Adding an identity line is as simple as assigning another layer. For straight lines, we use the geom_abline function. The default line is the identity line (slope = 1, intercept = 0).\n\nparams &lt;- heights |&gt; filter(sex==\"Male\") |&gt;\n  summarize(mean = mean(height), sd = sd(height))\n\nheights |&gt; filter(sex==\"Male\") |&gt;\n  ggplot(aes(sample = height)) +\n  geom_qq(dparams = params) +\n  geom_abline()\n\nAnother option here is to scale the data first and then make a qqplot against the standard normal.\n\nheights |&gt; \n  filter(sex==\"Male\") |&gt;\n  ggplot(aes(sample = scale(height))) + \n  geom_qq() +\n  geom_abline()\n\n\n\n11.14.6 Images\nWe introduce the two geometries used to create images: geom_tile and geom_raster. They behave similarly; to see how they differ, please consult the help file. To create an image in ggplot2 we need a data frame with the x and y coordinates as well as the values associated with each of these. Here is a data frame.\n\nx &lt;- expand.grid(x = 1:12, y = 1:10) |&gt; \n  mutate(z = 1:120) \n\nNote that this is the tidy version of a matrix, matrix(1:120, 12, 10). To plot the image we use the following code:\n\nx |&gt; ggplot(aes(x, y, fill = z)) + \n  geom_raster()\n\nWith these images you will often want to change the color scale. This can be done through the scale_fill_gradientn layer.\n\nx |&gt; ggplot(aes(x, y, fill = z)) + \n  geom_raster() + \n  scale_fill_gradientn(colors =  terrain.colors(10, alpha = 1))"
  },
  {
    "objectID": "11-ggplot2.html#exercises",
    "href": "11-ggplot2.html#exercises",
    "title": "11  ggplot2",
    "section": "11.15 Exercises",
    "text": "11.15 Exercises\n\nCreate a ggplot object using the pipe to assign the heights data to a ggplot object. Assign height to the x values through the aes function.\nAdd a layer to actually make the histogram. Use the object created in the previous exercise and the geom_histogram function to make the histogram.\nNote that when we run the code in the previous exercise we get the warning: stat_bin() using bins = 30. Pick better value with binwidth. Use the binwidth argument to change the histogram made in the previous exercise to use bins of size 1 inch.\nInstead of a histogram, we are going to make a smooth density plot. In this case we will not make an object, but instead render the plot with one line of code. Change the geometry in the code previously used to make a smooth density instead of a histogram.\nNow we are going to make a density plot for males and females separately. We can do this using the group argument. We assign groups via the aesthetic mapping as each point needs to a group before making the calculations needed to estimate a density.\nWe can also assign groups through the color argument. This has the added benefit that it uses color to distinguish the groups. Change the code above to use color.\nWe can also assign groups through the fill argument. This has the added benefit that it uses colors to distinguish the groups, like this:\n\n\nheights |&gt; \n  ggplot(aes(height, fill = sex)) + \n  geom_density() \n\nHowever, here the second density is drawn over the other. We can make the curves more visible by using alpha blending to add transparency. Set the alpha parameter to 0.2 in the geom_density function to make this change.\n\nUsing the pipe |&gt;, create an object p with the heights dataset as the data.\nNow we are going to add a layer and the corresponding aesthetic mappings. For the murders data we plotted total murders versus population sizes. Explore the murders data frame to remind yourself what are the names for these two variables and select the correct answer.\n\n\nstate and abb.\ntotal_murders and population_size.\ntotal and population.\nmurders and size.\n\n\nTo create a scatterplot we add a layer with geom_point. The aesthetic mappings require us to define the x-axis and y-axis variables, respectively. So the code looks like this:\n\n\nmurders |&gt; ggplot(aes(x = , y = )) +\n  geom_point()\n\nexcept we have to define the two variables x and y. Fill this out with the correct variable names.\n\nNote that if we don’t use argument names, we can obtain the same plot by making sure we enter the variable names in the right order like this:\n\n\nmurders |&gt; ggplot(aes(population, total)) +\n  geom_point()\n\nRemake the plot but now with total in the x-axis and population in the y-axis.\n\nIf instead of points we want to add text, we can use the geom_text() or geom_label() geometries. The following code\n\n\nmurders |&gt; ggplot(aes(population, total)) + geom_label()\n\nwill give us the error message: Error: geom_label requires the following missing aesthetics: label\nWhy is this?\n\nWe need to map a character to each point through the label argument in aes.\nWe need to let geom_label know what character to use in the plot.\nThe geom_label geometry does not require x-axis and y-axis values.\ngeom_label is not a ggplot2 command.\n\n\nRewrite the code above to use abbreviation as the label through aes\nChange the color of the labels to blue. How will we do this?\n\n\nAdding a column called blue to murders.\nBecause each label needs a different color we map the colors through aes.\nUse the color argument in ggplot.\nBecause we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label.\n\n\nRewrite the code above to make the labels blue.\nNow suppose we want to use color to represent the different regions. In this case which of the following is most appropriate:\n\n\nAdding a column called color to murders with the color we want to use.\nBecause each label needs a different color we map the colors through the color argument of aes .\nUse the color argument in ggplot.\nBecause we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label.\n\n\nRewrite the code above to make the labels’ color be determined by the state’s region.\nNow we are going to change the x-axis to a log scale to account for the fact the distribution of population is skewed. Let’s start by defining an object p holding the plot we have made up to now\n\n\np &lt;- murders |&gt; \n  ggplot(aes(population, total, label = abb, color = region)) +\n  geom_label() \n\nTo change the y-axis to a log scale we learned about the scale_x_log10() function. Add this layer to the object p to change the scale and render the plot.\n\nRepeat the previous exercise but now change both axes to be in the log scale.\nNow edit the code above to add the title “Gun murder data” to the plot. Hint: use the ggtitle function."
  },
  {
    "objectID": "12-dataviz-principles.html#encoding-data-using-visual-cues",
    "href": "12-dataviz-principles.html#encoding-data-using-visual-cues",
    "title": "12  Data visualization principles",
    "section": "12.1 Encoding data using visual cues",
    "text": "12.1 Encoding data using visual cues\nWe start by describing some principles for encoding data. There are several approaches at our disposal including position, aligned lengths, angles, area, brightness, and color hue.\nTo illustrate how some of these strategies compare, let’s suppose we want to report the results from two hypothetical polls regarding browser preference taken in 2000 and then 2015. For each year, we are simply comparing five quantities – the five percentages. A widely used graphical representation of percentages, popularized by Microsoft Excel, is the pie chart:\n\n\n\n\n\nHere we are representing quantities with both areas and angles, since both the angle and area of each pie slice are proportional to the quantity the slice represents. This turns out to be a sub-optimal choice since, as demonstrated by perception studies, humans are not good at precisely quantifying angles and are even worse when area is the only available visual cue. The donut chart is an example of a plot that uses only area:\n\n\n\n\n\nTo see how hard it is to quantify angles and area, note that the rankings and all the percentages in the plots above changed from 2000 to 2015. Can you determine the actual percentages and rank the browsers’ popularity? Can you see how the percentages changed from 2000 to 2015? It is not easy to tell from the plot. In this case, simply showing the numbers is not only clearer, but would also save on printing costs if printing a paper copy:\n\n\n\n\n\nBrowser\n2000\n2015\n\n\n\n\nOpera\n3\n2\n\n\nSafari\n21\n22\n\n\nFirefox\n23\n21\n\n\nChrome\n26\n29\n\n\nIE\n28\n27\n\n\n\n\n\n\n\nThe preferred way to plot these quantities is to use length and position as visual cues, since humans are much better at judging linear measures. The barplot uses this approach by using bars of length proportional to the quantities of interest. By adding horizontal lines at strategically chosen values, in this case at every multiple of 10, we ease the visual burden of quantifying through the position of the top of the bars. Compare and contrast the information we can extract from the two figures.\n\n\n\n\n\nNotice how much easier it is to see the differences in the barplot. In fact, we can now determine the actual percentages by following a horizontal line to the x-axis.\nIf for some reason you need to make a pie chart, label each pie slice with its respective percentage so viewers do not have to infer them from the angles or area:\n\n\n\n\n\nIn general, when displaying quantities, position and length are preferred over angles and/or area. Brightness and color are even harder to quantify than angles. But, as we will see later, they are sometimes useful when more than two dimensions must be displayed at once."
  },
  {
    "objectID": "12-dataviz-principles.html#know-when-to-include-0",
    "href": "12-dataviz-principles.html#know-when-to-include-0",
    "title": "12  Data visualization principles",
    "section": "12.2 Know when to include 0",
    "text": "12.2 Know when to include 0\nWhen using barplots, it is misinformative not to start the bars at 0. This is because, by using a barplot, we are implying the length is proportional to the quantities being displayed. By avoiding 0, relatively small differences can be made to look much bigger than they actually are. This approach is often used by politicians or media organizations trying to exaggerate a difference.\n\n\n\n\n\n(Source: Fox News, via Media Matters1.)\nFrom the plot above, it appears that apprehensions have almost tripled when, in fact, they have only increased by about 16%. Starting the graph at 0 illustrates this clearly:\n\n\n\n\n\nHere is another example:\n\n\n\n\n\n(Source: Fox News, via Flowing Data2.)\nThis plot makes a 13% increase look like a five fold change. Here is the appropriate plot:\n\n\n\n\n\nFinally, here is an extreme example that makes a very small difference of under 2% look like a 10-100 fold change:\n\n\n\n\n\n(Source: Venezolana de Televisión via Pakistan Today3 and Diego Mariano.)\nHere is the appropriate plot:\n\n\n\n\n\nWhen using position rather than length, it is then not necessary to include 0. This is particularly the case when we want to compare differences between groups relative to the within-group variability. Here is an illustrative example showing country average life expectancy stratified across continents in 2012:\n\n\n\n\n\nNote that in the plot on the left, which includes 0, the space between 0 and 43 adds no information and makes it harder to compare the between and within group variability."
  },
  {
    "objectID": "12-dataviz-principles.html#do-not-distort-quantities",
    "href": "12-dataviz-principles.html#do-not-distort-quantities",
    "title": "12  Data visualization principles",
    "section": "12.3 Do not distort quantities",
    "text": "12.3 Do not distort quantities\nDuring President Barack Obama’s 2011 State of the Union Address, the following chart was used to compare the US GDP to the GDP of four competing nations:\n\n\n\n\n\n(Source: The 2011 State of the Union Address4)\nJudging by the area of the circles, the US appears to have an economy over five times larger than China’s and over 30 times larger than France’s. However, if we look at the actual numbers, we see that this is not the case. The actual ratios are 2.6 and 5.8 times bigger than China and France, respectively. The reason for this distortion is that the radius, rather than the area, was made to be proportional to the quantity, which implies that the proportion between the areas is squared: 2.6 turns into 6.5 and 5.8 turns into 34.1. Here is a comparison of the circles we get if we make the value proportional to the radius and to the area:\n\n\n\n\n\nNot surprisingly, ggplot2 defaults to using area rather than radius. Of course, in this case, we really should not be using area at all since we can use position and length:"
  },
  {
    "objectID": "12-dataviz-principles.html#order-categories-by-a-meaningful-value",
    "href": "12-dataviz-principles.html#order-categories-by-a-meaningful-value",
    "title": "12  Data visualization principles",
    "section": "12.4 Order categories by a meaningful value",
    "text": "12.4 Order categories by a meaningful value\nWhen one of the axes is used to show categories, as is done in barplots, the default ggplot2 behavior is to order the categories alphabetically when they are defined by character strings. If they are defined by factors, they are ordered by the factor levels. We rarely want to use alphabetical order. Instead, we should order by a meaningful quantity. In all the cases above, the barplots were ordered by the values being displayed. The exception was the graph showing barplots comparing browsers. In this case, we kept the order the same across the barplots to ease the comparison. Specifically, instead of ordering the browsers separately in the two years, we ordered both years by the average value of 2000 and 2015.\n\n\n\n\n\nWe can make the second plot like this:\n\nmurders |&gt; mutate(murder_rate = total / population * 100000) |&gt;\n  mutate(state = reorder(state, murder_rate)) |&gt;\n  ggplot(aes(state, murder_rate)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  theme(axis.text.y = element_text(size = 6)) +\n  xlab(\"\")\n\nThe reorder function lets us reorder groups as well. Earlier we saw an example related to income distributions across regions. Here are the two versions plotted against each other:\n\n\n\n\n\nThe first orders the regions alphabetically, while the second orders them by the group’s median."
  },
  {
    "objectID": "12-dataviz-principles.html#show-the-data",
    "href": "12-dataviz-principles.html#show-the-data",
    "title": "12  Data visualization principles",
    "section": "12.5 Show the data",
    "text": "12.5 Show the data\nTo motivate our first principle, “show the data”, we go back to our artificial example of describing heights to ET, an extraterrestrial. This time let’s assume ET is interested in the difference in heights between males and females. A commonly seen plot used for comparisons between groups, popularized by software such as Microsoft Excel, is the dynamite plot, which shows the average and standard errors (standard errors are defined in a later chapter, but do not confuse them with the standard deviation of the data). The plot looks like this:\n\n\n\n\n\nThe average of each group is represented by the top of each bar and the antennae extend out from the average to the average plus two standard errors. If all ET receives is this plot, he will have little information on what to expect if he meets a group of human males and females. The bars go to 0: does this mean there are tiny humans measuring less than one foot? Are all males taller than the tallest females? Is there a range of heights? ET can’t answer these questions since we have provided almost no information on the height distribution.\nThis brings us to our first principle: show the data. This simple ggplot2 code already generates a more informative plot than the barplot by simply showing all the data points:\n\nheights |&gt; \n  ggplot(aes(sex, height)) + \n  geom_point() \n\n\n\n\nFor example, this plot gives us an idea of the range of the data. However, this plot has limitations as well, since we can’t really see all the 238 and 812 points plotted for females and males, respectively, and many points are plotted on top of each other. As we have previously described, visualizing the distribution is much more informative. But before doing this, we point out two ways we can improve a plot showing all the points.\nThe first is to add jitter, which adds a small random shift to each point. In this case, adding horizontal jitter does not alter the interpretation, since the point heights do not change, but we minimize the number of points that fall on top of each other and, therefore, get a better visual sense of how the data is distributed. A second improvement comes from using alpha blending: making the points somewhat transparent. The more points fall on top of each other, the darker the plot, which also helps us get a sense of how the points are distributed. Here is the same plot with jitter and alpha blending:\n\nheights |&gt; \n  ggplot(aes(sex, height)) +\n  geom_jitter(width = 0.1, alpha = 0.2) \n\n\n\n\nNow we start getting a sense that, on average, males are taller than females. We also note dark horizontal bands of points, demonstrating that many report values that are rounded to the nearest integer."
  },
  {
    "objectID": "12-dataviz-principles.html#ease-comparisons",
    "href": "12-dataviz-principles.html#ease-comparisons",
    "title": "12  Data visualization principles",
    "section": "12.6 Ease comparisons",
    "text": "12.6 Ease comparisons\n\n12.6.1 Use common axes\nSince there are so many points, it is more effective to show distributions rather than individual points. We therefore show histograms for each group:\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nHowever, from this plot it is not immediately obvious that males are, on average, taller than females. We have to look carefully to notice that the x-axis has a higher range of values in the male histogram. An important principle here is to keep the axes the same when comparing data across two plots. Below we see how the comparison becomes easier:\n\n\n\n\n\n\n\n12.6.2 Align plots vertically to see horizontal changes and horizontally to see vertical changes\nIn these histograms, the visual cue related to decreases or increases in height are shifts to the left or right, respectively: horizontal changes. Aligning the plots vertically helps us see this change when the axes are fixed:\n\n\n\n\n\n\nheights |&gt; \n  ggplot(aes(height, ..density..)) +\n  geom_histogram(binwidth = 1, color=\"black\") +\n  facet_grid(sex~.)\n\nThis plot makes it much easier to notice that men are, on average, taller.\nIf , we want the more compact summary provided by boxplots, we then align them horizontally since, by default, boxplots move up and down with changes in height. Following our show the data principle, we then overlay all the data points:\n\n\n\n\n\n\n heights |&gt; \n  ggplot(aes(sex, height)) + \n  geom_boxplot(coef=3) + \n  geom_jitter(width = 0.1, alpha = 0.2) +\n  ylab(\"Height in inches\")\n\nNow contrast and compare these three plots, based on exactly the same data:\n\n\n\n\n\nNotice how much more we learn from the two plots on the right. Barplots are useful for showing one number, but not very useful when we want to describe distributions.\n\n\n12.6.3 Consider transformations\nWe have motivated the use of the log transformation in cases where the changes are multiplicative. Population size was an example in which we found a log transformation to yield a more informative transformation.\nThe combination of an incorrectly chosen barplot and a failure to use a log transformation when one is merited can be particularly distorting. As an example, consider this barplot showing the average population sizes for each continent in 2015:\n\n\n\n\n\nFrom this plot, one would conclude that countries in Asia are much more populous than in other continents. Following the show the data principle, we quickly notice that this is due to two very large countries, which we assume are India and China:\n\n\n\n\n\nUsing a log transformation here provides a much more informative plot. We compare the original barplot to a boxplot using the log scale transformation for the y-axis:\n\n\n\n\n\nWith the new plot, we realize that countries in Africa actually have a larger median population size than those in Asia.\nOther transformations you should consider are the logistic transformation (logit), useful to better see fold changes in odds, and the square root transformation (sqrt), useful for count data.\n\n\n12.6.4 Visual cues to be compared should be adjacent\nFor each continent, let’s compare income in 1970 versus 2010. When comparing income data across regions between 1970 and 2010, we made a figure similar to the one below, but this time we investigate continents rather than regions.\n\n\n\n\n\nThe default in ggplot2 is to order labels alphabetically so the labels with 1970 come before the labels with 2010, making the comparisons challenging because a continent’s distribution in 1970 is visually far from its distribution in 2010. It is much easier to make the comparison between 1970 and 2010 for each continent when the boxplots for that continent are next to each other:\n\n\n\n\n\n\n\n12.6.5 Use color\nThe comparison becomes even easier to make if we use color to denote the two things we want to compare:"
  },
  {
    "objectID": "12-dataviz-principles.html#think-of-the-color-blind",
    "href": "12-dataviz-principles.html#think-of-the-color-blind",
    "title": "12  Data visualization principles",
    "section": "12.7 Think of the color blind",
    "text": "12.7 Think of the color blind\nAbout 10% of the population is color blind. Unfortunately, the default colors used in ggplot2 are not optimal for this group. However, ggplot2 does make it easy to change the color palette used in the plots. An example of how we can use a color blind friendly palette is described here: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette:\n\ncolor_blind_friendly_cols &lt;- \n  c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n    \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nHere are the colors\n\n\n\n\n\nThere are several resources that can help you select colors, for example this one: http://bconnelly.net/2013/10/creating-colorblind-friendly-figures/."
  },
  {
    "objectID": "12-dataviz-principles.html#plots-for-two-variables",
    "href": "12-dataviz-principles.html#plots-for-two-variables",
    "title": "12  Data visualization principles",
    "section": "12.8 Plots for two variables",
    "text": "12.8 Plots for two variables\nIn general, you should use scatterplots to visualize the relationship between two variables. In every single instance in which we have examined the relationship between two variables, including total murders versus population size, life expectancy versus fertility rates, and infant mortality versus income, we have used scatterplots. This is the plot we generally recommend. However, there are some exceptions and we describe two alternative plots here: the slope chart and the Bland-Altman plot.\n\n12.8.1 Slope charts\nOne exception where another type of plot may be more informative is when you are comparing variables of the same type, but at different time points and for a relatively small number of comparisons. For example, comparing life expectancy between 2010 and 2015. In this case, we might recommend a slope chart.\nThere is no geometry for slope charts in ggplot2, but we can construct one using geom_line. We need to do some tinkering to add labels. Below is an example comparing 2010 to 2015 for large western countries:\n\nwest &lt;- c(\"Western Europe\",\"Northern Europe\",\"Southern Europe\",\n          \"Northern America\",\"Australia and New Zealand\")\n\ndat &lt;- gapminder |&gt; \n  filter(year %in% c(2010, 2015) & region %in% west & \n           !is.na(life_expectancy) & population &gt; 10^7) \n\ndat |&gt;\n  mutate(location = ifelse(year == 2010, 1, 2), \n         location = ifelse(year == 2015 & \n                             country %in% c(\"United Kingdom\", \"Portugal\"),\n                           location+0.22, location),\n         hjust = ifelse(year == 2010, 1, 0)) |&gt;\n  mutate(year = as.factor(year)) |&gt;\n  ggplot(aes(year, life_expectancy, group = country)) +\n  geom_line(aes(color = country), show.legend = FALSE) +\n  geom_text(aes(x = location, label = country, hjust = hjust), \n            show.legend = FALSE) +\n  xlab(\"\") + ylab(\"Life Expectancy\")\n\n\n\n\nAn advantage of the slope chart is that it permits us to quickly get an idea of changes based on the slope of the lines. Although we are using angle as the visual cue, we also have position to determine the exact values. Comparing the improvements is a bit harder with a scatterplot:\n\n\n\n\n\nIn the scatterplot, we have followed the principle use common axes since we are comparing these before and after. However, if we have many points, slope charts stop being useful as it becomes hard to see all the lines.\n\n\n12.8.2 Bland-Altman plot\nSince we are primarily interested in the difference, it makes sense to dedicate one of our axes to it. The Bland-Altman plot, also known as the Tukey mean-difference plot and the MA-plot, shows the difference versus the average:\n\nlibrary(ggrepel)\ndat |&gt; \n  mutate(year = paste0(\"life_expectancy_\", year)) |&gt;\n  select(country, year, life_expectancy) |&gt; \n  pivot_wider(names_from = \"year\", values_from=\"life_expectancy\") |&gt; \n  mutate(average = (life_expectancy_2015 + life_expectancy_2010)/2,\n         difference = life_expectancy_2015 - life_expectancy_2010) |&gt;\n  ggplot(aes(average, difference, label = country)) + \n  geom_point() +\n  geom_text_repel() +\n  geom_abline(lty = 2) +\n  xlab(\"Average of 2010 and 2015\") + \n  ylab(\"Difference between 2015 and 2010\")\n\n\n\n\nHere, by simply looking at the y-axis, we quickly see which countries have shown the most improvement. We also get an idea of the overall value from the x-axis."
  },
  {
    "objectID": "12-dataviz-principles.html#encoding-a-third-variable",
    "href": "12-dataviz-principles.html#encoding-a-third-variable",
    "title": "12  Data visualization principles",
    "section": "12.9 Encoding a third variable",
    "text": "12.9 Encoding a third variable\nAn earlier scatterplot showed the relationship between infant survival and average income. Below is a version of this plot that encodes three variables: OPEC membership, region, and population.\n\n\n\n\n\nWe encode categorical variables with color and shape. These shapes can be controlled with shape argument. Below are the shapes available for use in R. For the last five, the color goes inside.\n\n\n\n\n\nFor continuous variables, we can use color, intensity, or size. We now show an example of how we do this with a case study.\nWhen selecting colors to quantify a numeric variable, we choose between two options: sequential and diverging. Sequential colors are suited for data that goes from high to low. High values are clearly distinguished from low values. Here are some examples offered by the package RColorBrewer:\n\nlibrary(RColorBrewer)\ndisplay.brewer.all(type=\"seq\")\n\n\n\n\n\n\nDiverging colors are used to represent values that diverge from a center. We put equal emphasis on both ends of the data range: higher than the center and lower than the center. An example of when we would use a divergent pattern would be if we were to show height in standard deviations away from the average. Here are some examples of divergent patterns:\n\nlibrary(RColorBrewer)\ndisplay.brewer.all(type=\"div\")"
  },
  {
    "objectID": "12-dataviz-principles.html#avoid-pseudo-three-dimensional-plots",
    "href": "12-dataviz-principles.html#avoid-pseudo-three-dimensional-plots",
    "title": "12  Data visualization principles",
    "section": "12.10 Avoid pseudo-three-dimensional plots",
    "text": "12.10 Avoid pseudo-three-dimensional plots\nThe figure below, taken from the scientific literature5, shows three variables: dose, drug type and survival. Although your screen/book page is flat and two-dimensional, the plot tries to imitate three dimensions and assigned a dimension to each variable.\n\n\n\n\n\n(Image courtesy of Karl Broman)\nHumans are not good at seeing in three dimensions (which explains why it is hard to parallel park) and our limitation is even worse with regard to pseudo-three-dimensions. To see this, try to determine the values of the survival variable in the plot above. Can you tell when the purple ribbon intersects the red one? This is an example in which we can easily use color to represent the categorical variable instead of using a pseudo-3D:\n\n\n\n\n\nNotice how much easier it is to determine the survival values.\nPseudo-3D is sometimes used completely gratuitously: plots are made to look 3D even when the 3rd dimension does not represent a quantity. This only adds confusion and makes it harder to relay your message. Here are two examples:\n\n\n\n\n\n\n\n\n(Images courtesy of Karl Broman)"
  },
  {
    "objectID": "12-dataviz-principles.html#avoid-too-many-significant-digits",
    "href": "12-dataviz-principles.html#avoid-too-many-significant-digits",
    "title": "12  Data visualization principles",
    "section": "12.11 Avoid too many significant digits",
    "text": "12.11 Avoid too many significant digits\nBy default, statistical software like R returns many significant digits. The default behavior in R is to show 7 significant digits. That many digits often adds no information and the added visual clutter can make it hard for the viewer to understand the message. As an example, here are the per 10,000 disease rates, computed from totals and population in R, for California across the five decades:\n\n\n\n\n\nstate\nyear\nMeasles\nPertussis\nPolio\n\n\n\n\nCalifornia\n1940\n37.8826320\n18.3397861\n0.8266512\n\n\nCalifornia\n1950\n13.9124205\n4.7467350\n1.9742639\n\n\nCalifornia\n1960\n14.1386471\nNA\n0.2640419\n\n\nCalifornia\n1970\n0.9767889\nNA\nNA\n\n\nCalifornia\n1980\n0.3743467\n0.0515466\nNA\n\n\n\n\n\n\n\nWe are reporting precision up to 0.00001 cases per 10,000, a very small value in the context of the changes that are occurring across the dates. In this case, two significant figures is more than enough and clearly makes the point that rates are decreasing:\n\n\n\n\n\nstate\nyear\nMeasles\nPertussis\nPolio\n\n\n\n\nCalifornia\n1940\n37.9\n18.3\n0.8\n\n\nCalifornia\n1950\n13.9\n4.7\n2.0\n\n\nCalifornia\n1960\n14.1\nNA\n0.3\n\n\nCalifornia\n1970\n1.0\nNA\nNA\n\n\nCalifornia\n1980\n0.4\n0.1\nNA\n\n\n\n\n\n\n\nUseful ways to change the number of significant digits or to round numbers are signif and round. You can define the number of significant digits globally by setting options like this: options(digits = 3).\nAnother principle related to displaying tables is to place values being compared on columns rather than rows. Note that our table above is easier to read than this one:\n\n\n\n\n\nstate\ndisease\n1940\n1950\n1960\n1970\n1980\n\n\n\n\nCalifornia\nMeasles\n37.9\n13.9\n14.1\n1\n0.4\n\n\nCalifornia\nPertussis\n18.3\n4.7\nNA\nNA\n0.1\n\n\nCalifornia\nPolio\n0.8\n2.0\n0.3\nNA\nNA"
  },
  {
    "objectID": "12-dataviz-principles.html#know-your-audience",
    "href": "12-dataviz-principles.html#know-your-audience",
    "title": "12  Data visualization principles",
    "section": "12.12 Know your audience",
    "text": "12.12 Know your audience\nGraphs can be used for 1) our own exploratory data analysis, 2) to convey a message to experts, or 3) to help tell a story to a general audience. Make sure that the intended audience understands each element of the plot.\nAs a simple example, consider that for your own exploration it may be more useful to log-transform data and then plot it. However, for a general audience that is unfamiliar with converting logged values back to the original measurements, using a log-scale for the axis instead of log-transformed values will be much easier to digest."
  },
  {
    "objectID": "12-dataviz-principles.html#exercises",
    "href": "12-dataviz-principles.html#exercises",
    "title": "12  Data visualization principles",
    "section": "12.13 Exercises",
    "text": "12.13 Exercises\nFor these exercises, we will be using the vaccines data in the dslabs package:\n\nlibrary(dslabs)\n\n1. Pie charts are appropriate:\n\nWhen we want to display percentages.\nWhen ggplot2 is not available.\nWhen I am in a bakery.\nNever. Barplots and tables are always better.\n\n2. What is the problem with the plot below:\n\n\n\n\n\n\nThe values are wrong. The final vote was 306 to 232.\nThe axis does not start at 0. Judging by the length, it appears Trump received 3 times as many votes when, in fact, it was about 30% more.\nThe colors should be the same.\nPercentages should be shown as a pie chart.\n\n3. Take a look at the following two plots. They show the same information: 1928 rates of measles across the 50 states.\n\n\n\n\n\nWhich plot is easier to read if you are interested in determining which are the best and worst states in terms of rates, and why?\n\nThey provide the same information, so they are both equally as good.\nThe plot on the right is better because it orders the states alphabetically.\nThe plot on the right is better because alphabetical order has nothing to do with the disease and by ordering according to actual rate, we quickly see the states with most and least rates.\nBoth plots should be a pie chart.\n\n4. To make the plot on the left, we have to reorder the levels of the states’ variables.\n\ndat &lt;- us_contagious_diseases |&gt;  \n  filter(year == 1967 & disease==\"Measles\" & !is.na(population)) |&gt;\n  mutate(rate = count / population * 10000 * 52 / weeks_reporting)\n\nNote what happens when we make a barplot:\n\ndat |&gt; ggplot(aes(state, rate)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() \n\n\n\n\nDefine these objects:\n\nstate &lt;- dat$state\nrate &lt;- dat$count/dat$population*10000*52/dat$weeks_reporting\n\nRedefine the state object so that the levels are re-ordered. Print the new object state and its levels so you can see that the vector is not re-ordered by the levels.\n5. Now with one line of code, define the dat table as done above, but change the use mutate to create a rate variable and re-order the state variable so that the levels are re-ordered by this variable. Then make a barplot using the code above, but for this new dat.\n6. Say we are interested in comparing gun homicide rates across regions of the US. We see this plot:\n\nlibrary(dslabs)\nmurders |&gt; mutate(rate = total/population*100000) |&gt;\ngroup_by(region) |&gt;\nsummarize(avg = mean(rate)) |&gt;\nmutate(region = factor(region)) |&gt;\nggplot(aes(region, avg)) +\ngeom_bar(stat=\"identity\") +\nylab(\"Murder Rate Average\")\n\n\n\n\nand decide to move to a state in the western region. What is the main problem with this interpretation?\n\nThe categories are ordered alphabetically.\nThe graph does not show standarad errors.\nIt does not show all the data. We do not see the variability within a region and it’s possible that the safest states are not in the West.\nThe Northeast has the lowest average.\n\n7. Make a boxplot of the murder rates defined as\n\nmurders |&gt; mutate(rate = total/population*100000)\n\nby region, showing all the points and ordering the regions by their median rate.\n8. The plots below show three continuous variables.\n\n\n\n\n\nThe line \\(x=2\\) appears to separate the points. But it is actually not the case, which we can see by plotting the data in a couple of two-dimensional points.\n\n\n\n\n\nWhy is this happening?\n\nHumans are not good at reading pseudo-3D plots.\nThere must be an error in the code.\nThe colors confuse us.\nScatterplots should not be used to compare two variables when we have access to 3."
  },
  {
    "objectID": "12-dataviz-principles.html#footnotes",
    "href": "12-dataviz-principles.html#footnotes",
    "title": "12  Data visualization principles",
    "section": "",
    "text": "http://mediamatters.org/blog/2013/04/05/fox-news-newest-dishonest-chart-immigration-enf/193507↩︎\nhttp://flowingdata.com/2012/08/06/fox-news-continues-charting-excellence/↩︎\nhttps://www.pakistantoday.com.pk/2018/05/18/whats-at-stake-in-venezuelan-presidential-vote↩︎\nhttps://www.youtube.com/watch?v=kl2g40GoRxg↩︎\nhttps://projecteuclid.org/download/pdf_1/euclid.ss/1177010488↩︎"
  },
  {
    "objectID": "13-wrangling.html#reshaping-data",
    "href": "13-wrangling.html#reshaping-data",
    "title": "13  Wrangling",
    "section": "13.1 Reshaping data",
    "text": "13.1 Reshaping data\n\nlibrary(tidyverse) \nlibrary(dslabs)\npath &lt;- system.file(\"extdata\", package = \"dslabs\")\nfilename &lt;- file.path(path, \"fertility-two-countries-example.csv\")\nwide_data &lt;- read_csv(filename)\n\n\n13.1.1 pivot_longer\n\nwide_data |&gt; pivot_longer(`1960`:`2015`)\n\n# A tibble: 112 × 3\n   country name  value\n   &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;\n 1 Germany 1960   2.41\n 2 Germany 1961   2.44\n 3 Germany 1962   2.47\n 4 Germany 1963   2.49\n 5 Germany 1964   2.49\n 6 Germany 1965   2.48\n 7 Germany 1966   2.44\n 8 Germany 1967   2.37\n 9 Germany 1968   2.28\n10 Germany 1969   2.17\n# ℹ 102 more rows\n\n\nWe can also use the pipe like this:\n\nnew_tidy_data &lt;- wide_data |&gt; \n  pivot_longer(`1960`:`2015`, names_to = \"year\", values_to = \"fertility\")\nhead(new_tidy_data)\n\n# A tibble: 6 × 3\n  country year  fertility\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 Germany 1960       2.41\n2 Germany 1961       2.44\n3 Germany 1962       2.47\n4 Germany 1963       2.49\n5 Germany 1964       2.49\n6 Germany 1965       2.48\n\n\nUsually its easier to name the columns not to be pivoted.\n\nnew_tidy_data &lt;- wide_data |&gt;\n  pivot_longer(-country, names_to = \"year\", values_to = \"fertility\")\n\nThe new_tidy_data object looks like the original tidy_data we defined this way\n\ntidy_data &lt;- gapminder |&gt; \n  filter(country %in% c(\"South Korea\", \"Germany\") & !is.na(fertility)) |&gt;\n  select(country, year, fertility)\n\nwith just one minor difference. Can you spot it? Look at the data type of the year column:\n\nclass(tidy_data$year)\n\n[1] \"integer\"\n\nclass(new_tidy_data$year)\n\n[1] \"character\"\n\n\nThe pivot_longer function assumes that column names are characters. So we need a bit more wrangling before we are ready to make a plot. We need to convert the year column to be numbers:\n\nnew_tidy_data &lt;- wide_data |&gt;\n  pivot_longer(-country, names_to = \"year\", values_to = \"fertility\") |&gt;\n  mutate(year = as.integer(year))\n\nNow that the data is tidy, we can use this relatively simple ggplot code:\n\nnew_tidy_data |&gt; ggplot(aes(year, fertility, color = country)) + \n  geom_point()\n\n\n\n13.1.2 pivot_wider\n\nnew_wide_data &lt;- new_tidy_data |&gt; \n  pivot_wider(names_from = year, values_from = fertility)\nselect(new_wide_data, country, `1960`:`1967`)\n\n# A tibble: 2 × 9\n  country     `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Germany       2.41   2.44   2.47   2.49   2.49   2.48   2.44   2.37\n2 South Korea   6.16   5.99   5.79   5.57   5.36   5.16   4.99   4.85\n\n\nSimilar to pivot_wider, names_from and values_from default to name and value.\n\n\n13.1.3 separate\nThe data wrangling shown above was simple compared to what is usually required. In our example spreadsheet files, we include an illustration that is slightly more complicated. It contains two variables: life expectancy and fertility. However, the way it is stored is not tidy and, as we will explain, not optimal.\n\npath &lt;- system.file(\"extdata\", package = \"dslabs\")\n\nfilename &lt;- \"life-expectancy-and-fertility-two-countries-example.csv\"\nfilename &lt;-  file.path(path, filename)\n\nraw_dat &lt;- read_csv(filename)\nselect(raw_dat, 1:5)\n\n# A tibble: 2 × 5\n  country     `1960_fertility` `1960_life_expectancy` `1961_fertility`\n  &lt;chr&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;            &lt;dbl&gt;\n1 Germany                 2.41                   69.3             2.44\n2 South Korea             6.16                   53.0             5.99\n# ℹ 1 more variable: `1961_life_expectancy` &lt;dbl&gt;\n\n\n\ndat &lt;- raw_dat |&gt; pivot_longer(-country)\nhead(dat)\n\n# A tibble: 6 × 3\n  country name                 value\n  &lt;chr&gt;   &lt;chr&gt;                &lt;dbl&gt;\n1 Germany 1960_fertility        2.41\n2 Germany 1960_life_expectancy 69.3 \n3 Germany 1961_fertility        2.44\n4 Germany 1961_life_expectancy 69.8 \n5 Germany 1962_fertility        2.47\n6 Germany 1962_life_expectancy 70.0 \n\n\nThe result is not exactly what we refer to as tidy since each observation is associated with two, not one, rows. We want to have the values from the two variables, fertility and life expectancy, in two separate columns. The first challenge to achieve this is to separate the name column into the year and the variable type. Notice that the entries in this column separate the year from the variable name with an underscore:\n\ndat$name[1:5]\n\n[1] \"1960_fertility\"       \"1960_life_expectancy\" \"1961_fertility\"      \n[4] \"1961_life_expectancy\" \"1962_fertility\"      \n\n\nEncoding multiple variables in a column name is such a common problem that the readr package includes a function to separate these columns into two or more. Apart from the data, the separate function takes three arguments: the name of the column to be separated, the names to be used for the new columns, and the character that separates the variables. So, a first attempt at this is:\n\ndat |&gt; separate(name, c(\"year\", \"name\"), \"_\")\n\nBecause _ is the default separator assumed by separate, we do not have to include it in the code:\n\ndat |&gt; separate(name, c(\"year\", \"name\"))\n\nWarning: Expected 2 pieces. Additional pieces discarded in 112 rows [2, 4, 6, 8, 10, 12,\n14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, ...].\n\n\n# A tibble: 224 × 4\n   country year  name      value\n   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 Germany 1960  fertility  2.41\n 2 Germany 1960  life      69.3 \n 3 Germany 1961  fertility  2.44\n 4 Germany 1961  life      69.8 \n 5 Germany 1962  fertility  2.47\n 6 Germany 1962  life      70.0 \n 7 Germany 1963  fertility  2.49\n 8 Germany 1963  life      70.1 \n 9 Germany 1964  fertility  2.49\n10 Germany 1964  life      70.7 \n# ℹ 214 more rows\n\n\nWe get a warning. Here we tell it to fill the column on the right:\n\nvar_names &lt;- c(\"year\", \"first_variable_name\", \"second_variable_name\")\ndat |&gt; separate(name, var_names, fill = \"right\")\n\n# A tibble: 224 × 5\n   country year  first_variable_name second_variable_name value\n   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;               &lt;chr&gt;                &lt;dbl&gt;\n 1 Germany 1960  fertility           &lt;NA&gt;                  2.41\n 2 Germany 1960  life                expectancy           69.3 \n 3 Germany 1961  fertility           &lt;NA&gt;                  2.44\n 4 Germany 1961  life                expectancy           69.8 \n 5 Germany 1962  fertility           &lt;NA&gt;                  2.47\n 6 Germany 1962  life                expectancy           70.0 \n 7 Germany 1963  fertility           &lt;NA&gt;                  2.49\n 8 Germany 1963  life                expectancy           70.1 \n 9 Germany 1964  fertility           &lt;NA&gt;                  2.49\n10 Germany 1964  life                expectancy           70.7 \n# ℹ 214 more rows\n\n\nHowever, if we read the separate help file, we find that a better approach is to merge the last two variables when there is an extra separation:\n\ndat |&gt; separate(name, c(\"year\", \"name\"), extra = \"merge\")\n\n# A tibble: 224 × 4\n   country year  name            value\n   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Germany 1960  fertility        2.41\n 2 Germany 1960  life_expectancy 69.3 \n 3 Germany 1961  fertility        2.44\n 4 Germany 1961  life_expectancy 69.8 \n 5 Germany 1962  fertility        2.47\n 6 Germany 1962  life_expectancy 70.0 \n 7 Germany 1963  fertility        2.49\n 8 Germany 1963  life_expectancy 70.1 \n 9 Germany 1964  fertility        2.49\n10 Germany 1964  life_expectancy 70.7 \n# ℹ 214 more rows\n\n\nThis achieves the separation we wanted. However, we are not done yet. We need to create a column for each variable. As we learned, the pivot_wider function can do this:\n\ndat |&gt; \n  separate(name, c(\"year\", \"name\"), extra = \"merge\") |&gt;\n  pivot_wider() |&gt;\n  mutate(year = as.integer(year)) |&gt;\n  ggplot(aes(fertility, life_expectancy, color = country)) + geom_point()\n\n\n\n\nThe data is now in tidy format with one row for each observation with three variables: year, fertility, and life expectancy.\n\n\n13.1.4 unite\nIt is sometimes useful to do the inverse of separate, unite two columns into one. To demonstrate how to use unite, we show code that, although not the optimal approach, serves as an illustration. Suppose that we did not know about extra and used this command to separate:\n\nvar_names &lt;- c(\"year\", \"first_variable_name\", \"second_variable_name\")\ndat |&gt; \n  separate(name, var_names, fill = \"right\")\n\n# A tibble: 224 × 5\n   country year  first_variable_name second_variable_name value\n   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;               &lt;chr&gt;                &lt;dbl&gt;\n 1 Germany 1960  fertility           &lt;NA&gt;                  2.41\n 2 Germany 1960  life                expectancy           69.3 \n 3 Germany 1961  fertility           &lt;NA&gt;                  2.44\n 4 Germany 1961  life                expectancy           69.8 \n 5 Germany 1962  fertility           &lt;NA&gt;                  2.47\n 6 Germany 1962  life                expectancy           70.0 \n 7 Germany 1963  fertility           &lt;NA&gt;                  2.49\n 8 Germany 1963  life                expectancy           70.1 \n 9 Germany 1964  fertility           &lt;NA&gt;                  2.49\n10 Germany 1964  life                expectancy           70.7 \n# ℹ 214 more rows\n\n\nWe can achieve the same final result by uniting the second and third columns, then pivoting the columns and renaming fertility_NA to fertility:\n\ndat |&gt; \n  separate(name, var_names, fill = \"right\") |&gt;\n  unite(name, first_variable_name, second_variable_name) |&gt;\n  pivot_wider() |&gt;\n  rename(fertility = fertility_NA)\n\n# A tibble: 112 × 4\n   country year  fertility life_expectancy\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1 Germany 1960       2.41            69.3\n 2 Germany 1961       2.44            69.8\n 3 Germany 1962       2.47            70.0\n 4 Germany 1963       2.49            70.1\n 5 Germany 1964       2.49            70.7\n 6 Germany 1965       2.48            70.6\n 7 Germany 1966       2.44            70.8\n 8 Germany 1967       2.37            71.0\n 9 Germany 1968       2.28            70.6\n10 Germany 1969       2.17            70.5\n# ℹ 102 more rows"
  },
  {
    "objectID": "13-wrangling.html#joining-tables",
    "href": "13-wrangling.html#joining-tables",
    "title": "13  Wrangling",
    "section": "13.2 Joining tables",
    "text": "13.2 Joining tables\nThe information we need for a given analysis may not be just in one table. For example, when forecasting elections we used the function left_join to combine the information from two tables. Here we use a simpler example to illustrate the general challenge of combining tables.\nSuppose we want to explore the relationship between population size for US states and electoral votes. We have the population size in this table:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nand electoral votes in this one:\n\nhead(results_us_election_2016)\n\n         state electoral_votes clinton trump others\n1   California              55    61.7  31.6    6.7\n2        Texas              38    43.2  52.2    4.5\n3      Florida              29    47.8  49.0    3.2\n4     New York              29    59.0  36.5    4.5\n5     Illinois              20    55.8  38.8    5.4\n6 Pennsylvania              20    47.9  48.6    3.6\n\n\nJust concatenating these two tables together will not work since the order of the states is not the same.\n\nidentical(results_us_election_2016$state, murders$state)\n\n[1] FALSE\n\n\nThe join functions, described below, are designed to handle this challenge.\n\n13.2.1 Joins\nThe join functions in the dplyr package make sure that the tables are combined so that matching rows are together. If you know SQL, you will see that the approach and syntax is very similar. The general idea is that one needs to identify one or more columns that will serve to match the two tables. Then a new table with the combined information is returned. Notice what happens if we join the two tables above by state using left_join (we will remove the others column and rename electoral_votes so that the tables fit on the page):\n\ntab &lt;- left_join(murders, results_us_election_2016, by = \"state\") |&gt;\n  select(-others) |&gt; rename(ev = electoral_votes)\nhead(tab)\n\n       state abb region population total ev clinton trump\n1    Alabama  AL  South    4779736   135  9    34.4  62.1\n2     Alaska  AK   West     710231    19  3    36.6  51.3\n3    Arizona  AZ   West    6392017   232 11    45.1  48.7\n4   Arkansas  AR  South    2915918    93  6    33.7  60.6\n5 California  CA   West   37253956  1257 55    61.7  31.6\n6   Colorado  CO   West    5029196    65  9    48.2  43.3\n\n\nThe data has been successfully joined and we can now, for example, make a plot to explore the relationship:\n\nlibrary(ggrepel)\ntab |&gt; ggplot(aes(population/10^6, ev)) +\n  geom_point() +\n  geom_text_repel(aes(label = abb), max.overlaps = 20) + \n  scale_x_continuous(trans = \"log2\") +\n  scale_y_continuous(trans = \"log2\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nWe see the relationship is close to linear with about 2 electoral votes for every million persons, but with very small states getting higher ratios.\nIn practice, it is not always the case that each row in one table has a matching row in the other. For this reason, we have several versions of join. To illustrate this challenge, we will take subsets of the tables above. We create the tables tab1 and tab2 so that they have some states in common but not all:\n\ntab_1 &lt;- slice(murders, 1:6) |&gt; select(state, population)\ntab_1\n\n       state population\n1    Alabama    4779736\n2     Alaska     710231\n3    Arizona    6392017\n4   Arkansas    2915918\n5 California   37253956\n6   Colorado    5029196\n\ntab_2 &lt;- results_us_election_2016 |&gt; \n  filter(state %in% c(\"Alabama\", \"Alaska\", \"Arizona\", \n                    \"California\", \"Connecticut\", \"Delaware\")) |&gt; \n  select(state, electoral_votes) |&gt; rename(ev = electoral_votes)\ntab_2\n\n        state ev\n1  California 55\n2     Arizona 11\n3     Alabama  9\n4 Connecticut  7\n5      Alaska  3\n6    Delaware  3\n\n\nWe will use these two tables as examples in the next sections.\n\n13.2.1.1 Left join\nSuppose we want a table like tab_1, but adding electoral votes to whatever states we have available. For this, we use left_join with tab_1 as the first argument. We specify which column to use to match with the by argument.\n\nleft_join(tab_1, tab_2, by = \"state\")\n\n       state population ev\n1    Alabama    4779736  9\n2     Alaska     710231  3\n3    Arizona    6392017 11\n4   Arkansas    2915918 NA\n5 California   37253956 55\n6   Colorado    5029196 NA\n\n\nNote that NAs are added to the two states not appearing in tab_2. Also, notice that this function, as well as all the other joins, can receive the first arguments through the pipe:\n\ntab_1 |&gt; left_join(tab_2, by = \"state\")\n\n\n\n13.2.1.2 Right join\nIf instead of a table with the same rows as first table, we want one with the same rows as second table, we can use right_join:\n\ntab_1 |&gt; right_join(tab_2, by = \"state\")\n\n        state population ev\n1     Alabama    4779736  9\n2      Alaska     710231  3\n3     Arizona    6392017 11\n4  California   37253956 55\n5 Connecticut         NA  7\n6    Delaware         NA  3\n\n\nNow the NAs are in the column coming from tab_1.\n\n\n13.2.1.3 Inner join\nIf we want to keep only the rows that have information in both tables, we use inner_join. You can think of this as an intersection:\n\ninner_join(tab_1, tab_2, by = \"state\")\n\n       state population ev\n1    Alabama    4779736  9\n2     Alaska     710231  3\n3    Arizona    6392017 11\n4 California   37253956 55\n\n\n\n\n13.2.1.4 Full join\nIf we want to keep all the rows and fill the missing parts with NAs, we can use full_join. You can think of this as a union:\n\nfull_join(tab_1, tab_2, by = \"state\")\n\n        state population ev\n1     Alabama    4779736  9\n2      Alaska     710231  3\n3     Arizona    6392017 11\n4    Arkansas    2915918 NA\n5  California   37253956 55\n6    Colorado    5029196 NA\n7 Connecticut         NA  7\n8    Delaware         NA  3\n\n\n\n\n13.2.1.5 Semi join\nThe semi_join function lets us keep the part of first table for which we have information in the second. It does not add the columns of the second:\n\nsemi_join(tab_1, tab_2, by = \"state\")\n\n       state population\n1    Alabama    4779736\n2     Alaska     710231\n3    Arizona    6392017\n4 California   37253956\n\n\n\n\n13.2.1.6 Anti join\nThe function anti_join is the opposite of semi_join. It keeps the elements of the first table for which there is no information in the second:\n\nanti_join(tab_1, tab_2, by = \"state\")\n\n     state population\n1 Arkansas    2915918\n2 Colorado    5029196\n\n\n\n\n\n13.2.2 Set operators\nYou can use set operators on data frames:\n\n13.2.2.1 Intersect\nYou can take intersections of vectors of any type, such as numeric:\n\nintersect(1:10, 6:15)\n\n[1]  6  7  8  9 10\n\n\nor characters:\n\nintersect(c(\"a\",\"b\",\"c\"), c(\"b\",\"c\",\"d\"))\n\n[1] \"b\" \"c\"\n\n\n\ntab_1 &lt;- tab[1:5,]\ntab_2 &lt;- tab[3:7,]\ndplyr::intersect(tab_1, tab_2)\n\n       state abb region population total ev clinton trump\n1    Arizona  AZ   West    6392017   232 11    45.1  48.7\n2   Arkansas  AR  South    2915918    93  6    33.7  60.6\n3 California  CA   West   37253956  1257 55    61.7  31.6\n\n\n\n\n13.2.2.2 Union\nSimilarly union takes the union of vectors. For example:\n\nunion(1:10, 6:15)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\nunion(c(\"a\",\"b\",\"c\"), c(\"b\",\"c\",\"d\"))\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\n\nThe dplyr package includes a version of union that combines all the rows of two tables with the same column names.\n\ntab_1 &lt;- tab[1:5,]\ntab_2 &lt;- tab[3:7,]\ndplyr::union(tab_1, tab_2) \n\n        state abb    region population total ev clinton trump\n1     Alabama  AL     South    4779736   135  9    34.4  62.1\n2      Alaska  AK      West     710231    19  3    36.6  51.3\n3     Arizona  AZ      West    6392017   232 11    45.1  48.7\n4    Arkansas  AR     South    2915918    93  6    33.7  60.6\n5  California  CA      West   37253956  1257 55    61.7  31.6\n6    Colorado  CO      West    5029196    65  9    48.2  43.3\n7 Connecticut  CT Northeast    3574097    97  7    54.6  40.9\n\n\n\n\n13.2.2.3 setdiff\nThe set difference between a first and second argument can be obtained with setdiff. Unlike intersect and union, this function is not symmetric:\n\nsetdiff(1:10, 6:15)\n\n[1] 1 2 3 4 5\n\nsetdiff(6:15, 1:10)\n\n[1] 11 12 13 14 15\n\n\nAs with the functions shown above, dplyr has a version for data frames:\n\ntab_1 &lt;- tab[1:5,]\ntab_2 &lt;- tab[3:7,]\ndplyr::setdiff(tab_1, tab_2)\n\n    state abb region population total ev clinton trump\n1 Alabama  AL  South    4779736   135  9    34.4  62.1\n2  Alaska  AK   West     710231    19  3    36.6  51.3\n\n\n\n\n13.2.2.4 setequal\nFinally, the function setequal tells us if two sets are the same, regardless of order. So notice that:\n\nsetequal(1:5, 1:6)\n\n[1] FALSE\n\n\nbut:\n\nsetequal(1:5, 5:1)\n\n[1] TRUE\n\n\nWhen applied to data frames that are not equal, regardless of order, the dplyr version provides a useful message letting us know how the sets are different:\n\ndplyr::setequal(tab_1, tab_2)\n\n[1] FALSE"
  },
  {
    "objectID": "13-wrangling.html#string-processing",
    "href": "13-wrangling.html#string-processing",
    "title": "13  Wrangling",
    "section": "13.3 String processing",
    "text": "13.3 String processing\n\n13.3.1 The stringr package\n\nlibrary(tidyverse)\nlibrary(stringr)\n\n\n\n13.3.2 Case study: self-reported heights\nThe dslabs package includes the raw data from which the heights dataset was obtained. You can load it like this:\n\nlibrary(dslabs)\nhead(reported_heights)\n\n           time_stamp    sex height\n1 2014-09-02 13:40:36   Male     75\n2 2014-09-02 13:46:59   Male     70\n3 2014-09-02 13:59:20   Male     68\n4 2014-09-02 14:51:53   Male     74\n5 2014-09-02 15:16:15   Male     61\n6 2014-09-02 15:16:16 Female     65\n\n\n\nclass(reported_heights$height)\n\n[1] \"character\"\n\n\nIf we try to parse it into numbers, we get a warning:\n\nx &lt;- as.numeric(reported_heights$height)\n\nWarning: NAs introduced by coercion\n\n\nAlthough most values appear to be height in inches as requested:\n\nhead(x)\n\n[1] 75 70 68 74 61 65\n\n\nwe do end up with many NAs:\n\nsum(is.na(x))\n\n[1] 81\n\n\nWe can see some of the entries that are not successfully converted by using filter to keep only the entries resulting in NAs:\n\nreported_heights |&gt; \n  mutate(new_height = as.numeric(height)) |&gt;\n  filter(is.na(new_height)) |&gt; \n  head(n = 10)\n\n            time_stamp    sex                 height new_height\n1  2014-09-02 15:16:28   Male                  5' 4\"         NA\n2  2014-09-02 15:16:37 Female                  165cm         NA\n3  2014-09-02 15:16:52   Male                    5'7         NA\n4  2014-09-02 15:16:56   Male                  &gt;9000         NA\n5  2014-09-02 15:16:56   Male                   5'7\"         NA\n6  2014-09-02 15:17:09 Female                   5'3\"         NA\n7  2014-09-02 15:18:00   Male 5 feet and 8.11 inches         NA\n8  2014-09-02 15:19:48   Male                   5'11         NA\n9  2014-09-04 00:46:45   Male                  5'9''         NA\n10 2014-09-04 10:29:44   Male                 5'10''         NA\n\n\nWe permit a range that covers about 99.9999% of the adult population. We also use suppressWarnings to avoid the warning message we know as.numeric will gives us.\n\nnot_inches &lt;- function(x, smallest = 50, tallest = 84){\n  inches &lt;- suppressWarnings(as.numeric(x))\n  ind &lt;- is.na(inches) | inches &lt; smallest | inches &gt; tallest\n  ind\n}\n\nWe apply this function and find the number of problematic entries:\n\nproblems &lt;- reported_heights |&gt; \n  filter(not_inches(height)) |&gt;\n  pull(height)\nproblems\n\n  [1] \"6\"                      \"5' 4\\\"\"                 \"5.3\"                   \n  [4] \"165cm\"                  \"511\"                    \"6\"                     \n  [7] \"2\"                      \"5'7\"                    \"&gt;9000\"                 \n [10] \"5'7\\\"\"                  \"5'3\\\"\"                  \"5 feet and 8.11 inches\"\n [13] \"5.25\"                   \"5'11\"                   \"5.5\"                   \n [16] \"11111\"                  \"5'9''\"                  \"6\"                     \n [19] \"6.5\"                    \"150\"                    \"5'10''\"                \n [22] \"103.2\"                  \"5.8\"                    \"19\"                    \n [25] \"5\"                      \"5.6\"                    \"175\"                   \n [28] \"177\"                    \"300\"                    \"5,3\"                   \n [31] \"6'\"                     \"6\"                      \"5.9\"                   \n [34] \"6,8\"                    \"5' 10\"                  \"5.5\"                   \n [37] \"178\"                    \"163\"                    \"6.2\"                   \n [40] \"175\"                    \"Five foot eight inches\" \"6.2\"                   \n [43] \"5.8\"                    \"5.1\"                    \"178\"                   \n [46] \"165\"                    \"5.11\"                   \"5'5\\\"\"                 \n [49] \"165\"                    \"180\"                    \"5'2\\\"\"                 \n [52] \"5.75\"                   \"169\"                    \"5,4\"                   \n [55] \"7\"                      \"5.4\"                    \"157\"                   \n [58] \"6.1\"                    \"169\"                    \"5'3\"                   \n [61] \"5.6\"                    \"214\"                    \"183\"                   \n [64] \"5.6\"                    \"6\"                      \"162\"                   \n [67] \"178\"                    \"180\"                    \"5'10''\"                \n [70] \"170\"                    \"5'3''\"                  \"178\"                   \n [73] \"0.7\"                    \"190\"                    \"5.4\"                   \n [76] \"184\"                    \"5'7''\"                  \"5.9\"                   \n [79] \"5'12\"                   \"5.6\"                    \"5.6\"                   \n [82] \"184\"                    \"6\"                      \"167\"                   \n [85] \"2'33\"                   \"5'11\"                   \"5'3\\\"\"                 \n [88] \"5.5\"                    \"5.2\"                    \"180\"                   \n [91] \"5.5\"                    \"5.5\"                    \"6.5\"                   \n [94] \"5,8\"                    \"180\"                    \"183\"                   \n [97] \"170\"                    \"5'6''\"                  \"172\"                   \n[100] \"612\"                    \"5.11\"                   \"168\"                   \n[103] \"5'4\"                    \"1,70\"                   \"172\"                   \n[106] \"87\"                     \"5.5\"                    \"176\"                   \n[109] \"5'7.5''\"                \"5'7.5''\"                \"111\"                   \n[112] \"5'2\\\"\"                  \"173\"                    \"174\"                   \n[115] \"176\"                    \"175\"                    \"5' 7.78\\\"\"             \n[118] \"6.7\"                    \"12\"                     \"6\"                     \n[121] \"5.1\"                    \"5.6\"                    \"5.5\"                   \n[124] \"yyy\"                    \"5.2\"                    \"5'5\"                   \n[127] \"5'8\"                    \"5'6\"                    \"5 feet 7inches\"        \n[130] \"89\"                     \"5.6\"                    \"5.7\"                   \n[133] \"183\"                    \"172\"                    \"34\"                    \n[136] \"25\"                     \"6\"                      \"5.9\"                   \n[139] \"168\"                    \"6.5\"                    \"170\"                   \n[142] \"175\"                    \"6\"                      \"22\"                    \n[145] \"5.11\"                   \"684\"                    \"6\"                     \n[148] \"1\"                      \"1\"                      \"6*12\"                  \n[151] \"5 .11\"                  \"87\"                     \"162\"                   \n[154] \"165\"                    \"184\"                    \"6\"                     \n[157] \"173\"                    \"1.6\"                    \"172\"                   \n[160] \"170\"                    \"5.7\"                    \"5.5\"                   \n[163] \"174\"                    \"170\"                    \"160\"                   \n[166] \"120\"                    \"120\"                    \"23\"                    \n[169] \"192\"                    \"5 11\"                   \"167\"                   \n[172] \"150\"                    \"1.7\"                    \"174\"                   \n[175] \"5.8\"                    \"6\"                      \"5'4\"                   \n[178] \"5'8\\\"\"                  \"5'5\"                    \"5.8\"                   \n[181] \"5.1\"                    \"5.11\"                   \"5.7\"                   \n[184] \"5'7\"                    \"5'6\"                    \"5'11\\\"\"                \n[187] \"5'7\\\"\"                  \"5'7\"                    \"172\"                   \n[190] \"5'8\"                    \"180\"                    \"5' 11\\\"\"               \n[193] \"5\"                      \"180\"                    \"180\"                   \n[196] \"6'1\\\"\"                  \"5.9\"                    \"5.2\"                   \n[199] \"5.5\"                    \"69\\\"\"                   \"5' 7\\\"\"                \n[202] \"5'10''\"                 \"5.51\"                   \"5'10\"                  \n[205] \"5'10\"                   \"5ft 9 inches\"           \"5 ft 9 inches\"         \n[208] \"5'2\"                    \"5'11\"                   \"5.8\"                   \n[211] \"5.7\"                    \"167\"                    \"168\"                   \n[214] \"6\"                      \"6.1\"                    \"5'11''\"                \n[217] \"5.69\"                   \"178\"                    \"182\"                   \n[220] \"164\"                    \"5'8\\\"\"                  \"185\"                   \n[223] \"6\"                      \"86\"                     \"5.7\"                   \n[226] \"708,661\"                \"5.25\"                   \"5.5\"                   \n[229] \"5 feet 6 inches\"        \"5'10''\"                 \"172\"                   \n[232] \"6\"                      \"5'8\"                    \"160\"                   \n[235] \"6'3\\\"\"                  \"649,606\"                \"10000\"                 \n[238] \"5.1\"                    \"152\"                    \"1\"                     \n[241] \"180\"                    \"728,346\"                \"175\"                   \n[244] \"158\"                    \"173\"                    \"164\"                   \n[247] \"6 04\"                   \"169\"                    \"0\"                     \n[250] \"185\"                    \"168\"                    \"5'9\"                   \n[253] \"169\"                    \"5'5''\"                  \"174\"                   \n[256] \"6.3\"                    \"179\"                    \"5'7\\\"\"                 \n[259] \"5.5\"                    \"6\"                      \"6\"                     \n[262] \"170\"                    \"6\"                      \"172\"                   \n[265] \"158\"                    \"100\"                    \"159\"                   \n[268] \"190\"                    \"5.7\"                    \"170\"                   \n[271] \"158\"                    \"6'4\\\"\"                  \"180\"                   \n[274] \"5.57\"                   \"5'4\"                    \"210\"                   \n[277] \"88\"                     \"6\"                      \"162\"                   \n[280] \"170 cm\"                 \"5.7\"                    \"170\"                   \n[283] \"157\"                    \"186\"                    \"170\"                   \n[286] \"7,283,465\"              \"5\"                      \"5\"                     \n[289] \"34\"                     \"161\"                    \"5'6\"                   \n[292] \"5'6\"                   \n\n\n\n\n13.3.3 Regular expressions\n\n13.3.3.1 Special characters\nNow let’s consider a slightly more complicated example. Which of the following strings contain the pattern cm or inches?\n\nyes &lt;- c(\"180 cm\", \"70 inches\")\nno &lt;- c(\"180\", \"70''\")\ns &lt;- c(yes, no)\n\n\nstr_detect(s, \"cm\") | str_detect(s, \"inches\")\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\nHowever, we don’t need to do this. The main feature that distinguishes the regex language from plain strings is that we can use special characters. These are characters with a meaning. We start by introducing | which means or. So if we want to know if either cm or inches appears in the strings, we can use the regex cm|inches:\n\nstr_detect(s, \"cm|inches\")\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\nand obtain the correct answer.\nAnother special character that will be useful for identifying feet and inches values is \\d which means any digit: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. The backslash is used to distinguish it from the character d. In R, we have to escape the backslash \\ so we actually have to use \\\\d to represent digits. Here is an example:\n\nyes &lt;- c(\"5\", \"6\", \"5'10\", \"5 feet\", \"4'11\")\nno &lt;- c(\"\", \".\", \"Five\", \"six\")\ns &lt;- c(yes, no)\npattern &lt;- \"\\\\d\"\nstr_detect(s, pattern)\n\n[1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\n\nWe take this opportunity to introduce the str_view_all function, which is helpful for troubleshooting as it shows us the first match for each string:\n\nstr_view_all(s, pattern)\n\nWarning: `str_view()` was deprecated in stringr 1.5.0.\nℹ Please use `str_view_all()` instead.\n\n\n[1] │ &lt;5&gt;\n[2] │ &lt;6&gt;\n[3] │ &lt;5&gt;'&lt;1&gt;&lt;0&gt;\n[4] │ &lt;5&gt; feet\n[5] │ &lt;4&gt;'&lt;1&gt;&lt;1&gt;\n[6] │ \n[7] │ .\n[8] │ Five\n[9] │ six\n\n\nand str_view_all shows us all the matches, so 3'2 has two matches and 5'10 has three.\n\nstr_view_all(s, pattern)\n\n[1] │ &lt;5&gt;\n[2] │ &lt;6&gt;\n[3] │ &lt;5&gt;'&lt;1&gt;&lt;0&gt;\n[4] │ &lt;5&gt; feet\n[5] │ &lt;4&gt;'&lt;1&gt;&lt;1&gt;\n[6] │ \n[7] │ .\n[8] │ Five\n[9] │ six\n\n\nThere are many other special characters. We will learn some others below, but you can see most or all of them in the cheat sheet1 mentioned earlier.\nFinally, a useful special character is \\w which stands for word character and it matches any letter, number, or underscore. It is equivalent to [a-zA-Z0-9_].\n\n\n13.3.3.2 Character classes\nCharacter classes are used to define a series of characters that can be matched. We define character classes with square brackets []. So, for example, if we want the pattern to match only if we have a 5 or a 6, we use the regex [56]:\n\nstr_view_all(s, \"[56]\")\n\n[1] │ &lt;5&gt;\n[2] │ &lt;6&gt;\n[3] │ &lt;5&gt;'10\n[4] │ &lt;5&gt; feet\n[5] │ 4'11\n[6] │ \n[7] │ .\n[8] │ Five\n[9] │ six\n\n\nSuppose we want to match values between 4 and 7. A common way to define character classes is with ranges. So, for example, [0-9] is equivalent to \\\\d. The pattern we want is therefore [4-7].\n\nyes &lt;- as.character(4:7)\nno &lt;- as.character(1:3)\ns &lt;- c(yes, no)\nstr_detect(s, \"[4-7]\")\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n\nHowever, it is important to know that in regex everything is a character; there are no numbers. So 4 is the character 4 not the number four. Notice, for example, that [1-20] does not mean 1 through 20, it means the characters 1 through 2 or the character 0. So [1-20] simply means the character class composed of 0, 1, and 2.\nKeep in mind that characters do have an order and the digits do follow the numeric order. So 0 comes before 1 which comes before 2 and so on. For the same reason, we can define lower case letters as [a-z], upper case letters as [A-Z], and [a-zA-z] as both.\n\n\n13.3.3.3 Anchors\nWhat if we want a match when we have exactly 1 digit? This will be useful in our case study since feet are never more than 1 digit so a restriction will help us. One way to do this with regex is by using anchors, which let us define patterns that must start or end at a specific place. The two most common anchors are ^ and $ which represent the beginning and end of a string, respectively. So the pattern ^\\\\d$ is read as “start of the string followed by one digit followed by end of string”.\nThis pattern now only detects the strings with exactly one digit:\n\npattern &lt;- \"^\\\\d$\"\nyes &lt;- c(\"1\", \"5\", \"9\")\nno &lt;- c(\"12\", \"123\", \" 1\", \"a4\", \"b\")\ns &lt;- c(yes, no)\nstr_view_all(s, pattern)\n\n[1] │ &lt;1&gt;\n[2] │ &lt;5&gt;\n[3] │ &lt;9&gt;\n[4] │ 12\n[5] │ 123\n[6] │  1\n[7] │ a4\n[8] │ b\n\n\nThe 1 does not match because it does not start with the digit but rather with a space, which is actually not easy to see.\n\n\n13.3.3.4 Quantifiers\nFor the inches part, we can have one or two digits. This can be specified in regex with quantifiers. This is done by following the pattern with curly brackets containing the number of times the previous entry can be repeated. We use an example to illustrate. The pattern for one or two digits is:\n\npattern &lt;- \"^\\\\d{1,2}$\"\nyes &lt;- c(\"1\", \"5\", \"9\", \"12\")\nno &lt;- c(\"123\", \"a4\", \"b\")\nstr_view_all(c(yes, no), pattern)\n\n[1] │ &lt;1&gt;\n[2] │ &lt;5&gt;\n[3] │ &lt;9&gt;\n[4] │ &lt;12&gt;\n[5] │ 123\n[6] │ a4\n[7] │ b\n\n\n\n\n13.3.3.5 White space \\s\nAnother problem we have are spaces. For example, our pattern does not match 5' 4\" because there is a space between ' and 4 which our pattern does not permit. Spaces are characters and R does not ignore them:\n\nidentical(\"Hi\", \"Hi \")\n\n[1] FALSE\n\n\nIn regex, \\s represents white space. To find patterns like 5' 4, we can change our pattern to:\n\npattern_2 &lt;- \"^[4-7]'\\\\s\\\\d{1,2}\\\"$\"\nstr_subset(problems, pattern_2)\n\n[1] \"5' 4\\\"\"  \"5' 11\\\"\" \"5' 7\\\"\" \n\n\nHowever, this will not match the patterns with no space. So do we need more than one regex pattern? It turns out we can use a quantifier for this as well.\n\n\n13.3.3.6 Quantifiers: *, ?, +\nWe want the pattern to permit spaces but not require them. Even if there are several spaces, like in this example 5'   4, we still want it to match. There is a quantifier for exactly this purpose. In regex, the character * means zero or more instances of the previous character. Here is an example:\n\nyes &lt;- c(\"AB\", \"A1B\", \"A11B\", \"A111B\", \"A1111B\")\nno &lt;- c(\"A2B\", \"A21B\")\nstr_detect(yes, \"A1*B\")\n\n[1] TRUE TRUE TRUE TRUE TRUE\n\nstr_detect(no, \"A1*B\")\n\n[1] FALSE FALSE\n\n\nThe above matches the first string which has zero 1s and all the strings with one or more 1. We can then improve our pattern by adding the * after the space character \\s.\nThere are two other similar quantifiers. For none or once, we can use ?, and for one or more, we can use +. You can see how they differ with this example:\n\ndata.frame(string = c(\"AB\", \"A1B\", \"A11B\", \"A111B\", \"A1111B\"),\n           none_or_more = str_detect(yes, \"A1*B\"),\n           nore_or_once = str_detect(yes, \"A1?B\"),\n           once_or_more = str_detect(yes, \"A1+B\"))\n\n  string none_or_more nore_or_once once_or_more\n1     AB         TRUE         TRUE        FALSE\n2    A1B         TRUE         TRUE         TRUE\n3   A11B         TRUE        FALSE         TRUE\n4  A111B         TRUE        FALSE         TRUE\n5 A1111B         TRUE        FALSE         TRUE\n\n\nWe will actually use all three in our reported heights example, but we will see these in a later section.\n\n\n13.3.3.7 Not\nTo specify patterns that we do not want to detect, we can use the ^ symbol but only inside square brackets. Remember that outside the square bracket ^ means the start of the string. So, for example, if we want to detect digits that are preceded by anything except a letter we can do the following:\n\npattern &lt;- \"[^a-zA-Z]\\\\d\"\nyes &lt;- c(\".3\", \"+2\", \"-0\",\"*4\")\nno &lt;- c(\"A3\", \"B2\", \"C0\", \"E4\")\nstr_detect(yes, pattern)\n\n[1] TRUE TRUE TRUE TRUE\n\nstr_detect(no, pattern)\n\n[1] FALSE FALSE FALSE FALSE\n\n\nAnother way to generate a pattern that searches for everything except is to use the upper case of the special character. For example \\\\D means anything other than a digit, \\\\S means anything except a space, and so on.\n\n\n13.3.3.8 Lookarounds\nLookarounds provide a way to ask for one or more conditions to be satisfied without moving the search forward or matching it. For example, you might want to check for multiple conditions and if they are matched, then return the pattern or part of the pattern that matched. An example: check if a string satisfies the conditions for a password and if it does return the password. Suppose the conditions are 1) 8-16 word characters, 2) starts with a letter, and 3) has at least one digit.\nThere are four types of lookarounds: lookahead (?=pattern), lookbehind (?&lt;=pattern), negative lookahead (?!pattern), and negative lookbehind (?&lt;!pattern). You can concatenate them to check for multiple conditions so for our example we can write it like this:\n\npattern &lt;- \"(?=\\\\w{8,16})(?=^[a-z|A-Z].*)(?=.*\\\\d+.*).*\"\nyes &lt;- c(\"Ihatepasswords1\", \"password1234\")\nno &lt;- c(\"sh0rt\", \"Ihaterpasswords\", \"7X%9,N`yrYG92b7\")\nstr_detect(yes, pattern)\n\n[1] TRUE TRUE\n\nstr_detect(no, pattern)\n\n[1] FALSE FALSE FALSE\n\nstr_extract(yes, pattern)\n\n[1] \"Ihatepasswords1\" \"password1234\"   \n\n\n\n\n13.3.3.9 Groups\nGroups are a powerful aspect of regex that permits the extraction of values. Groups are defined using parentheses. They don’t affect the pattern matching per se. Instead, it permits tools to identify specific parts of the pattern so we can extract them.\nWe want to change heights written like 5.6 to 5'6.\nTo avoid changing patterns such as 70.2, we will require that the first digit be between 4 and 7 [4-7] and that the second be none or more digits \\\\d*. Let’s start by defining a simple pattern that matches this:\n\npattern_without_groups &lt;- \"^[4-7],\\\\d*$\"\n\nWe want to extract the digits so we can then form the new version using a period. These are our two groups, so we encapsulate them with parentheses:\n\npattern_with_groups &lt;-  \"^([4-7]),(\\\\d*)$\"\n\nWe encapsulate the part of the pattern that matches the parts we want to keep for later use. Adding groups does not affect the detection, since it only signals that we want to save what is captured by the groups. Note that both patterns return the same result when using str_detect:\n\nyes &lt;- c(\"5,9\", \"5,11\", \"6,\", \"6,1\")\nno &lt;- c(\"5'9\", \",\", \"2,8\", \"6.1.1\")\ns &lt;- c(yes, no)\nstr_detect(s, pattern_without_groups)\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nstr_detect(s, pattern_with_groups)\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\n\nOnce we define groups, we can use the function str_match to extract the values these groups define:\n\nstr_match(s, pattern_with_groups)\n\n     [,1]   [,2] [,3]\n[1,] \"5,9\"  \"5\"  \"9\" \n[2,] \"5,11\" \"5\"  \"11\"\n[3,] \"6,\"   \"6\"  \"\"  \n[4,] \"6,1\"  \"6\"  \"1\" \n[5,] NA     NA   NA  \n[6,] NA     NA   NA  \n[7,] NA     NA   NA  \n[8,] NA     NA   NA  \n\n\nNotice that the second and third columns contain feet and inches, respectively. The first column is the part of the string matching the pattern. If no match occurred, we see an NA.\nNow we can understand the difference between the functions str_extract and str_match: str_extract extracts only strings that match a pattern, not the values defined by groups:\n\nstr_extract(s, pattern_with_groups)\n\n[1] \"5,9\"  \"5,11\" \"6,\"   \"6,1\"  NA     NA     NA     NA    \n\n\n\n\n\n13.3.4 Search and replace with regex\nEarlier we defined the object problems containing the strings that do not appear to be in inches. We can see that not too many of our problematic strings match the pattern:\n\npattern &lt;- \"^[4-7]'\\\\d{1,2}\\\"$\"\nsum(str_detect(problems, pattern))\n\n[1] 14\n\n\nTo see why this is, we show some examples that expose why we don’t have more matches:\n\nproblems[  c(2, 10, 11, 12, 15)] |&gt; str_view_all(pattern)\n\n[1] │ 5' 4\"\n[2] │ &lt;5'7\"&gt;\n[3] │ &lt;5'3\"&gt;\n[4] │ 5 feet and 8.11 inches\n[5] │ 5.5\n\n\nAn initial problem we see immediately is that some students wrote out the words “feet” and “inches”. We can see the entries that did this with the str_subset function:\n\nstr_subset(problems, \"inches\")\n\n[1] \"5 feet and 8.11 inches\" \"Five foot eight inches\" \"5 feet 7inches\"        \n[4] \"5ft 9 inches\"           \"5 ft 9 inches\"          \"5 feet 6 inches\"       \n\n\nWe also see that some entries used two single quotes '' instead of a double quote \".\n\nstr_subset(problems, \"''\")\n\n [1] \"5'9''\"   \"5'10''\"  \"5'10''\"  \"5'3''\"   \"5'7''\"   \"5'6''\"   \"5'7.5''\"\n [8] \"5'7.5''\" \"5'10''\"  \"5'11''\"  \"5'10''\"  \"5'5''\"  \n\n\nTo correct this, we can replace the different ways of representing inches and feet with a uniform symbol. We will use ' for feet, whereas for inches we will simply not use a symbol since some entries were of the form x'y. Now, if we no longer use the inches symbol, we have to change our pattern accordingly:\n\npattern &lt;- \"^[4-7]'\\\\d{1,2}$\"\n\nIf we do this replacement before the matching, we get many more matches:\n\nproblems |&gt; \n  str_replace(\"feet|ft|foot\", \"'\") |&gt; # replace feet, ft, foot with ' \n  str_replace(\"inches|in|''|\\\"\", \"\") |&gt; # remove all inches symbols\n  str_detect(pattern) |&gt; \n  sum()\n\n[1] 48\n\n\nHowever, we still have many cases to go.\nNote that in the code above, we leveraged the stringr consistency and used the pipe.\nFor now, we improve our pattern by adding \\\\s* in front of and after the feet symbol ' to permit space between the feet symbol and the numbers. Now we match a few more entries:\n\npattern &lt;- \"^[4-7]\\\\s*'\\\\s*\\\\d{1,2}$\"\nproblems |&gt; \n  str_replace(\"feet|ft|foot\", \"'\") |&gt; # replace feet, ft, foot with ' \n  str_replace(\"inches|in|''|\\\"\", \"\") |&gt; # remove all inches symbols\n  str_detect(pattern) |&gt; \n  sum()\n\n[1] 53\n\n\n\n13.3.4.1 Search and replace using groups\nAnother powerful aspect of groups is that you can refer to the extracted values in a regex when searching and replacing.\nThe regex special character for the i-th group is \\\\i. So \\\\1 is the value extracted from the first group, \\\\2 the value from the second and so on. As a simple example, note that the following code will replace a comma with period, but only if it is between two digits:\n\npattern_with_groups &lt;-  \"^([4-7]),(\\\\d*)$\"\nyes &lt;- c(\"5,9\", \"5,11\", \"6,\", \"6,1\")\nno &lt;- c(\"5'9\", \",\", \"2,8\", \"6.1.1\")\ns &lt;- c(yes, no)\nstr_replace(s, pattern_with_groups, \"\\\\1'\\\\2\")\n\n[1] \"5'9\"   \"5'11\"  \"6'\"    \"6'1\"   \"5'9\"   \",\"     \"2,8\"   \"6.1.1\"\n\n\n\n\n\n13.3.5 Trimming\nIn general, spaces at the start or end of the string are uninformative. These can be particularly deceptive because sometimes they can be hard to see:\n\ns &lt;- \"Hi \"\ncat(s)\n\nHi \n\nidentical(s, \"Hi\")\n\n[1] FALSE\n\n\nThis is a general enough problem that there is a function dedicated to removing them: str_trim.\n\nstr_trim(\" 5 ' 9 \")\n\n[1] \"5 ' 9\"\n\n\n\n\n13.3.6 Changing lettercase\n\ns &lt;- c(\"Five feet eight inches\")\nstr_to_lower(s)\n\n[1] \"five feet eight inches\"\n\n\nOther related functions are str_to_upper and str_to_title. We are now ready to define a procedure that converts all the problematic cases to inches.\n\n\n13.3.7 The extract function\nThe extract function is a useful tidyverse function for string processing that we will use in our final solution, so we introduce it here. In a previous section, we constructed a regex that lets us identify which elements of a character vector match the feet and inches pattern. However, we want to do more. We want to extract and save the feet and number values so that we can convert them to inches when appropriate.\nIf we have a simpler case like this:\n\ns &lt;- c(\"5'10\", \"6'1\")\ntab &lt;- data.frame(x = s)\n\nIn Section Section 13.1.3 we learned about the separate function, which can be used to achieve our current goal:\n\ntab |&gt; separate(x, c(\"feet\", \"inches\"), sep = \"'\")\n\n  feet inches\n1    5     10\n2    6      1\n\n\nThe extract function from the tidyr package lets us use regex groups to extract the desired values. Here is the equivalent to the code above using separate but using extract:\n\nlibrary(tidyr)\ntab |&gt; extract(x, c(\"feet\", \"inches\"), regex = \"(\\\\d)'(\\\\d{1,2})\")\n\n  feet inches\n1    5     10\n2    6      1\n\n\nSo why do we even need the new function extract? We have seen how small changes can throw off exact pattern matching. Groups in regex give us more flexibility. For example, if we define:\n\ns &lt;- c(\"5'10\", \"6'1\\\"\",\"5'8inches\")\ntab &lt;- data.frame(x = s)\n\nand we only want the numbers, separate fails:\n\ntab |&gt; separate(x, c(\"feet\",\"inches\"), sep = \"'\", fill = \"right\")\n\n  feet  inches\n1    5      10\n2    6      1\"\n3    5 8inches\n\n\nHowever, we can use extract. The regex here is a bit more complicated since we have to permit ' with spaces and feet. We also do not want the \" included in the value, so we do not include that in the group:\n\ntab |&gt; extract(x, c(\"feet\", \"inches\"), regex = \"(\\\\d)'(\\\\d{1,2})\")\n\n  feet inches\n1    5     10\n2    6      1\n3    5      8\n\n\n\n\n13.3.8 Putting it all together\nWe are now ready to put it all together and wrangle our reported heights data to try to recover as many heights as possible. The code is complex, but we will break it down into parts.\nWe start by cleaning up the height column so that the heights are closer to a feet’inches format. We added an original heights column so we can compare before and after.\nWe now put all of what we have learned together into a function that takes a string vector and tries to convert as many strings as possible to one format. We write a function that puts together what we have done above.\n\nconvert_format &lt;- function(s){\n  s |&gt;\n    str_replace(\"feet|foot|ft\", \"'\") |&gt; \n    str_replace_all(\"inches|in|''|\\\"|cm|and\", \"\") |&gt;  \n    str_replace(\"^([4-7])\\\\s*[,\\\\.\\\\s+]\\\\s*(\\\\d*)$\", \"\\\\1'\\\\2\") |&gt; \n    str_replace(\"^([56])'?$\", \"\\\\1'0\") |&gt; \n    str_replace(\"^([12])\\\\s*,\\\\s*(\\\\d*)$\", \"\\\\1\\\\.\\\\2\") |&gt;  \n    str_trim() \n}\nlibrary(english)\nwords_to_numbers &lt;- function(s){\n  s &lt;- str_to_lower(s)\n  for (i in 0:11)\n    s &lt;- str_replace_all(s, words(i), as.character(i))\n  s\n}\n\n\npattern &lt;- \"^([4-7])\\\\s*'\\\\s*(\\\\d+\\\\.?\\\\d*)$\"\n\nsmallest &lt;- 50\ntallest &lt;- 84\nnew_heights &lt;- reported_heights |&gt; \n  mutate(original = height, \n         height = words_to_numbers(height) |&gt; convert_format()) |&gt;\n  extract(height, c(\"feet\", \"inches\"), regex = pattern, remove = FALSE) |&gt; \n  mutate_at(c(\"height\", \"feet\", \"inches\"), as.numeric) |&gt;\n  mutate(guess = 12 * feet + inches) |&gt;\n  mutate(height = case_when(\n    is.na(height) ~ as.numeric(NA),\n    between(height, smallest, tallest) ~ height,  #inches\n    between(height/2.54, smallest, tallest) ~ height/2.54, #cm\n    between(height*100/2.54, smallest, tallest) ~ height*100/2.54, #meters\n    TRUE ~ as.numeric(NA))) |&gt;\n  mutate(height = ifelse(is.na(height) & \n                           inches &lt; 12 & between(guess, smallest, tallest),\n                         guess, height)) |&gt;\n  select(-guess)\n\nWe can check all the entries we converted by typing:\n\nnew_heights |&gt;\n  filter(not_inches(original)) |&gt;\n  select(original, height) |&gt; \n  arrange(height) |&gt;\n  View()\n\nA final observation is that if we look at the shortest students in our course:\n\nnew_heights |&gt; arrange(height) |&gt; head(n = 7)\n\n           time_stamp    sex height feet inches original\n1 2017-07-04 01:30:25   Male  50.00   NA     NA       50\n2 2017-09-07 10:40:35   Male  50.00   NA     NA       50\n3 2014-09-02 15:18:30 Female  51.00   NA     NA       51\n4 2016-06-05 14:07:20 Female  52.00   NA     NA       52\n5 2016-06-05 14:07:38 Female  52.00   NA     NA       52\n6 2014-09-23 03:39:56 Female  53.00   NA     NA       53\n7 2015-01-07 08:57:29   Male  53.77   NA     NA    53.77\n\n\nWe see heights of 53, 54, and 55. In the originals, we also have 51 and 52. These short heights are rare and it is likely that the students actually meant 5'1, 5'2, 5'3, 5'4, and 5'5. Because we are not completely sure, we will leave them as reported. The object new_heights contains our final solution for this case study.\n\n\n13.3.9 String splitting\nAnother very common data wrangling operation is string splitting. To illustrate how this comes up, we start with an illustrative example. Suppose we did not have the function read_csv or read.csv available to us. We instead have to read a csv file using the base R function readLines like this:\n\nfilename &lt;- system.file(\"extdata/murders.csv\", package = \"dslabs\")\nlines &lt;- readLines(filename)\n\nThis function reads-in the data line-by-line to create a vector of strings. In this case, one string for each row in the spreadsheet. The first six lines are:\n\nlines |&gt; head()\n\n[1] \"state,abb,region,population,total\" \"Alabama,AL,South,4779736,135\"     \n[3] \"Alaska,AK,West,710231,19\"          \"Arizona,AZ,West,6392017,232\"      \n[5] \"Arkansas,AR,South,2915918,93\"      \"California,CA,West,37253956,1257\" \n\n\nWe want to extract the values that are separated by a comma for each string in the vector. The command str_split does exactly this:\n\nx &lt;- str_split(lines, \",\") \nx |&gt; head(2)\n\n[[1]]\n[1] \"state\"      \"abb\"        \"region\"     \"population\" \"total\"     \n\n[[2]]\n[1] \"Alabama\" \"AL\"      \"South\"   \"4779736\" \"135\"    \n\nx &lt;- str_split_fixed(lines, \",\", 5)\n\n\n\n13.3.10 Recoding\nAnother common operation involving strings is recoding the names of categorical variables. Let’s say you have really long names for your levels and you will be displaying them in plots, you might want to use shorter versions of these names. For example, in character vectors with country names, you might want to change “United States of America” to “USA” and “United Kingdom” to UK, and so on. We can do this with case_when, although the tidyverse offers an option that is specifically designed for this task: the case_match function.\nHere is an example that shows how to rename countries with long names:\n\nlibrary(dslabs)\n\nSuppose we want to show life expectancy time series by country for the Caribbean:\n\ngapminder |&gt; \n  filter(region == \"Caribbean\") |&gt;\n  ggplot(aes(year, life_expectancy, color = country)) +\n  geom_line()\n\n\n\n\n\ngapminder |&gt; filter(region == \"Caribbean\") |&gt;\n  mutate(country = case_match(country, \n                          \"Antigua and Barbuda\" ~ \"Barbuda\",\n                          \"Dominican Republic\" ~ \"DR\",\n                          \"St. Vincent and the Grenadines\" ~ \"St. Vincent\",\n                          \"Trinidad and Tobago\" ~ \"Trinidad\",\n                          .default = country)) |&gt;\n  ggplot(aes(year, life_expectancy, color = country)) +\n  geom_line()\n\n\n\n\nThere are other similar functions in other R packages, such as recode_factor and fct_recoder of fct_collapse in the forcats package."
  },
  {
    "objectID": "13-wrangling.html#exercises",
    "href": "13-wrangling.html#exercises",
    "title": "13  Wrangling",
    "section": "13.4 Exercises",
    "text": "13.4 Exercises\n\nRun the following command to define the co2_wide object:\n\n\nco2_wide &lt;- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) |&gt; \n  setNames(1:12) |&gt;\n  mutate(year = as.character(1959:1997))\n\nUse the pivot_longer function to wrangle this into a tidy dataset. Call the column with the CO2 measurements co2 and call the month column month. Call the resulting object co2_tidy.\n\nPlot CO2 versus month with a different curve for each year using this code:\n\n\nco2_tidy |&gt; ggplot(aes(month, co2, color = year)) + geom_line()\n\nIf the expected plot is not made, it is probably because co2_tidy$month is not numeric:\n\nclass(co2_tidy$month)\n\nRewrite your code to make sure the month column is numeric. Then make the plot.\n\nWhat do we learn from this plot?\n\n\nCO2 measures increase monotonically from 1959 to 1997.\nCO2 measures are higher in the summer and the yearly average increased from 1959 to 1997.\nCO2 measures appear constant and random variability explains the differences.\nCO2 measures do not have a seasonal trend.\n\n\nNow load the admissions data set, which contains admission information for men and women across six majors and keep only the admitted percentage column:\n\n\nload(admissions)\ndat &lt;- admissions |&gt; select(-applicants)\n\nIf we think of an observation as a major, and that each observation has two variables (men admitted percentage and women admitted percentage) then this is not tidy. Use the pivot_wider function to wrangle into tidy shape: one row for each major.\n\nNow we will try a more advanced wrangling challenge. We want to wrangle the admissions data so that for each major we have 4 observations: admitted_men, admitted_women, applicants_men and applicants_women. The trick we perform here is actually quite common: first use pivot_longer to generate an intermediate data frame and then pivot_wider to obtain the tidy data we want. We will go step by step in this and the next two exercises.\n\n\nadmissions |&gt; pivot_longer(c(admitted, applicants)) |&gt;\n  unite(names, name, gender) |&gt;\n  pivot_wider(names_from = names, values_from = value)\n\n# A tibble: 6 × 5\n  major admitted_men applicants_men admitted_women applicants_women\n  &lt;chr&gt;        &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1 A               62            825             82              108\n2 B               63            560             68               25\n3 C               37            325             34              593\n4 D               33            417             35              375\n5 E               28            191             24              393\n6 F                6            373              7              341\n\n\nUse the pivot_longer function to create a tmp data.frame with a column containing the type of observation admitted or applicants. Call the new columns name and value.\n\nNow you have an object tmp with columns major, gender, name and value. Note that if you combine the name and gender, we get the column names we want: admitted_men, admitted_women, applicants_men and applicants_women. Use the function unite to create a new column called column_name.\nNow use the pivot_wider function to generate the tidy data with four variables for each major.\nNow use the pipe to write a line of code that turns admissions to the table produced in the previous exercise.\nInstall and load the Lahman library. This database includes data related to baseball teams. It includes summary statistics about how the players performed on offense and defense for several years. It also includes personal information about the players.\n\nThe Batting data frame contains the offensive statistics for all players for many years. You can see, for example, the top 10 hitters by running this code:\n\nlibrary(Lahman)\n\ntop &lt;- Batting |&gt; \n  filter(yearID == 2016) |&gt;\n  arrange(desc(HR)) |&gt;\n  slice(1:10)\n\ntop |&gt; as_tibble()\n\nBut who are these players? We see an ID, but not the names. The player names are in this table\n\nPeople |&gt; as_tibble()\n\nWe can see column names nameFirst and nameLast. Use the left_join function to create a table of the top home run hitters. The table should have playerID, first name, last name, and number of home runs (HR). Rewrite the object top with this new table.\n\nNow use the Salaries data frame to add each player’s salary to the t le you created in exercise 1. Note that salaries are different every year so make sure to filter for the year 2016, then use right_join. This time show first name, last name, team, HR, and salary.\nIn a previous exercise, we created a tidy version of the co2 dataset:\n\n\nco2_wide &lt;- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) |&gt; \n  setNames(1:12) |&gt;\n  mutate(year = 1959:1997) |&gt;\n  pivot_longer(-year, names_to = \"month\", values_to = \"co2\") |&gt;\n  mutate(month = as.numeric(month))\n\nWe want to see if the monthly trend is changing so we are going to remove the year effects and then plot the results. We will first compute the year averages. Use the group_by and summarize to compute the average co2 for each year. Save in an object called yearly_avg.\n\nNow use the left_join function to add the yearly average to the co2_wide dataset. Then compute the residuals: observed co2 measure - yearly average.\nMake a plot of the seasonal trends by year but only after removing the year effect."
  },
  {
    "objectID": "13-wrangling.html#footnotes",
    "href": "13-wrangling.html#footnotes",
    "title": "13  Wrangling",
    "section": "",
    "text": "https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf↩︎"
  },
  {
    "objectID": "14-web-scraping.html#html",
    "href": "14-web-scraping.html#html",
    "title": "14  Web scraping",
    "section": "14.1 HTML",
    "text": "14.1 HTML\nHere is the key piece of code:\n&lt;table class=\"wikitable sortable\"&gt;\n&lt;tr&gt;\n&lt;th&gt;State&lt;/th&gt;\n&lt;th&gt;&lt;a href=\"/wiki/List_of_U.S._states_and_territories_by_population\" \ntitle=\"List of U.S. states and territories by population\"&gt;Population&lt;/a&gt;&lt;br /&gt;\n&lt;small&gt;(total inhabitants)&lt;/small&gt;&lt;br /&gt;\n&lt;small&gt;(2015)&lt;/small&gt; &lt;sup id=\"cite_ref-1\" class=\"reference\"&gt;\n&lt;a href=\"#cite_note-1\"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/th&gt;\n&lt;th&gt;Murders and Nonnegligent\n&lt;p&gt;Manslaughter&lt;br /&gt;\n&lt;small&gt;(total deaths)&lt;/small&gt;&lt;br /&gt;\n&lt;small&gt;(2015)&lt;/small&gt; &lt;sup id=\"cite_ref-2\" class=\"reference\"&gt;\n&lt;a href=\"#cite_note-2\"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;\n&lt;/th&gt;\n&lt;th&gt;Murder and Nonnegligent\n&lt;p&gt;Manslaughter Rate&lt;br /&gt;\n&lt;small&gt;(per 100,000 inhabitants)&lt;/small&gt;&lt;br /&gt;\n&lt;small&gt;(2015)&lt;/small&gt;&lt;/p&gt;\n&lt;/th&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Alabama\" title=\"Alabama\"&gt;Alabama&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;4,853,875&lt;/td&gt;\n&lt;td&gt;348&lt;/td&gt;\n&lt;td&gt;7.2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Alaska\" title=\"Alaska\"&gt;Alaska&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;737,709&lt;/td&gt;\n&lt;td&gt;59&lt;/td&gt;\n&lt;td&gt;8.0&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;"
  },
  {
    "objectID": "14-web-scraping.html#the-rvest-package",
    "href": "14-web-scraping.html#the-rvest-package",
    "title": "14  Web scraping",
    "section": "14.2 The rvest package",
    "text": "14.2 The rvest package\nThe tidyverse provides a web harvesting package called rvest. The first step using this package is to import the webpage into R. The package makes this quite simple:\n\nlibrary(tidyverse)\nlibrary(rvest)\nh &lt;- read_html(url)\n\nNote that the entire Murders in the US Wikipedia webpage is now contained in h. The class of this object is:\n\nclass(h)\n\n[1] \"xml_document\" \"xml_node\"    \n\n\nThe rvest package is actually more general; it handles XML documents. XML is a general markup language (that’s what the ML stands for) that can be used to represent any kind of data. HTML is a specific type of XML specifically developed for representing webpages. Here we focus on HTML documents.\nNow, how do we extract the table from the object h? If we print h, we don’t really see much:\n\nh\n\n{html_document}\n&lt;html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-typography-survey-disabled vector-toc-available\" lang=\"en\" dir=\"ltr\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body class=\"skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr ...\n\n\nWe can see all the code that defines the downloaded webpage using the html_text function like this:\n\nhtml_text(h)\n\nIf we look at it, we can see the data we are after are stored in an HTML table: you can see this in this line of the HTML code above &lt;table class=\"wikitable sortable\"&gt;. The different parts of an HTML document, often defined with a message in between &lt; and &gt; are referred to as nodes. The rvest package includes functions to extract nodes of an HTML document: html_nodes extracts all nodes of different types and html_node extracts the first one. To extract the tables from the html code we use:\n\ntab &lt;- h |&gt; html_nodes(\"table\")\n\nNow, instead of the entire webpage, we just have the html code for the tables in the page:\n\ntab\n\n{xml_nodeset (2)}\n[1] &lt;table class=\"wikitable sortable\"&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;State\\n&lt;/th&gt;\\n&lt;th&gt;\\n ...\n[2] &lt;table class=\"nowraplinks hlist mw-collapsible mw-collapsed navbox-inner\" ...\n\n\nThe table we are interested is the first one:\n\ntab[[1]]\n\n{html_node}\n&lt;table class=\"wikitable sortable\"&gt;\n[1] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;State\\n&lt;/th&gt;\\n&lt;th&gt;\\n&lt;a href=\"/wiki/List_of_U.S._states ...\n\n\nThis is clearly not a tidy dataset, not even a data frame. In the code above, you can definitely see a pattern and writing code to extract just the data is very doable. In fact, rvest includes a function just for converting HTML tables into data frames:\n\ntab &lt;- read_html(url) |&gt; html_nodes(\"table\") %&gt;% .[[1]] |&gt; html_table()\nclass(tab)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWe are now much closer to having a usable data table:\n\ntab &lt;- tab |&gt; setNames(c(\"state\", \"population\", \"total\", \"murder_rate\")) \nhead(tab)\n\n# A tibble: 6 × 4\n  state      population total murder_rate\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt;\n1 Alabama    4,853,875  348           7.2\n2 Alaska     737,709    59            8  \n3 Arizona    6,817,565  309           4.5\n4 Arkansas   2,977,853  181           6.1\n5 California 38,993,940 1,861         4.8\n6 Colorado   5,448,819  176           3.2\n\n\nWe still have some wrangling to do. For example, we need to remove the commas and turn characters into numbers. Before continuing with this, we will learn a more general approach to extracting information from web sites."
  },
  {
    "objectID": "14-web-scraping.html#sec-css-selectors",
    "href": "14-web-scraping.html#sec-css-selectors",
    "title": "14  Web scraping",
    "section": "14.3 CSS selectors",
    "text": "14.3 CSS selectors\nThe default look of a webpage made with the most basic HTML is quite unattractive. The aesthetically pleasing pages we see today are made using CSS to define the look and style of webpages. The fact that all pages for a company have the same style usually results from their use of the same CSS file to define the style. The general way these CSS files work is by defining how each of the elements of a webpage will look. The title, headings, itemized lists, tables, and links, for example, each receive their own style including font, color, size, and distance from the margin. CSS does this by leveraging patterns used to define these elements, referred to as selectors. An example of such a pattern, which we used above, is table, but there are many, many more.\nIf we want to grab data from a webpage and we happen to know a selector that is unique to the part of the page containing this data, we can use the html_nodes function. However, knowing which selector can be quite complicated. In fact, the complexity of webpages has been increasing as they become more sophisticated. For some of the more advanced ones, it seems almost impossible to find the nodes that define a particular piece of data. However, selector gadgets actually make this possible.\nSelectorGadget1 is piece of software that allows you to interactively determine what CSS selector you need to extract specific components from the webpage. If you plan on scraping data other than tables from html pages, we highly recommend you install it. A Chrome extension is available which permits you to turn on the gadget and then, as you click through the page, it highlights parts and shows you the selector you need to extract these parts. There are various demos of how to do this including rvest author Hadley Wickham’s vignette2 and other tutorials based on the vignette3 4."
  },
  {
    "objectID": "14-web-scraping.html#json",
    "href": "14-web-scraping.html#json",
    "title": "14  Web scraping",
    "section": "14.4 JSON",
    "text": "14.4 JSON\nSharing data on the internet has become more and more common. Unfortunately, providers use different formats, which makes it harder for data scientists to wrangle data into R. Yet there are some standards that are also becoming more common. Currently, a format that is widely being adopted is the JavaScript Object Notation or JSON. Because this format is very general, it is nothing like a spreadsheet. This JSON file looks more like the code you use to define a list. Here is an example of information stored in a JSON format:\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\n[\n  {\n    \"name\": \"Miguel\",\n    \"student_id\": 1,\n    \"exam_1\": 85,\n    \"exam_2\": 86\n  },\n  {\n    \"name\": \"Sofia\",\n    \"student_id\": 2,\n    \"exam_1\": 94,\n    \"exam_2\": 93\n  },\n  {\n    \"name\": \"Aya\",\n    \"student_id\": 3,\n    \"exam_1\": 87,\n    \"exam_2\": 88\n  },\n  {\n    \"name\": \"Cheng\",\n    \"student_id\": 4,\n    \"exam_1\": 90,\n    \"exam_2\": 91\n  }\n] \n\n\nThe file above actually represents a data frame. To read it, we can use the function fromJSON from the jsonlite package. Note that JSON files are often made available via the internet. Several organizations provide a JSON API or a web service that you can connect directly to and obtain data. Here is an example providing information Nobel prize winners:\n\nnobel &lt;- fromJSON(\"http://api.nobelprize.org/v1/prize.json\")\n\nThis downloads a list. The first argument is a table with information about Nobel prize winners:\n\nfilter(nobel$prize, category == \"literature\" & year == \"1971\") |&gt; pull(laureates)\n\n[[1]]\n   id firstname surname\n1 645     Pablo  Neruda\n                                                                                               motivation\n1 \"for a poetry that with the action of an elemental force brings alive a continent's destiny and dreams\"\n  share\n1     1\n\n\nYou can learn much more by examining tutorials and help files from the jsonlite package. This package is intended for relatively simple tasks such as converting data into tables. For more flexibility, we recommend rjson."
  },
  {
    "objectID": "14-web-scraping.html#apis",
    "href": "14-web-scraping.html#apis",
    "title": "14  Web scraping",
    "section": "14.5 APIs",
    "text": "14.5 APIs\nAn API, or Application Programming Interface, is a set of rules and protocols that allows different software entities to communicate with each other. It defines methods and data formats that software components should use when requesting and exchanging information.\nAPIs can be understood as middlemen between different software systems, facilitating their interactions. They play a crucial role in enabling integration that make today’s software so interconnected and versatile.\nThere are several types of APIs. The main ones related to retrieving data are:\n\nWeb Services:\n\nOften built using protocols like HTTP/HTTPS.\nCommonly used to enable applications to communicate with each other over the web. For instance, a weather application for a smartphone may use a web API to request weather data from a remote server.\n\nDatabases:\n\nEnable communication between an application and a database.\nExamples include SQL-based calls or more abstracted ORM (Object-Relational Mapping) frameworks.\n\n\nOther APIs include: * Hardware APIs: Used for applications to interact with hardware, such as printers, cameras, or graphics cards.\n\nRemote Procedure Calls: Allows a protocol to execute a program or procedure on a remote server rather than on a local system.\nLibrary or Framework APIs: Define how to interact with a particular programming library or framework.\nOperating Systems:Define how software applications request and perform lower-level services performed by an operating system.\n\nKey concepts associated with APIs:\n\nEndpoints: Specific functions available through the API. For web APIs, an endpoint is usually a specific URL where the API can be accessed.\nMethods: Actions that can be performed. In web APIs, these often correspond to HTTP methods like GET, POST, PUT, DELETE.\nRequests and Responses: The act of asking the API to perform its function is a request, and the data it returns to you is the response.\nRate Limits: Restrictions on how often you can call the API, often used to prevent abuse or overloading of the service.\nAuthentication and Authorization: Mechanisms to ensure that only approved users or applications can use the API. Common methods include API keys, OAuth, or JWT.\nData Formats: Many web APIs exchange data in a specific format, often JSON or XML.\n\nIn today’s digital age, APIs are foundational. They enable the creation of complex, feature-rich applications by allowing different software components to leverage each other’s capabilities seamlessly."
  },
  {
    "objectID": "14-web-scraping.html#httr2-package",
    "href": "14-web-scraping.html#httr2-package",
    "title": "14  Web scraping",
    "section": "14.6 httr2 package",
    "text": "14.6 httr2 package\nThe httr2 package provides functions to work with HTTP requests. One of the core functions in this package is request, which is used to form request to send to web services. The req_perform function sends the request.\nThis request function formas a HTTP GET request to the specified URL. Typically, HTTP GET requests are used to retrieve information from a server based on the provided URL.\nThe function returns an object of class response. This object contains all the details of the server’s response, including status code, headers, and content. You can then use other httr2 functions to extract or interpret information from this response.\nLet’s say you want to retrieve a webpage or API endpoint:\n\nlibrary(readr)\nlibrary(httr2)\nurl &lt;- \"https://data.cdc.gov/resource/gvsb-yw6g.csv\"\nresponse &lt;- request(url) |&gt; req_perform()\n\nWe can see the results of the request by looking at the returned object.\n\nresponse\n\n&lt;httr2_response&gt;\n\n\nGET https://data.cdc.gov/resource/gvsb-yw6g.csv\n\n\nStatus: 200 OK\n\n\nContent-Type: text/csv\n\n\nBody: In memory (111411 bytes)\n\n\nTo extract the body, which is where the data are, we can use resp_body_string and send the result, a comma delimited string, to read_csv.\n\ntab &lt;- response |&gt; resp_body_string() |&gt; read_csv()\n\nRows: 1000 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): level\ndbl  (7): perc_diff, percent_pos, percent_pos_2_week, percent_pos_4_week, nu...\ndttm (2): posted, mmwrweek_end\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote it is only 1000 entries. API often limit how much you can download. We can change this by adding a query parameter to thr request. Here we use the req_url_path_append.\n\ntab &lt;- request(url) |&gt; \n   req_url_path_append(\"?$limit=10000000\") |&gt; \n   req_perform() |&gt;\n   resp_body_string() |&gt; \n   read_csv()\n\nRows: 34361 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): level\ndbl  (7): perc_diff, percent_pos, percent_pos_2_week, percent_pos_4_week, nu...\ndttm (2): posted, mmwrweek_end\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndim(tab)\n\n[1] 34361    10\n\n\nThe CDC service returns data in csv format. A more common format used by web services is JSON. The CDC also provides data in json format through a the url:\n\nurl &lt;- \"https://data.cdc.gov/resource/gvsb-yw6g.json\"\n\nTo extract the data table we use the fromJSON function.\n\ntab &lt;- request(url) |&gt; \n   req_perform() |&gt; \n   resp_body_string() |&gt; \n   fromJSON(flatten = TRUE)\n\nWhen working with APIs, it’s essential to check the API’s documentation for rate limits, required headers, or authentication methods. The httr package provides tools to handle these requirements, such as setting headers or using OAuth authentication."
  },
  {
    "objectID": "14-web-scraping.html#exercises-will-not-be-included-in-midterm",
    "href": "14-web-scraping.html#exercises-will-not-be-included-in-midterm",
    "title": "14  Web scraping",
    "section": "14.7 Exercises (will not be included in midterm)",
    "text": "14.7 Exercises (will not be included in midterm)\n\nVisit the following web page: https://web.archive.org/web/20181024132313/http://www.stevetheump.com/Payrolls.htm\n\nNotice there are several tables. Say we are interested in comparing the payrolls of teams across the years. The next few exercises take us through the steps needed to do this.\nStart by applying what you learned to read in the website into an object called h.\n\nNote that, although not very useful, we can actually see the content of the page by typing:\n\n\nhtml_text(h)\n\nThe next step is to extract the tables. For this, we can use the html_nodes function. We learned that tables in html are associated with the table node. Use the html_nodes function and the table node to extract the first table. Store it in an object nodes.\n\nThe html_nodes function returns a list of objects of class xml_node. We can see the content of each one using, for example, the html_text function. You can see the content for an arbitrarily picked component like this:\n\n\nhtml_text(nodes[[8]])\n\nIf the content of this object is an html table, we can use the html_table function to convert it to a data frame. Use the html_table function to convert the 8th entry of nodes into a table.\n\nRepeat the above for the first 4 components of nodes. Which of the following are payroll tables:\n\n\nAll of them.\n1\n2\n2-4\n\n\nRepeat the above for the first last 3 components of nodes. Which of the following is true:\n\n\nThe last entry in nodes shows the average across all teams through time, not payroll per team.\nAll three are payroll per team tables.\nAll three are like the first entry, not a payroll table.\nAll of the above.\n\n\nWe have learned that the first and last entries of nodes are not payroll tables. Redefine nodes so that these two are removed.\nWe saw in the previous analysis that the first table node is not actually a table. This happens sometimes in html because tables are used to make text look a certain way, as opposed to storing numeric values. Remove the first component and then use sapply and html_table to convert each node in nodes into a table. Note that in this case, sapply will return a list of tables. You can also use lapply to assure that a list is applied.\nLook through the resulting tables. Are they all the same? Could we just join them with bind_rows?\nCreate two tables, call them tab_1 and tab_2 using entries 10 and 19.\nUse a full_join function to combine these two tables. Before you do this you will have to fix the missing header problem. You will also need to make the names match.\nAfter joining the tables, you see several NAs. This is because some teams are in one table and not the other. Use the anti_join function to get a better idea of why this is happening.\nWe see see that one of the problems is that Yankees are listed as both N.Y. Yankees and NY Yankees. In the next section, we will learn efficient approaches to fixing problems like this. Here we can do it “by hand” as follows:\n\n\ntab_1 &lt;- tab_1 |&gt;\n  mutate(Team = ifelse(Team == \"N.Y. Yankees\", \"NY Yankees\", Team))\n\nNow join the tables and show only Oakland and the Yankees and the payroll columns."
  },
  {
    "objectID": "14-web-scraping.html#footnotes",
    "href": "14-web-scraping.html#footnotes",
    "title": "14  Web scraping",
    "section": "",
    "text": "http://selectorgadget.com/↩︎\nhttps://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html↩︎\nhttps://stat4701.github.io/edav/2015/04/02/rvest_tutorial/↩︎\nhttps://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/↩︎"
  },
  {
    "objectID": "15-locales.html",
    "href": "15-locales.html",
    "title": "15  Locales",
    "section": "",
    "text": "Notice the character on this file.\n\nfn &lt;- file.path(system.file(\"extdata\", package = \"dslabs\"), \"calificaciones.csv\")\nreadLines(fn)\n\n[1] \"\\\"nombre\\\",\\\"f.n.\\\",\\\"estampa\\\",\\\"puntuaci\\xf3n\\\"\"                       \n[2] \"\\\"Beyonc\\xe9\\\",\\\"04 de septiembre de 1981\\\",2023-09-22 02:11:02,\\\"87,5\\\"\"\n[3] \"\\\"Bl\\xfcmchen\\\",\\\"20 de abril de 1980\\\",2023-09-22 03:23:05,\\\"99,0\\\"\"    \n[4] \"\\\"Jo\\xe3o\\\",\\\"10 de junio de 1931\\\",2023-09-21 22:43:28,\\\"98,9\\\"\"        \n[5] \"\\\"L\\xf3pez\\\",\\\"24 de julio de 1969\\\",2023-09-22 01:06:59,\\\"88,7\\\"\"       \n[6] \"\\\"\\xd1engo\\\",\\\"15 de diciembre de 1981\\\",2023-09-21 23:35:37,\\\"93,1\\\"\"   \n[7] \"\\\"Pl\\xe1cido\\\",\\\"24 de enero de 1941\\\",2023-09-21 23:17:21,\\\"88,7\\\"\"     \n[8] \"\\\"Thal\\xeda\\\",\\\"26 de agosto de 1971\\\",2023-09-21 23:08:02,\\\"83,0\\\"\"     \n\n\nThe unrecognizable characters actually lead to read.csv failing:\n\ntry({x &lt;- read.csv(fn)})\n\nError in make.names(col.names, unique = TRUE) : \n  invalid multibyte string 4\n\n\nThis is because it is not UTF encoding, which is the default:\n\nSys.getlocale()\n\n[1] \"en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\"\n\n\nThe locale is a group of information about your system. This includes the encoding, the language, and the time zone. This can affect how data is read into R. A mismatch of encodings creates weird problems often without warning or error.\nYou can use the stri_enc_detect function in the stringi package to predict the encoding of a character:\n\nlibrary(stringi)\nx &lt;- readLines(fn, n = 1)\nstri_enc_detect(x)\n\n[[1]]\n    Encoding Language Confidence\n1 ISO-8859-1       es       0.75\n2 ISO-8859-2       cs       0.18\n3   UTF-16BE                0.10\n4   UTF-16LE                0.10\n5  Shift_JIS       ja       0.10\n6    GB18030       zh       0.10\n7       Big5       zh       0.10\n\n\nWe can also use this readr function to detect encoding of files:\n\nlibrary(readr)\nguess_encoding(fn)\n\n# A tibble: 3 × 2\n  encoding   confidence\n  &lt;chr&gt;           &lt;dbl&gt;\n1 ISO-8859-1       0.92\n2 ISO-8859-2       0.72\n3 ISO-8859-9       0.53\n\n\nThe read_csv permits us to define elements of the encoding through the locale argument. It switches the local only temporarily, while running the parser read_csv. The locale for R remains the same after calling this.\n\nx &lt;- read_csv(fn, locale = locale(encoding = \"ISO-8859-1\"))\n\nRows: 7 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): nombre, f.n.\nnum  (1): puntuación\ndttm (1): estampa\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nx\n\n# A tibble: 7 × 4\n  nombre   f.n.                     estampa             puntuación\n  &lt;chr&gt;    &lt;chr&gt;                    &lt;dttm&gt;                   &lt;dbl&gt;\n1 Beyoncé  04 de septiembre de 1981 2023-09-22 02:11:02        875\n2 Blümchen 20 de abril de 1980      2023-09-22 03:23:05        990\n3 João     10 de junio de 1931      2023-09-21 22:43:28        989\n4 López    24 de julio de 1969      2023-09-22 01:06:59        887\n5 Ñengo    15 de diciembre de 1981  2023-09-21 23:35:37        931\n6 Plácido  24 de enero de 1941      2023-09-21 23:17:21        887\n7 Thalía   26 de agosto de 1971     2023-09-21 23:08:02        830\n\n\nNow notice the last column. Compare it to what we saw with readLines. They were numbers that used the European decimal point. This confuses read_csv. We can also change the encoding so the Europearn decimals are used.\n\nx &lt;- read_csv(fn, locale = locale(encoding = \"ISO-8859-1\", decimal_mark = \",\"))\n\nRows: 7 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): nombre, f.n.\ndbl  (1): puntuación\ndttm (1): estampa\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nx\n\n# A tibble: 7 × 4\n  nombre   f.n.                     estampa             puntuación\n  &lt;chr&gt;    &lt;chr&gt;                    &lt;dttm&gt;                   &lt;dbl&gt;\n1 Beyoncé  04 de septiembre de 1981 2023-09-22 02:11:02       87.5\n2 Blümchen 20 de abril de 1980      2023-09-22 03:23:05       99  \n3 João     10 de junio de 1931      2023-09-21 22:43:28       98.9\n4 López    24 de julio de 1969      2023-09-22 01:06:59       88.7\n5 Ñengo    15 de diciembre de 1981  2023-09-21 23:35:37       93.1\n6 Plácido  24 de enero de 1941      2023-09-21 23:17:21       88.7\n7 Thalía   26 de agosto de 1971     2023-09-21 23:08:02       83  \n\n\nNow let’s try to change the dates to date format:\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\ndmy(x$f.n.)\n\nWarning: All formats failed to parse. No formats found.\n\n\n[1] NA NA NA NA NA NA NA\n\n\nNothing gets correctly converted. This is because the dates are in Spanish. You can change the locale to use Spanish as the language:\n\nparse_date(x$f.n., format = \"%d de %B de %Y\", locale = locale(date_names = \"es\"))\n\n[1] \"1981-09-04\" \"1980-04-20\" \"1931-06-10\" \"1969-07-24\" \"1981-12-15\"\n[6] \"1941-01-24\" \"1971-08-26\"\n\n\nFinally notice that two students turned in the homework past the deadline of September 21\n\nx$estampa &gt;= make_date(2023, 9, 22)\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE\n\n\nHowever, with times we have to be particularly careful as some functions default to UTC.\n\ntz(x$estampa)\n\n[1] \"UTC\"\n\n\nBut these times are in the default GMT. If we change to out timezone:\n\nwith_tz(x$estampa, tz =  Sys.timezone()) &gt;= make_date(2023, 9, 22)\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nwe see everybody turned it in on time."
  },
  {
    "objectID": "16-text-mining.html#case-study-trump-tweets",
    "href": "16-text-mining.html#case-study-trump-tweets",
    "title": "16  Text mining",
    "section": "16.1 Case study: Trump tweets",
    "text": "16.1 Case study: Trump tweets\nDuring the 2016 US presidential election, then candidate Donald J. Trump used his twitter account as a way to communicate with potential voters. On August 6, 2016, Todd Vaziri tweeted1 about Trump that “Every non-hyperbolic tweet is from iPhone (his staff). Every hyperbolic tweet is from Android (from him).” Data scientist David Robinson conducted an analysis2 to determine if data supported this assertion. Here, we go through David’s analysis to learn some of the basics of text mining. To learn more about text mining in R, we recommend the Text Mining with R book3 by Julia Silge and David Robinson.\nWe will use the following libraries:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\n\nIn general, we can extract data directly from Twitter using the rtweet package. However, in this case, a group has already compiled data for us and made it available at http://www.trumptwitterarchive.com. We can get the data from their JSON API using a script like this:\n\nurl &lt;- 'http://www.trumptwitterarchive.com/data/realdonaldtrump/%s.json'\ntrump_tweets &lt;- map(2009:2017, ~sprintf(url, .x)) |&gt;\n  map_df(jsonlite::fromJSON, simplifyDataFrame = TRUE) |&gt;\n  filter(!is_retweet & !str_detect(text, '^\"')) |&gt;\n  mutate(created_at = parse_date_time(created_at, \n                                      orders = \"a b! d! H!:M!:S! z!* Y!\",\n                                      tz=\"EST\")) \n\nFor convenience, we include the result of the code above in the dslabs package:\n\nlibrary(dslabs)\n\nYou can see the data frame with information about the tweets by typing\n\nhead(trump_tweets)\n\nwith the following variables included:\n\nnames(trump_tweets)\n\n[1] \"source\"                  \"id_str\"                 \n[3] \"text\"                    \"created_at\"             \n[5] \"retweet_count\"           \"in_reply_to_user_id_str\"\n[7] \"favorite_count\"          \"is_retweet\"             \n\n\nThe help file ?trump_tweets provides details on what each variable represents. The tweets are represented by the text variable:\n\ntrump_tweets$text[16413] |&gt; str_wrap(width = options()$width) |&gt; cat()\n\nGreat to be back in Iowa! #TBT with @JerryJrFalwell joining me in Davenport-\nthis past winter. #MAGA https://t.co/A5IF0QHnic\n\n\nand the source variable tells us which device was used to compose and upload each tweet:\n\ntrump_tweets |&gt; count(source) |&gt; arrange(desc(n)) |&gt; head(5)\n\n               source     n\n1  Twitter Web Client 10718\n2 Twitter for Android  4652\n3  Twitter for iPhone  3962\n4           TweetDeck   468\n5     TwitLonger Beta   288\n\n\nWe are interested in what happened during the campaign, so for this analysis we will focus on what was tweeted between the day Trump announced his campaign and election day. We define the following table containing just the tweets from that time period. Note that we use extract to remove the Twitter for part of the source and filter out retweets.\n\ncampaign_tweets &lt;- trump_tweets |&gt; \n  extract(source, \"source\", \"Twitter for (.*)\") |&gt;\n  filter(source %in% c(\"Android\", \"iPhone\") &\n           created_at &gt;= ymd(\"2015-06-17\") & \n           created_at &lt; ymd(\"2016-11-08\")) |&gt;\n  filter(!is_retweet) |&gt;\n  arrange(created_at) |&gt; \n  as_tibble()\n\nWe can now use data visualization to explore the possibility that two different groups were tweeting from these devices. For each tweet, we will extract the hour, East Coast time (EST), it was tweeted and then compute the proportion of tweets tweeted at each hour for each device:\n\ncampaign_tweets |&gt;\n  mutate(hour = hour(with_tz(created_at, \"EST\"))) |&gt;\n  count(source, hour) |&gt;\n  group_by(source) |&gt;\n  mutate(percent = n / sum(n)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(hour, percent, color = source)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(labels = percent_format()) +\n  labs(x = \"Hour of day (EST)\", y = \"% of tweets\", color = \"\")\n\n\n\n\nWe notice a big peak for the Android in the early hours of the morning, between 6 and 8 AM. There seems to be a clear difference in these patterns. We will therefore assume that two different entities are using these two devices.\nWe will now study how the tweets differ when we compare Android to iPhone. To do this, we introduce the tidytext package."
  },
  {
    "objectID": "16-text-mining.html#text-as-data",
    "href": "16-text-mining.html#text-as-data",
    "title": "16  Text mining",
    "section": "16.2 Text as data",
    "text": "16.2 Text as data\nThe tidytext package helps us convert free form text into a tidy table. Having the data in this format greatly facilitates data visualization and the use of statistical techniques.\n\nlibrary(tidytext)\n\nThe main function needed to achieve this is unnest_tokens. A token refers to a unit that we are considering to be a data point. The most common token will be words, but they can also be single characters, ngrams, sentences, lines, or a pattern defined by a regex. The functions will take a vector of strings and extract the tokens so that each one gets a row in the new table. Here is a simple example:\n\npoem &lt;- c(\"Roses are red,\", \"Violets are blue,\", \n          \"Sugar is sweet,\", \"And so are you.\")\nexample &lt;- tibble(line = c(1, 2, 3, 4),\n                      text = poem)\nexample\n\n# A tibble: 4 × 2\n   line text             \n  &lt;dbl&gt; &lt;chr&gt;            \n1     1 Roses are red,   \n2     2 Violets are blue,\n3     3 Sugar is sweet,  \n4     4 And so are you.  \n\nexample |&gt; unnest_tokens(word, text)\n\n# A tibble: 13 × 2\n    line word   \n   &lt;dbl&gt; &lt;chr&gt;  \n 1     1 roses  \n 2     1 are    \n 3     1 red    \n 4     2 violets\n 5     2 are    \n 6     2 blue   \n 7     3 sugar  \n 8     3 is     \n 9     3 sweet  \n10     4 and    \n11     4 so     \n12     4 are    \n13     4 you    \n\n\nNow let’s look at an example from the tweets. We will look at tweet number 3008 because it will later permit us to illustrate a couple of points:\n\ni &lt;- 3008\ncampaign_tweets$text[i] |&gt; str_wrap(width = 65) |&gt; cat()\n\nGreat to be back in Iowa! #TBT with @JerryJrFalwell joining me in\nDavenport- this past winter. #MAGA https://t.co/A5IF0QHnic\n\ncampaign_tweets[i,] |&gt; \n  unnest_tokens(word, text) |&gt;\n  pull(word) \n\n [1] \"great\"          \"to\"             \"be\"             \"back\"          \n [5] \"in\"             \"iowa\"           \"tbt\"            \"with\"          \n [9] \"jerryjrfalwell\" \"joining\"        \"me\"             \"in\"            \n[13] \"davenport\"      \"this\"           \"past\"           \"winter\"        \n[17] \"maga\"           \"https\"          \"t.co\"           \"a5if0qhnic\"    \n\n\nNote that the function tries to convert tokens into words. A minor adjustment is to remove the links to pictures:\n\nlinks &lt;- \"https://t.co/[A-Za-z\\\\d]+|&amp;\"\ncampaign_tweets[i,] |&gt; \n  mutate(text = str_replace_all(text, links, \"\"))  |&gt;\n  unnest_tokens(word, text) |&gt;\n  pull(word)\n\n [1] \"great\"          \"to\"             \"be\"             \"back\"          \n [5] \"in\"             \"iowa\"           \"tbt\"            \"with\"          \n [9] \"jerryjrfalwell\" \"joining\"        \"me\"             \"in\"            \n[13] \"davenport\"      \"this\"           \"past\"           \"winter\"        \n[17] \"maga\"          \n\n\nNow we are now ready to extract the words for all our tweets.\n\ntweet_words &lt;- campaign_tweets |&gt; \n  mutate(text = str_replace_all(text, links, \"\"))  |&gt;\n  unnest_tokens(word, text)\n\nAnd we can now answer questions such as “what are the most commonly used words?”:\n\ntweet_words |&gt; \n  count(word) |&gt;\n  arrange(desc(n))\n\n# A tibble: 6,264 × 2\n   word      n\n   &lt;chr&gt; &lt;int&gt;\n 1 the    2330\n 2 to     1413\n 3 and    1245\n 4 in     1190\n 5 i      1151\n 6 a      1121\n 7 you     999\n 8 of      982\n 9 is      944\n10 on      880\n# ℹ 6,254 more rows\n\n\nIt is not surprising that these are the top words. The top words are not informative. The tidytext package has a database of these commonly used words, referred to as stop words, in text mining:\n\nstop_words\n\n# A tibble: 1,149 × 2\n   word        lexicon\n   &lt;chr&gt;       &lt;chr&gt;  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n# ℹ 1,139 more rows\n\n\nIf we filter out rows representing stop words with filter(!word %in% stop_words$word):\n\ntweet_words &lt;- campaign_tweets |&gt; \n  mutate(text = str_replace_all(text, links, \"\"))  |&gt;\n  unnest_tokens(word, text) |&gt;\n  filter(!word %in% stop_words$word) \n\nwe end up with a much more informative set of top 10 tweeted words:\n\ntweet_words |&gt; \n  count(word) |&gt;\n  top_n(10, n) |&gt;\n  mutate(word = reorder(word, n)) |&gt;\n  arrange(desc(n))\n\n# A tibble: 10 × 2\n   word                      n\n   &lt;fct&gt;                 &lt;int&gt;\n 1 trump2016               415\n 2 hillary                 407\n 3 people                  304\n 4 makeamericagreatagain   298\n 5 america                 255\n 6 clinton                 240\n 7 poll                    220\n 8 crooked                 205\n 9 trump                   204\n10 cruz                    161\n\n\nSome exploration of the resulting words (not shown here) reveals a couple of unwanted characteristics in our tokens. First, some of our tokens are just numbers (years, for example). We want to remove these and we can find them using the regex ^\\d+$. Second, some of our tokens come from a quote and they start with '. We want to remove the ' when it is at the start of a word so we will just str_replace. We add these two lines to the code above to generate our final table:\n\ntweet_words &lt;- campaign_tweets |&gt; \n  mutate(text = str_replace_all(text, links, \"\"))  |&gt;\n  unnest_tokens(word, text) |&gt;\n  filter(!word %in% stop_words$word &\n           !str_detect(word, \"^\\\\d+$\")) |&gt;\n  mutate(word = str_replace(word, \"^'\", \"\"))\n\nNow that we have all our words in a table, along with information about what device was used to compose the tweet they came from, we can start exploring which words are more common when comparing Android to iPhone.\nFor each word, we want to know if it is more likely to come from an Android tweet or an iPhone tweet. We therefore compute, for each word, what proportion of all words it represent for Android and iPhone, respectively.\n\nandroid_vs_iphone &lt;- tweet_words |&gt;\n  count(word, source) |&gt;\n  pivot_wider(names_from = \"source\", values_from = \"n\", values_fill = 0) |&gt;\n  mutate(p_a = Android / sum(Android), p_i = iPhone / sum(iPhone),\n         percent_diff = (p_a - p_i) / ((p_a + p_i)/2) * 100)\n\nFor words appearing at least 100 times in total, here are the highest percent differences for Android\n\nandroid_vs_iphone |&gt; filter(Android + iPhone &gt;= 100) |&gt;\n  arrange(desc(percent_diff))\n\n# A tibble: 30 × 6\n   word        Android iPhone     p_a     p_i percent_diff\n   &lt;chr&gt;         &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n 1 bad             104     26 0.00645 0.00188        110. \n 2 crooked         156     49 0.00968 0.00354         92.9\n 3 cnn             116     37 0.00720 0.00267         91.7\n 4 ted              86     28 0.00533 0.00202         90.1\n 5 interviewed      76     25 0.00471 0.00180         89.3\n 6 media            78     26 0.00484 0.00188         88.2\n 7 cruz            115     46 0.00713 0.00332         72.9\n 8 hillary         290    117 0.0180  0.00845         72.2\n 9 win              74     30 0.00459 0.00217         71.8\n10 president        84     35 0.00521 0.00253         69.4\n# ℹ 20 more rows\n\n\nand the top for iPhone:\n\nandroid_vs_iphone |&gt; filter(Android + iPhone &gt;= 100) |&gt; \n  arrange(percent_diff)\n\n# A tibble: 30 × 6\n   word                  Android iPhone       p_a     p_i percent_diff\n   &lt;chr&gt;                   &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n 1 makeamericagreatagain       0    298 0         0.0215       -200   \n 2 join                        1    157 0.0000620 0.0113       -198.  \n 3 trump2016                   3    412 0.000186  0.0297       -198.  \n 4 tomorrow                   24    101 0.00149   0.00729      -132.  \n 5 vote                       46     67 0.00285   0.00484       -51.6 \n 6 america                   114    141 0.00707   0.0102        -36.0 \n 7 tonight                    70     84 0.00434   0.00606       -33.1 \n 8 iowa                       62     65 0.00385   0.00469       -19.8 \n 9 poll                      117    103 0.00726   0.00744        -2.43\n10 trump                     112     92 0.00695   0.00664         4.49\n# ℹ 20 more rows\n\n\nWe already see somewhat of a pattern in the types of words that are being tweeted more from one device versus the other. However, we are not interested in specific words but rather in the tone. Vaziri’s assertion is that the Android tweets are more hyperbolic. So how can we check this with data? Hyperbolic is a hard sentiment to extract from words as it relies on interpreting phrases. However, words can be associated to more basic sentiment such as anger, fear, joy, and surprise. In the next section, we demonstrate basic sentiment analysis."
  },
  {
    "objectID": "16-text-mining.html#sentiment-analysis",
    "href": "16-text-mining.html#sentiment-analysis",
    "title": "16  Text mining",
    "section": "16.3 Sentiment analysis",
    "text": "16.3 Sentiment analysis\nIn sentiment analysis, we assign a word to one or more “sentiments”. Although this approach will miss context-dependent sentiments, such as sarcasm, when performed on large numbers of words, summaries can provide insights.\nThe first step in sentiment analysis is to assign a sentiment to each word. As we demonstrate, the tidytext package includes several maps or lexicons. We will also be using the textdata package.\n\nlibrary(tidytext)\nlibrary(textdata)\n\nThe bing lexicon divides words into positive and negative sentiments. We can see this using the tidytext function get_sentiments:\n\nget_sentiments(\"bing\")\n\nThe AFINN lexicon assigns a score between -5 and 5, with -5 the most negative and 5 the most positive. Note that this lexicon needs to be downloaded the first time you call the function get_sentiment:\n\nget_sentiments(\"afinn\")\n\nThe loughran and nrc lexicons provide several different sentiments. Note that these also have to be downloaded the first time you use them.\n\nget_sentiments(\"loughran\") |&gt; count(sentiment)\n\n# A tibble: 6 × 2\n  sentiment        n\n  &lt;chr&gt;        &lt;int&gt;\n1 constraining   184\n2 litigious      904\n3 negative      2355\n4 positive       354\n5 superfluous     56\n6 uncertainty    297\n\n\n\nget_sentiments(\"nrc\") |&gt; count(sentiment)\n\n# A tibble: 10 × 2\n   sentiment        n\n   &lt;chr&gt;        &lt;int&gt;\n 1 anger         1245\n 2 anticipation   837\n 3 disgust       1056\n 4 fear          1474\n 5 joy            687\n 6 negative      3316\n 7 positive      2308\n 8 sadness       1187\n 9 surprise       532\n10 trust         1230\n\n\nFor our analysis, we are interested in exploring the different sentiments of each tweet so we will use the nrc lexicon:\n\nnrc &lt;- get_sentiments(\"nrc\") |&gt;\n  select(word, sentiment)\n\nWe can combine the words and sentiments using inner_join, which will only keep words associated with a sentiment. Here are 10 random words extracted from the tweets:\n\ntweet_words |&gt; inner_join(nrc, by = \"word\", relationship = \"many-to-many\") |&gt; \n  select(source, word, sentiment) |&gt; \n  sample_n(5)\n\n# A tibble: 5 × 3\n  source  word     sentiment   \n  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;       \n1 Android enjoy    joy         \n2 iPhone  terrific sadness     \n3 iPhone  tactics  trust       \n4 Android clue     anticipation\n5 iPhone  change   fear        \n\n\nNow we are ready to perform a quantitative analysis comparing Android and iPhone by comparing the sentiments of the tweets posted from each device. Here we could perform a tweet-by-tweet analysis, assigning a sentiment to each tweet. However, this will be challenging since each tweet will have several sentiments attached to it, one for each word appearing in the lexicon. For illustrative purposes, we will perform a much simpler analysis: we will count and compare the frequencies of each sentiment appearing in each device.\n\nsentiment_counts &lt;- tweet_words |&gt;\n  left_join(nrc, by = \"word\", relationship = \"many-to-many\") |&gt;\n  count(source, sentiment) |&gt;\n  pivot_wider(names_from = \"source\", values_from = \"n\") |&gt;\n  mutate(sentiment = replace_na(sentiment, replace = \"none\"))\nsentiment_counts\n\n# A tibble: 11 × 3\n   sentiment    Android iPhone\n   &lt;chr&gt;          &lt;int&gt;  &lt;int&gt;\n 1 anger            962    527\n 2 anticipation     917    707\n 3 disgust          639    314\n 4 fear             799    486\n 5 joy              695    536\n 6 negative        1657    931\n 7 positive        1827   1494\n 8 sadness          901    514\n 9 surprise         530    365\n10 trust           1248   1001\n11 none           11834  10793\n\n\nFor each sentiment, we can compute the percent difference in proportion for Android compared to iPhone:\n\nsentiment_counts |&gt;\n  mutate(p_a = Android / sum(Android) , \n         p_i = iPhone / sum(iPhone), \n         percent_diff = (p_a - p_i) / ((p_a + p_i)/2) * 100) |&gt;\n  arrange(desc(percent_diff))\n\n# A tibble: 11 × 6\n   sentiment    Android iPhone    p_a    p_i percent_diff\n   &lt;chr&gt;          &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n 1 disgust          639    314 0.0290 0.0178      48.1   \n 2 anger            962    527 0.0437 0.0298      37.8   \n 3 negative        1657    931 0.0753 0.0527      35.3   \n 4 sadness          901    514 0.0409 0.0291      33.8   \n 5 fear             799    486 0.0363 0.0275      27.6   \n 6 surprise         530    365 0.0241 0.0207      15.3   \n 7 anticipation     917    707 0.0417 0.0400       4.04  \n 8 joy              695    536 0.0316 0.0303       4.01  \n 9 trust           1248   1001 0.0567 0.0567       0.0846\n10 positive        1827   1494 0.0830 0.0846      -1.85  \n11 none           11834  10793 0.538  0.611      -12.7   \n\n\nSo we do see some differences and the order is interesting: the largest three sentiments are disgust, anger, and negative!\nIf we are interested in exploring which specific words are driving these differences, we can refer back to our android_iphone_or object:\n\nandroid_vs_iphone |&gt; inner_join(nrc, by = \"word\") |&gt;\n  filter(sentiment == \"disgust\") |&gt;\n  arrange(desc(percent_diff))\n\n# A tibble: 157 × 7\n   word      Android iPhone       p_a   p_i percent_diff sentiment\n   &lt;chr&gt;       &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;    \n 1 abuse           1      0 0.0000620     0          200 disgust  \n 2 angry          10      0 0.000620      0          200 disgust  \n 3 arrogant        2      0 0.000124      0          200 disgust  \n 4 attacking       5      0 0.000310      0          200 disgust  \n 5 belittle        2      0 0.000124      0          200 disgust  \n 6 blame           1      0 0.0000620     0          200 disgust  \n 7 bleeding        1      0 0.0000620     0          200 disgust  \n 8 bombed          5      0 0.000310      0          200 disgust  \n 9 clumsy          1      0 0.0000620     0          200 disgust  \n10 crushed         1      0 0.0000620     0          200 disgust  \n# ℹ 147 more rows\n\n\nand we can make a graph:\n\n\n\n\n\nThis is just a simple example of the many analyses one can perform with tidytext. To learn more, we again recommend the Tidy Text Mining book4."
  },
  {
    "objectID": "16-text-mining.html#exercises-will-not-be-included-in-midterm",
    "href": "16-text-mining.html#exercises-will-not-be-included-in-midterm",
    "title": "16  Text mining",
    "section": "16.4 Exercises (will not be included in midterm)",
    "text": "16.4 Exercises (will not be included in midterm)\nProject Gutenberg is a digital archive of public domain books. The R package gutenbergr facilitates the importation of these texts into R.\nYou can install and load by typing:\n\ninstall.packages(\"gutenbergr\")\nlibrary(gutenbergr)\n\nYou can see the books that are available like this:\n\ngutenberg_metadata\n\n\nUse str_detect to find the ID of the novel Pride and Prejudice.\nWe notice that there are several versions. The gutenberg_works() function filters this table to remove replicates and include only English language works. Read the help file and use this function to find the ID for Pride and Prejudice.\n\n\nUse the gutenberg_download function to download the text for Pride and Prejudice. Save it to an object called book.\n\n\nUse the tidytext package to create a tidy table with all the words in the text. Save the table in an object called words\nWe will later make a plot of sentiment versus location in the book. For this, it will be useful to add a column with the word number to the table.\nRemove the stop words and numbers from the words object. Hint: use the anti_join.\nNow use the AFINN lexicon to assign a sentiment value to each word.\nMake a plot of sentiment score versus location in the book and add a smoother.\nAssume there are 300 words per page. Convert the locations to pages and then compute the average sentiment in each page. Plot that average score by page. Add a smoother that appears to go through data."
  },
  {
    "objectID": "16-text-mining.html#footnotes",
    "href": "16-text-mining.html#footnotes",
    "title": "16  Text mining",
    "section": "",
    "text": "https://twitter.com/tvaziri/status/762005541388378112/photo/1↩︎\nhttp://varianceexplained.org/r/trump-tweets/↩︎\nhttps://www.tidytextmining.com/↩︎\nhttps://www.tidytextmining.com/↩︎"
  },
  {
    "objectID": "17-probability.html#monte-carlo",
    "href": "17-probability.html#monte-carlo",
    "title": "17  Probability",
    "section": "17.1 Monte Carlo",
    "text": "17.1 Monte Carlo\nSuppose we take a random sample of \\(n\\) people not born on Feb 29. What is the chance that two or more people have the same birthday?\nWe can figure this out with math, but lets use a Monte Carlo simulation instead.\nThe sample function take a random sample with or without replacement. So here is a randome sample of \\(n = 25\\), 365 days\n\nset.seed(2023-10-1)\nn &lt;- 25\nbdays &lt;- sample(1:365, size = n, replace = TRUE)\n\nDid two or more people have the same birthday?\n\nany(duplicated(bdays))\n\n[1] FALSE\n\n\nThe probability can be thought of as taking \\(n\\) samples over and over again, for ever. The proportion of times we see two or more can be defined as the probability. So approximate \\(\\infty\\) with 100,000 and report the proportion for \\(n=25\\).\n\nn &lt;- 25\nB &lt;- 10^5\nreplicate(B,{\n  bdays &lt;- sample(1:365, size = n, replace = TRUE)\n  any(duplicated(bdays))\n}) |&gt; mean()\n\n[1] 0.56906\n\n\nUse Monte Carlo to estimate the probability for \\(n=1,\\dots,50\\). Make a plot of the probability versus \\(n\\).\n\nn &lt;- seq(1, 50)\npr &lt;- function(n){\n  replicate(B,{\n    bdays &lt;- sample(1:365, size = n, replace = TRUE)\n    any(duplicated(bdays))\n  }) |&gt; mean()\n}\np &lt;- sapply(n, pr)\nplot(n, p)\n\n\n\n\nWe can compute the exact probability:\n\\[\n1 - 1 \\times \\frac{364}{365}\\times\\frac{363}{365} \\dots \\frac{365-n + 1}{365}\n\\]\nWrite a function that computes this for any \\(n\\) then add a curve to the previous plot to confirm that our approximation worked.\n\nepr &lt;- function(n){\n  1 - prod(seq(365, 365 - n + 1)/365)\n}\nep &lt;- sapply(n, epr)\nplot(n, p)\nlines(n, ep, col = \"red\")"
  },
  {
    "objectID": "17-probability.html#random-variables",
    "href": "17-probability.html#random-variables",
    "title": "17  Probability",
    "section": "17.2 Random variables",
    "text": "17.2 Random variables\nWhat is the chance that a casino loses money on 1,000 people betting on black? Let’s play once:\n\nx &lt;- sample(c(-1, 1), size = 1, prob = c(9/19, 10/19))\n\nIf we play 10 times we get different values:\n\nreplicate(10, sample(c(-1, 1), size = 1, prob = c(9/19, 10/19)))\n\n [1]  1  1 -1  1 -1  1 -1 -1  1  1\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can get the same results using\n\nsample(c(-1, 1), size = 10, replace = TRUE, prob = c(9/19, 10/19))\n\n [1]  1  1 -1 -1  1 -1  1  1  1 -1\n\n\neven faster is to use\n\nx &lt;- rbinom(10, 1, 10/19); x &lt;- x*2 - 1\n\n\n\nThese are the outcomes of a random variable \\(X\\) with\n\\[\n\\mbox{Pr}(X = 1) = 10/19 \\mbox{ and } \\mbox{Pr}(X=-1) = 9/19\n\\]\nWe are interested in the sum of \\(n\\) realizations of this random variable: \\[\nS_n = \\sum_{i=1}^{n} X_i\n\\]\nAnd want to know\n\\[\n\\mbox{Pr}(S_n&lt;0)\n\\]\nHere is one realization of \\(S_n\\) with \\(n=1000\\):\n\nx &lt;- sample(c(-1, 1), size = 1000, replace = TRUE, prob = c(9/19, 10/19))\ns &lt;- sum(x)\n\nWe can use math to compute the probability above, but we will instead use a Monte Carlo simulation. This can be useful for checking our math and aslo for cases when we can’t do the math.\n\n17.2.1 Monte Carlo\nWe can see the distribution of \\(S\\) by randomly generating it an infinite number of times. Let’s approximate with 100,000 and look at the distribution of \\(S\\)\n\ns &lt;- replicate(10^5,{\n  x &lt;- sample(c(-1, 1), size = 1000, replace = TRUE, prob = c(9/19, 10/19))\n  sum(x)\n})\nplot(table(s))\n\n\n\n\nWe can use this compute the probability of interest:\n\nmean(s &lt; 0)\n\n[1] 0.04535"
  },
  {
    "objectID": "17-probability.html#clt",
    "href": "17-probability.html#clt",
    "title": "17  Probability",
    "section": "17.3 CLT",
    "text": "17.3 CLT\nDo you recognize the distribution of \\(S_n\\) shown above?\nIn your Probability class you learn about the Central Limit Theorem: the distribution of the sum of independent equally distributed random variables can be approximated with a normally distribution.\nBut for this to be useful, we need to know the mean \\(\\mbox{E}[S_n]\\) and standard error \\(\\mbox{SE}[S_n]\\)\nOnce we know this we can estimate the probability of \\(S_n\\) being in an interval, for any interval \\((a,b)\\):\n\\[\n\\mbox{Pr}(a&lt;S_n&lt;b) = \\mbox{Pr}\\left(\\frac{a-\\mbox{E}[S_n]}{\\mbox{SE}[S_n]} &lt; Z &lt; \\frac{b-\\mbox{E}[S_n]}{\\mbox{SE}[S_n]}\\right)\n\\] with\n\\[\nZ = \\frac{S_n-\\mbox{E}[S_n]}{\\mbox{SE}[S_n]}\n\\]\nan standard normal random variable.\nSo all we need is \\(\\mbox{E}[S_n]\\) and \\(\\mbox{SE}[S_n]\\). This is usually easier to compute than the actual distribution of \\(S_n\\)\nWhat is the expected value of \\(X_i\\)? Call it \\(\\mu\\).\n\\[\n\\mu = \\mbox{E}[X_i] = -1 \\times 9/19 + 1 \\times 10/19 = 1/19 \\approx 0.05\n\\]\nWhat is the standard error? Call it \\(\\sigma\\)\n\\[\n\\begin{align}\n\\sigma^2 &= \\mbox{Var}[X_i] = \\mbox{E}[(X_i-\\mu)^2] = (-1 - 1/19)^2 \\times 9/19 + (1 - 1/19)^2 \\times 10/19\\\\\n& = 4 \\times 10/19 \\times 9/19\n\\end{align}\n\\] Let’s check with a Monte Carlo simulation:\n\nx &lt;- sample(c(-1, 1), size = 10^6, replace = TRUE, prob = c(9/19, 10/19))\nmean(x)\n\n[1] 0.05299\n\nsqrt(mean((x - mean(x))^2))\n\n[1] 0.998595\n\nsd(x)\n\n[1] 0.9985955\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe sd function in R does not compute the standard error or population standard deviation. It returns an estimate of the population standard deviation based on s sample. It divides by \\(n-1\\) instead of \\(n\\). Here there practically no difference as \\(n\\) is 1 million.\n\n\nThis let’s us compute:\n\\[\n\\mbox{E}[S_n] = \\mbox{E}\\left[\\sum_{i=1}^n X_i\\right] =  \\sum_{i=1}^n \\mbox{E}[X_i] = n \\mu\n\\]\n\\[\n\\mbox{Var}[S_n] =\\mbox{Var}[\\sum_{i=1}^n X_i] =  \\sum_{i=1}^n\\mbox{Var}[X_i] = n \\sigma^2\n\\]\nSo\n\\[\n\\mbox{SE}[S]  = \\sqrt{n} \\sigma\n\\]\nSo \\[\n\\mbox{Pr}(S_n &lt; 0) = \\mbox{Pr}\\left(Z &lt; \\frac{-n\\mu}{\\sqrt{n}\\sigma}\\right)\n= \\mbox{Pr}\\left(Z &lt; \\frac{-\\sqrt{n}\\mu}{\\sigma}\\right)\n\\]\n\nn &lt;- 1000\nmu &lt;- 1/19\nsigma &lt;- 2*sqrt(9/19*10/19)\npnorm(-sqrt(n)*mu/sigma)\n\n[1] 0.04779035\n\n\nNow to answer “How many people need to play to lower the chance of losing money to less than 1%?” we want:\n\\[\n\\mbox{Pr}\\left(Z &lt; \\frac{-\\sqrt{n}\\mu}{\\sigma}\\right)\n\\]\nNotice that the the quantity \\(\\frac{-\\sqrt{n}\\mu}{\\sigma}\\) goes to \\(-\\infty\\) as \\(n\\) gets larger so we can make \\(\\mbox{Pr}(S_n&lt;0)\\) as small as we desire by simply making the \\(n\\) larger.\nWe again can use CLT\n\\[\n\\mbox{Pr}\\left(Z&lt;\\frac{-\\sqrt{n}\\mu}{\\sigma} \\right) \\leq 0.01 \\implies \\frac{-\\sqrt{n}\\mu}{\\sigma} = \\Phi^{-1}(0.01)\\implies \\sqrt{n} = -\\frac{\\sigma}{\\mu}{\\Phi^{-1}(0.01)}\n\\]\n\nn &lt;- ceiling((-sigma/mu*qnorm(0.01))^2)"
  },
  {
    "objectID": "17-probability.html#exercises",
    "href": "17-probability.html#exercises",
    "title": "17  Probability",
    "section": "17.4 Exercises",
    "text": "17.4 Exercises\n\nCheck that you selected the correct \\(n\\) with Monte Carlo simulation.\n\n\n## Hint: copy and paste one of the previous Monte Carlo simulations and change n\ns &lt;- replicate(10^5,{\n  x &lt;- sample(c(-1, 1), size = n, replace = TRUE, prob = c(9/19, 10/19))\n  sum(x)\n})\nmean(s &lt; 0)\n\n[1] 0.00999\n\n\n\nSuppose instead of the sum we are interested in the average:\n\n\\[\n\\bar{X}_n = S_n / n\n\\]\n\nWhat are the expected value of \\(\\bar{X}\\)?\n\n\\[\n\\mbox{E}[\\bar{X}_n] =  \\mbox{E}[\\bar{S}_n/n] = 1/n \\mbox{E}[\\bar{S}_n]  = 1/n \\times n \\times \\mu = \\mu\n\\]\n\nWhat is the standard error of \\(\\bar{X}_n\\)?\n\n\\[\n\\mbox{SE}[\\bar{X}_n] = 1/n \\mbox{SE}[\\bar{S}_n] = 1/n \\times \\sqrt{n} \\sigma = \\sigma / \\sqrt{n}\n\\]\n\nWhat do the previous two results tell us about the difference between \\(\\bar{X}_n\\) and \\(\\mu\\) when \\(n\\) is very large."
  },
  {
    "objectID": "17-probability.html#law-of-small-numbers",
    "href": "17-probability.html#law-of-small-numbers",
    "title": "17  Probability",
    "section": "17.5 Law of small numbers",
    "text": "17.5 Law of small numbers\nWhat is the distribution of the number of people that win the lottery if the probability of winning is 1 in a million and 500,000 people by one tickets?\nIf we define \\(X_i\\) as 1 if person \\(i\\)’s ticket won and 0 otherwise, then the number of people that win is the sum\n\\[\nS_n = \\sum_{i=1}^n X_i\n\\]\nwith \\(n=500,000\\) and\n\\[\n\\mbox{Pr}(X_i=1) = 0.000001\n\\].\nCLT says \\(S_n\\) should follow a normal distribution. Let’s run Monte Carlo to check:\n\ns &lt;- replicate(10^3,{\n  x &lt;- sample(c(0, 1), size = 5*10^5, replace = TRUE, \n              prob = c(1 - 10^-6, 10^-6))\n  sum(x)\n})\nplot(table(s))\n\n\n\n\nDoes that look normal?\nWhat happened? The rule of thumb of \\(n=30\\) was well surpassed.\nThe problem here is that \\(S_n\\) can’t be negative, the expected value is \\(n/10^{6}\\), but the standard error is about \\(\\sqrt{n}/10^3\\). So to even get a symmetric distribution we need \\(\\sqrt{n}/10^3\\) at least twice as big as \\(n/10^{6}\\).\nHowever, there is a distribution the does approximate the distribution of \\(S_n\\)."
  },
  {
    "objectID": "17-probability.html#exercise",
    "href": "17-probability.html#exercise",
    "title": "17  Probability",
    "section": "17.6 Exercise",
    "text": "17.6 Exercise\n\nCompare the estimates of the probabilities\n\n\\[\n\\mbox{Pr}(S_n = k), k=0,1,2,3,4\n\\]\nTo the probabilities of a Poisson distributed variavbel with rate \\(np = 500000/1000000 = 0.5\\). Hint: Use dpois function.\n\nsapply(0:3, function(k) mean(s == k))\n\n[1] 0.569 0.332 0.086 0.012\n\ndpois(0:3, 0.5)\n\n[1] 0.60653066 0.30326533 0.07581633 0.01263606"
  },
  {
    "objectID": "17-probability.html#financial-crisis",
    "href": "17-probability.html#financial-crisis",
    "title": "17  Probability",
    "section": "17.7 2008 Financial Crisis",
    "text": "17.7 2008 Financial Crisis\n\np &lt;- 0.04 #default prob\ns &lt;- replicate(10^5,{\n  x &lt;- sample(c(-200000, 10000), size = 10000, replace = TRUE, prob = c(p, 1-p))\n  sum(x)\n})\nhist(s)\n\n\n\n\n\np &lt;- 0.04 #default prob\ns &lt;- replicate(10^5,{\n  p &lt;- 0.04 + runif(1, -0.02, 0.02)\n  x &lt;- sample(c(-200000, 10000), size = 10000, replace = TRUE, prob = c(p, 1 - p))\n  sum(x)\n})\nhist(s)\n\n\n\n\nWhat assumption is violated now?\nRemember the formula for sum of variance is actually:\n\\[\n\\mbox{Var}(X_1 + X_2) = \\mbox{Var}(X_1) + \\mbox{Var}(X_2) + 2\\mbox{Cov}(X_1, X_2)\n\\]\nIf variables are positively correlated, variance goes up."
  },
  {
    "objectID": "17-probability.html#exercises-1",
    "href": "17-probability.html#exercises-1",
    "title": "17  Probability",
    "section": "17.8 Exercises",
    "text": "17.8 Exercises\nUse Monte Carlo simulations to answer these questions. Feel free to confirm using math.\n\nTwo teams, say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game. What is the probability that the Celtics win at least one game? Use a Monte Carlo simulation to compute (or confirm) your answer.\n\n\nb &lt;- 10^5\nnowins &lt;- replicate(b, {\n  x &lt;- sample(c(0, 1), size = 4, replace = TRUE, prob = c(0.6, 0.4))\n  sum(x) == 0\n})\n1 - mean(nowins)\n\n[1] 0.87002\n\n## using math\n1 - 0.6^4\n\n[1] 0.8704\n\n\n\nTwo teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games, therefore, wins the series. The teams are equally good so they each have a 50-50 chance of winning each game. If the Cavs lose the first game, what is the probability that they win the series?\nTwo teams, \\(A\\) and \\(B\\), are playing a seven game series. Team \\(A\\) is better than team \\(B\\) and has a \\(p&gt;0.5\\) chance of winning each game. Write a function that uses Monte Carlo simulation to compute the probability of the underdog winning the series for any value \\(p\\). The plot the results for p &lt;- seq(0.5, 0.95, 0.025).\nRepeat the exercise above, but now keep the probability fixed at p &lt;- 0.75 and compute the probability for different series lengths: best of 1 game, 3 games, 5 games,… Specifically, n &lt;- seq(1, 25, 2).\nAssume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is between 61 and 67 inches?\nImagine the distribution of male adults is approximately normal with an expected value of 69 and a standard deviation of 3. How tall is the male in the 99th percentile? Hint: use qnorm.\nThe distribution of IQ scores is approximately normally distributed. The average is 100 and the standard deviation is 15. Suppose you want to know the distribution of the highest IQ across all graduating classes if 10,000 people are born each in your school district. Run a Monte Carlo simulation with B=1000 generating 10,000 IQ scores and keeping the highest. Make a histogram.\nIn American Roulette you can also bet on green. There are 18 reds, 18 blacks and 2 greens (0 and 00). What are the chances the green comes out?\n\n\np &lt;- 1/19\n\n\nThe payout for winning on green is $17 dollars. This means that if you bet a dollar and it lands on green, you get $17. Create a sampling model using sample to simulate the random variable \\(X\\) for your winnings.\n\n\nsample(c(-1, 17), size = 1, replace = TRUE, prob = c(1-p, p))\n\n[1] -1\n\n\n\nCompute the expected value of \\(X\\).\n\n\n-1 * (1-p) + 17*p\n\n[1] -0.05263158\n\n\n\nCompute the standard error of \\(X\\).\n\n\n18 * sqrt(p * (1-p))\n\n[1] 4.019344\n\n\n\nNow create a random variable \\(S\\) that is the sum of your winnings after betting on green 1000 times.\n\n\nn &lt;- 1000\nx &lt;- sample(c(-1, 17), size = n, replace = TRUE, prob = c(1-p, p))\nsum(x)\n\n[1] -244\n\n\n\nWhat is the expected value of \\(S\\)?\n\n\nmu &lt;- n * (-1 * (1-p) + 17*p)\n\n\nWhat is the standard error of \\(S\\)?\n\n\nse &lt;- sqrt(n) * 18*sqrt(p*(1 - p))\n\n\nUse CLT to estimate the probability that you end up winning money?\n\n\\[\nPr(Z &gt; -\\mu/se) = ?\n\\]\n\n1 - pnorm(-mu/se)\n\n[1] 0.3394053\n\n\n\nCreate a Monte Carlo simulation that generates 1,000 outcomes of \\(S\\). Compute the average and standard deviation of the resulting list to confirm the results of 6 and 7.\n\n\nB &lt;- 100000\ns &lt;- replicate(B,{\n  n &lt;- 1000\n  x &lt;- sample(c(-1, 17), size = n, replace = TRUE, prob = c(18/19, 1/19))\n  sum(x)\n})\nmean(s &gt; 0)\n\n[1] 0.33563\n\n\n\nNow check your answer based on CLT using the Monte Carlo result."
  },
  {
    "objectID": "18-inference.html#parameters-and-estimates",
    "href": "18-inference.html#parameters-and-estimates",
    "title": "18  Inference",
    "section": "18.1 Parameters and Estimates",
    "text": "18.1 Parameters and Estimates\nReal Clear Politics is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results reporting estimates of the popular vote for the 2016 presidential election\n\n\n\n\n\nPoll\nDate\nSample\nMoE\nClinton\nTrump\nSpread\n\n\n\n\nRCP Average\n10/31 - 11/7\n--\n--\n47.2\n44.3\nClinton +2.9\n\n\nBloomberg\n11/4 - 11/6\n799 LV\n3.5\n46.0\n43.0\nClinton +3\n\n\nEconomist\n11/4 - 11/7\n3669 LV\n--\n49.0\n45.0\nClinton +4\n\n\nIBD\n11/3 - 11/6\n1026 LV\n3.1\n43.0\n42.0\nClinton +1\n\n\nABC\n11/3 - 11/6\n2220 LV\n2.5\n49.0\n46.0\nClinton +3\n\n\nFOX News\n11/3 - 11/6\n1295 LV\n2.5\n48.0\n44.0\nClinton +4\n\n\nMonmouth\n11/3 - 11/6\n748 LV\n3.6\n50.0\n44.0\nClinton +6\n\n\nCBS News\n11/2 - 11/6\n1426 LV\n3.0\n47.0\n43.0\nClinton +4\n\n\nLA Times\n10/31 - 11/6\n2935 LV\n4.5\n43.0\n48.0\nTrump +5\n\n\nNBC News\n11/3 - 11/5\n1282 LV\n2.7\n48.0\n43.0\nClinton +5\n\n\nNBC News\n10/31 - 11/6\n30145 LV\n1.0\n51.0\n44.0\nClinton +7\n\n\nMcClatchy\n11/1 - 11/3\n940 LV\n3.2\n46.0\n44.0\nClinton +2\n\n\nReuters\n10/31 - 11/4\n2244 LV\n2.2\n44.0\n40.0\nClinton +4\n\n\nGravisGravis\n10/31 - 10/31\n5360 RV\n1.3\n50.0\n50.0\nTie\n\n\n\n\n\n\n\nLet’s make some observations about the table above.\n\nDifferent polls, all taken days before the election, report a different spread\nClinton won the popular vote by 2.1%.\nWe also see a column titled MoE which stands for margin of error."
  },
  {
    "objectID": "18-inference.html#predict-percents-competition",
    "href": "18-inference.html#predict-percents-competition",
    "title": "18  Inference",
    "section": "18.2 Predict percents competition",
    "text": "18.2 Predict percents competition\nWhat percent of the beads are red?\n\nRules:\n\nWinners gets a $25 gift certificate\nProvide an estimate and an interval.\nIf the true percent is not in the interval you are eliminated\nSmallest interval wins.\nYou can take a sample (with replacement) from the urn.\nIt costs you $0.10 per each bead you sample. Example: if your sample size is 250, and you win, you will break even since you will pay $25 to collect your $25 prize.\n\nThe dslabs package includes a function that shows a random draw from this urn:"
  },
  {
    "objectID": "18-inference.html#populations-samples-parameters-and-estimates",
    "href": "18-inference.html#populations-samples-parameters-and-estimates",
    "title": "18  Inference",
    "section": "18.3 Populations, samples, parameters, and estimates",
    "text": "18.3 Populations, samples, parameters, and estimates\nWe want to predict the proportion of red beads in the urn. Let’s call this quantity \\(p\\), which then tells us the proportion of blue beads \\(1-p\\), and the spread \\(p - (1-p)\\), which simplifies to \\(2p - 1\\).\nIn statistical textbooks:\n\nthe beads in the urn are called the population.\nThe proportion of red beads in the population \\(p\\) is called a parameter.\nThe 25 beads we see in the previous plot are called a sample.\n\nThe task of statistical inference is to predict the parameter \\(p\\) using the observed data in the sample."
  },
  {
    "objectID": "18-inference.html#the-sample-average",
    "href": "18-inference.html#the-sample-average",
    "title": "18  Inference",
    "section": "18.4 The sample average",
    "text": "18.4 The sample average\n\\[\\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i\\]\nHas some desirable properties:\n\\[\n\\mbox{E}(\\bar{X}) = p\n\\]\n\\[\n\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\n\\]\nThis result reveals the power of polls. The expected value of the sample proportion \\(\\bar{X}\\) is the parameter of interest \\(p\\) and we can make the standard error as small as we want by increasing \\(N\\). The law of large numbers tells us that with a large enough poll, our estimate converges to \\(p\\).\nHere is the standard error for \\(p=.25, .5,\\) and \\(0.75\\):\n\n\n\n\n\nFrom the plot we see that we would need a very large polls to get the standarr error below 1%.\nBut there is one more very useful property based on CLT:\n\\[\n\\bar{X} \\sim \\mbox{Normal}\\left(p, \\sqrt{p(1-p)/N}\\right)\n\\]"
  },
  {
    "objectID": "18-inference.html#clt-in-practice",
    "href": "18-inference.html#clt-in-practice",
    "title": "18  Inference",
    "section": "18.5 CLT in practice",
    "text": "18.5 CLT in practice\nNow we can ask a more practical questions such as what is the probability that our estimate is within 1% of the actual \\(p\\). We can write it like this and actually use CLT to answer the question:\n\\[\n\\mbox{Pr}(| \\bar{X} - p| \\leq .01)\n\\]\nwhich is the same as:\n\\[\n\\mbox{Pr}(\\bar{X} - p\\leq  .01) - \\mbox{Pr}(\\bar{X} - p \\leq - .01)\n\\]\nwe standardize \\(\\bar{X}\\) to get a approximately standard normal \\(Z\\):\n\\[\n\\mbox{Pr}\\left(Z \\leq \\frac{ \\,.01} {\\mbox{SE}(\\bar{X})} \\right) -\n\\mbox{Pr}\\left(Z \\leq - \\frac{ \\,.01} {\\mbox{SE}(\\bar{X})} \\right)\n\\]\nWe are almost ready to get a number except since we don’t know \\(p\\), we don’t know \\(\\mbox{SE}(\\bar{X})\\).\nBut it turns out that the CLT still works if we estimate the standard error by using \\(\\bar{X}\\) in place of \\(p\\). We say that we plug-in the estimate. Our estimate of the standard error is therefore:\n\\[\n\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N}\n\\]\nNow we continue with our calculation, but dividing by \\(\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N})\\) instead. In our first sample we had 12 blue and 13 red so \\(\\bar{X} = 0.52\\) and our estimate of standard error is:\n\nx_hat &lt;- 0.52\nse &lt;- sqrt(x_hat*(1 - x_hat)/25)\nse\n\n[1] 0.09991997\n\n\nAnd now we can answer the question of the probability of being close to \\(p\\). The answer is:\n\npnorm(0.01/se) - pnorm(-0.01/se)\n\n[1] 0.07971926\n\n\nEarlier we mentioned the margin of error. Now we can define it because it is simply two times the standard error, which we can now estimate. In our case it is:\n\n1.96*se\n\n[1] 0.1958431\n\n\nWhy do we multiply by \\(1.96 \\hat{\\mbox{SE}}(\\bar{X})\\)? Because if you ask what is the probability that we are within 1.96 standard errors from \\(p\\), we get:\n\\[\n\\mbox{Pr}\\left(Z \\leq \\, 1.96\\,\\hat{\\mbox{SE}}(\\bar{X})  / \\hat{\\mbox{SE}}(\\bar{X}) \\right) -\n\\mbox{Pr}\\left(Z \\leq - 1.96\\, \\hat{\\mbox{SE}}(\\bar{X}) / \\hat{\\mbox{SE}}(\\bar{X}) \\right)\n\\]\nwhich is:\n\\[\n\\mbox{Pr}\\left(Z \\leq 1.96 \\right) -\n\\mbox{Pr}\\left(Z \\leq - 1.96\\right)\n\\]\nwhich we know is about 95%:\n\npnorm(1.96) - pnorm(-1.96)\n\n[1] 0.9500042\n\n\nHence, there is a 95% probability that \\(\\bar{X}\\) will be within \\(1.96\\times \\hat{SE}(\\bar{X})\\), in our case within about 0.2, of \\(p\\). Note that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error. We often round 1.96 up to 2 for simplicity of presentation.\nIn summary, the CLT tells us that our poll based on a sample size of \\(25\\) is not very useful. We don’t really learn much when the margin of error is this large. All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes.\n\n18.5.1 A Monte Carlo simulation\nSuppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:\n\nB &lt;- 10000\nN &lt;- 1000\np &lt;- 0.45\nx_hat &lt;- replicate(B, {\n  x &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\n  mean(x)\n})\n\nThe problem is, of course, we don’t know p. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function take_poll(n=1000) instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.\nOne thing we therefore do to corroborate theoretical results is to pick one or several values of p and run the simulations. Let’s set p=0.45. We can then simulate a poll:\n\np &lt;- 0.45\nN &lt;- 1000\n\nx &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))\nx_hat &lt;- mean(x)\n\nIn this particular sample, our estimate is x_hat. We can use that code to do a Monte Carlo simulation:\n\nB &lt;- 10000\nx_hat &lt;- replicate(B, {\n  x &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))\n  mean(x)\n})\n\nTo review, the theory tells us that \\(\\bar{X}\\) is approximately normally distributed, has expected value \\(p=\\) 0.45 and standard error \\(\\sqrt{p(1-p)/N}\\) = 0.0157321. The simulation confirms this:\n\nmean(x_hat)\n\n[1] 0.4500729\n\nmean((x_hat-mean(x_hat))^2)\n\n[1] 0.0002484474\n\n\nA histogram and qq-plot confirm that the normal approximation is accurate as well:\n\n\n\n\n\nOf course, in real life we would never be able to run such an experiment because we don’t know \\(p\\). But we could run it for various values of \\(p\\) and \\(N\\) and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing p and N."
  },
  {
    "objectID": "18-inference.html#confidence-intervals",
    "href": "18-inference.html#confidence-intervals",
    "title": "18  Inference",
    "section": "18.6 Confidence intervals",
    "text": "18.6 Confidence intervals\nWe want to know the probability that the interval\n\\[\n[\\bar{X} - 1.96\\hat{\\mbox{SE}}(\\bar{X}), \\bar{X} + 1.96\\hat{\\mbox{SE}}(\\bar{X})]\n\\]\ncontains the true proportion \\(p\\). First, consider that the start and end of these intervals are random variables: every time we take a sample, they change. To illustrate this, run the Monte Carlo simulation above twice. We use the same parameters as above:\n\np &lt;- 0.45\nN &lt;- 1000\n\nAnd notice that the interval here:\n\nx &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))\nx_hat &lt;- mean(x)\nse_hat &lt;- sqrt(x_hat * (1 - x_hat) / N)\nx_hat + c(-1, 1)*se_hat * 1.96\n\n[1] 0.4420549 0.5039451\n\n\nis different from this one:\n\nx &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))\nx_hat &lt;- mean(x)\nse_hat &lt;- sqrt(x_hat * (1 - x_hat) / N)\nc(x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)\n\n[1] 0.3993149 0.4606851\n\n\nKeep sampling and creating intervals and you will see the random variation.\nTo determine the probability that the interval includes \\(p\\), we need to compute this:\n\\[\n\\mbox{Pr}\\left(\\bar{X} - 1.96\\hat{\\mbox{SE}}(\\bar{X}) \\leq p \\leq \\bar{X} + 1.96\\hat{\\mbox{SE}}(\\bar{X})\\right)\n\\]\nBy subtracting and dividing the same quantities in all parts of the equation, we get that the above is equivalent to:\n\\[\n\\mbox{Pr}\\left(-1.96 \\leq \\frac{\\bar{X}- p}{\\hat{\\mbox{SE}}(\\bar{X})} \\leq  1.96\\right)\n\\]\nThe term in the middle is an approximately normal random variable with expected value 0 and standard error 1, which we have been denoting with \\(Z\\), so we have:\n\\[\n\\mbox{Pr}\\left(-1.96 \\leq Z \\leq  1.96\\right)\n\\]\nwhich we can quickly compute using :\n\npnorm(1.96) - pnorm(-1.96)\n\n[1] 0.9500042\n\n\nproving that we have a 95% probability.\nIf we want to have a larger probability, say 99%, we need to multiply by whatever z satisfies the following:\n\\[\n\\mbox{Pr}\\left(-z \\leq Z \\leq  z\\right) = 0.99\n\\]\nUsing:\n\nz &lt;- qnorm(0.995)\nz\n\n[1] 2.575829\n\n\nwill achieve this because by definition pnorm(qnorm(0.995)) is 0.995 and by symmetry pnorm(1-qnorm(0.995)) is 1 - 0.995. As a consequence, we have that:\n\npnorm(z) - pnorm(-z)\n\n[1] 0.99\n\n\nis 0.995 - 0.005 = 0.99.\nWe can use this approach for any probability, not just 0.95 and 0.99. In statistics textbooks, these are usually written for any probability as \\(1-\\alpha\\). We can then obtain the \\(z\\) for the equation above noting using z = qnorm(1 - alpha / 2) because \\(1 - \\alpha/2 - \\alpha/2 = 1 - \\alpha\\).\nSo, for example, for \\(\\alpha=0.05\\), \\(1 - \\alpha/2 = 0.975\\) and we get the 1.96 we have been using:\n\nqnorm(0.975)\n\n[1] 1.959964\n\n\n\n18.6.1 A Monte Carlo simulation\nWe can run a Monte Carlo simulation to confirm that, in fact, a 95% confidence interval includes \\(p\\) 95% of the time.\n\nN &lt;- 1000\nB &lt;- 10000\ninside &lt;- replicate(B, {\n  x &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))\n  x_hat &lt;- mean(x)\n  se_hat &lt;- sqrt(x_hat * (1 - x_hat) / N)\n  between(p, x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)\n})\nmean(inside)\n\n[1] 0.9482\n\n\nThe following plot shows the first 100 confidence intervals. In this case, we created the simulation so the black line denotes the parameter we are trying to estimate:"
  },
  {
    "objectID": "18-inference.html#exercises",
    "href": "18-inference.html#exercises",
    "title": "18  Inference",
    "section": "18.7 Exercises",
    "text": "18.7 Exercises\n\nWrite a line of code that gives you the standard error se for the proportion of red beads for several values of \\(p\\), specifically for p &lt;- seq(0, 1, length = 100). Make a plot of se versus p.\nIf we are interested in the difference in proportions, \\(\\mu = p - (1-p)\\), our estimate is \\(\\hat{\\mu} = \\bar{X} - (1-\\bar{X})\\) expected value and SE of \\(\\hat{\\mu}\\).\nIf the actual \\(p=.45\\), it means the Republicans are winning by a relatively large margin since \\(\\mu = -.1\\), which is a 10% margin of victory. In this case, what is the standard error of \\(2\\hat{X}-1\\) if we take a sample of \\(N=25\\)?\nGiven the answer to 9, which of the following best describes your strategy of using a sample size of \\(N=25\\)?\n\n\nThe expected value of our estimate \\(2\\bar{X}-1\\) is \\(\\mu\\), so our prediction will be right on.\nOur standard error is larger than the difference, so the chances of \\(2\\bar{X}-1\\) being positive and throwing us off were not that small. We should pick a larger sample size.\nThe difference is 10% and the standard error is about 0.2, therefore much smaller than the difference.\nBecause we don’t know \\(p\\), we have no way of knowing that making \\(N\\) larger would actually improve our standard error.\n\nFor the next exercises, we will use actual polls from the 2016 election. You can load the data from the dslabs package.\n\nlibrary(dslabs)\n\nSpecifically, we will use all the national polls that ended within one week before the election.\n\nlibrary(tidyverse)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(enddate &gt;= \"2016-10-31\" & state == \"U.S.\") \n\n\nFor the first poll, you can obtain the samples size and estimated Clinton percentage with:\n\n\nN &lt;- polls$samplesize[1]\nx_hat &lt;- polls$rawpoll_clinton[1]/100\n\nAssume there are only two candidates and construct a 95% confidence interval for the election night proportion \\(p\\).\n\nNow use dplyr to add a confidence interval as two columns, call them lower and upper, to the object poll. Then use select to show the pollster, enddate, x_hat,lower, upper variables. Hint: define temporary columns x_hat and se_hat.\nThe final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column, call it hit, to the previous table stating if the confidence interval included the true proportion \\(p=0.482\\) or not.\nFor the table you just created, what proportion of confidence intervals included \\(p\\)?\nIf these confidence intervals are constructed correctly, and the theory holds up, what proportion should include \\(p\\)?\n\n(@). A much smaller proportion of the polls than expected produce confidence intervals containing \\(p\\). If you look closely at the table, you will see that most polls that fail to include \\(p\\) are underestimating. The reason for this is undecided voters, individuals polled that do not yet know who they will vote for or do not want to say. Because, historically, undecideds divide evenly between the two main candidates on election day, it is more informative to estimate the spread or the difference between the proportion of two candidates \\(\\mu\\), which in this election was \\(0. 482 - 0.461 = 0.021\\). Assume that there are only two parties and that \\(\\mu = 2p - 1\\), redefine polls as below and re-do exercise 1, but for the difference.\n\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(enddate &gt;= \"2016-10-31\" & state == \"U.S.\")  |&gt;\n  mutate(mu_hat = rawpoll_clinton / 100 - rawpoll_trump / 100)\n\n\nNow recalculate confidence interavls for the difference and see how often the polls include the election night difference of 0.21.\n\n\npolls |&gt; mutate(p = (mu_hat + 1)/2, se = 2*sqrt(p*(1-p))/sqrt(samplesize)) |&gt;\n  mutate(lower = mu_hat - 1.96*se, upper = mu_hat + 1.96*se)\n\n   state  startdate    enddate\n1   U.S. 2016-11-03 2016-11-06\n2   U.S. 2016-11-01 2016-11-07\n3   U.S. 2016-11-02 2016-11-06\n4   U.S. 2016-11-04 2016-11-07\n5   U.S. 2016-11-03 2016-11-06\n6   U.S. 2016-11-03 2016-11-06\n7   U.S. 2016-11-02 2016-11-06\n8   U.S. 2016-11-03 2016-11-05\n9   U.S. 2016-11-04 2016-11-07\n10  U.S. 2016-11-04 2016-11-06\n11  U.S. 2016-11-01 2016-11-04\n12  U.S. 2016-11-03 2016-11-06\n13  U.S. 2016-11-01 2016-11-03\n14  U.S. 2016-11-05 2016-11-07\n15  U.S. 2016-11-01 2016-11-07\n16  U.S. 2016-11-01 2016-11-05\n17  U.S. 2016-10-31 2016-11-06\n18  U.S. 2016-11-04 2016-11-05\n19  U.S. 2016-10-31 2016-11-06\n20  U.S. 2016-11-02 2016-11-06\n21  U.S. 2016-11-04 2016-11-07\n22  U.S. 2016-10-20 2016-11-01\n23  U.S. 2016-11-01 2016-11-03\n24  U.S. 2016-10-28 2016-11-01\n25  U.S. 2016-11-02 2016-11-05\n26  U.S. 2016-10-31 2016-11-04\n27  U.S. 2016-11-01 2016-11-04\n28  U.S. 2016-10-04 2016-11-06\n29  U.S. 2016-11-03 2016-11-06\n30  U.S. 2016-10-31 2016-11-03\n31  U.S. 2016-10-30 2016-11-03\n32  U.S. 2016-11-02 2016-11-05\n33  U.S. 2016-10-30 2016-11-02\n34  U.S. 2016-10-29 2016-11-01\n35  U.S. 2016-10-28 2016-10-31\n36  U.S. 2016-10-30 2016-11-03\n37  U.S. 2016-11-01 2016-11-04\n38  U.S. 2016-10-30 2016-11-01\n39  U.S. 2016-10-26 2016-10-31\n40  U.S. 2016-10-29 2016-11-02\n41  U.S. 2016-11-01 2016-11-03\n42  U.S. 2016-11-04 2016-11-06\n43  U.S. 2016-10-28 2016-11-01\n44  U.S. 2016-10-29 2016-11-02\n45  U.S. 2016-10-28 2016-11-01\n46  U.S. 2016-10-31 2016-11-02\n47  U.S. 2016-10-27 2016-10-31\n48  U.S. 2016-11-03 2016-11-05\n49  U.S. 2016-10-27 2016-10-31\n50  U.S. 2016-10-25 2016-10-31\n51  U.S. 2016-10-30 2016-11-05\n52  U.S. 2016-10-30 2016-11-01\n53  U.S. 2016-10-29 2016-11-04\n54  U.S. 2016-11-02 2016-11-04\n55  U.S. 2016-10-31 2016-11-06\n56  U.S. 2016-10-28 2016-11-03\n57  U.S. 2016-11-01 2016-11-03\n58  U.S. 2016-10-30 2016-11-05\n59  U.S. 2016-10-27 2016-11-02\n60  U.S. 2016-10-29 2016-11-04\n61  U.S. 2016-10-26 2016-11-01\n62  U.S. 2016-10-31 2016-11-02\n63  U.S. 2016-10-31 2016-10-31\n64  U.S. 2016-10-28 2016-11-03\n65  U.S. 2016-10-30 2016-11-01\n66  U.S. 2016-10-27 2016-11-02\n67  U.S. 2016-11-01 2016-11-02\n68  U.S. 2016-10-26 2016-11-01\n69  U.S. 2016-10-29 2016-10-31\n70  U.S. 2016-10-25 2016-10-31\n                                                     pollster grade samplesize\n1                                    ABC News/Washington Post    A+       2220\n2                                     Google Consumer Surveys     B      26574\n3                                                       Ipsos    A-       2195\n4                                                      YouGov     B       3677\n5                                            Gravis Marketing    B-      16639\n6  Fox News/Anderson Robbins Research/Shaw & Company Research     A       1295\n7                                     CBS News/New York Times    A-       1426\n8                                NBC News/Wall Street Journal    A-       1282\n9                                                    IBD/TIPP    A-       1107\n10                                           Selzer & Company    A+        799\n11                                          Angus Reid Global    A-       1151\n12                                        Monmouth University    A+        748\n13                                             Marist College     A        940\n14                                   The Times-Picayune/Lucid  &lt;NA&gt;       2521\n15                                      USC Dornsife/LA Times  &lt;NA&gt;       2972\n16                      RKM Research and Communications, Inc.    B+       1009\n17                                       CVOTER International    C+       1625\n18                                            Morning Consult  &lt;NA&gt;       1482\n19                                               SurveyMonkey    C-      70194\n20                   Rasmussen Reports/Pulse Opinion Research    C+       1500\n21                                              Insights West  &lt;NA&gt;        940\n22                                 RAND (American Life Panel)    B-       2269\n23 Fox News/Anderson Robbins Research/Shaw & Company Research     A       1107\n24                                    CBS News/New York Times    A-        927\n25                                   ABC News/Washington Post    A+       1937\n26                                                      Ipsos    A-       2244\n27                                   ABC News/Washington Post    A+       1685\n28                                                     YouGov     B      84292\n29                                                   IBD/TIPP    A-       1026\n30                                   ABC News/Washington Post    A+       1419\n31                                                   IBD/TIPP    A-        898\n32                                                   IBD/TIPP    A-        903\n33                                   ABC News/Washington Post    A+       1151\n34                                   ABC News/Washington Post    A+       1167\n35                                   ABC News/Washington Post    A+       1182\n36                                                      Ipsos    A-       2021\n37                                                   IBD/TIPP    A-        804\n38                                                     YouGov     B       1233\n39                                                   IBD/TIPP    A-       1018\n40                                                      Ipsos    A-       1922\n41                   Rasmussen Reports/Pulse Opinion Research    C+       1500\n42                                   The Times-Picayune/Lucid  &lt;NA&gt;       2584\n43                                                      Ipsos    A-       1772\n44                                                   IBD/TIPP    A-        867\n45                                                   IBD/TIPP    A-        862\n46                   Rasmussen Reports/Pulse Opinion Research    C+       1500\n47                                                      Ipsos    A-       1464\n48                                   The Times-Picayune/Lucid  &lt;NA&gt;       2526\n49                   Rasmussen Reports/Pulse Opinion Research    C+       1500\n50                                    Google Consumer Surveys     B      24316\n51                                       CVOTER International    C+       1572\n52                   Rasmussen Reports/Pulse Opinion Research    C+       1500\n53                                       CVOTER International    C+       1479\n54                                   The Times-Picayune/Lucid  &lt;NA&gt;       2561\n55                                      USC Dornsife/LA Times  &lt;NA&gt;       2935\n56                                       CVOTER International    C+       1395\n57                                   The Times-Picayune/Lucid  &lt;NA&gt;       2627\n58                                      USC Dornsife/LA Times  &lt;NA&gt;       2988\n59                                       CVOTER International    C+       1329\n60                                      USC Dornsife/LA Times  &lt;NA&gt;       2987\n61                                       CVOTER International    C+       1383\n62                                   The Times-Picayune/Lucid  &lt;NA&gt;       2620\n63                                           Gravis Marketing    B-       5360\n64                                      USC Dornsife/LA Times  &lt;NA&gt;       2962\n65                                   The Times-Picayune/Lucid  &lt;NA&gt;       2617\n66                                      USC Dornsife/LA Times  &lt;NA&gt;       2938\n67                                           Gravis Marketing    B-       2435\n68                                      USC Dornsife/LA Times  &lt;NA&gt;       3004\n69                                   The Times-Picayune/Lucid  &lt;NA&gt;       2600\n70                                      USC Dornsife/LA Times  &lt;NA&gt;       3145\n   population rawpoll_clinton rawpoll_trump rawpoll_johnson rawpoll_mcmullin\n1          lv           47.00         43.00            4.00               NA\n2          lv           38.03         35.69            5.46               NA\n3          lv           42.00         39.00            6.00               NA\n4          lv           45.00         41.00            5.00               NA\n5          rv           47.00         43.00            3.00               NA\n6          lv           48.00         44.00            3.00               NA\n7          lv           45.00         41.00            5.00               NA\n8          lv           44.00         40.00            6.00               NA\n9          lv           41.20         42.70            7.10               NA\n10         lv           44.00         41.00            4.00               NA\n11         lv           48.00         44.00            6.00               NA\n12         lv           50.00         44.00            4.00               NA\n13         lv           44.00         43.00            6.00               NA\n14         lv           45.00         40.00            5.00               NA\n15         lv           43.61         46.84              NA               NA\n16         lv           47.60         44.40            4.30               NA\n17         lv           48.91         46.13              NA               NA\n18         lv           45.00         42.00            8.00               NA\n19         lv           47.00         41.00            6.00               NA\n20         lv           45.00         43.00            4.00               NA\n21         lv           49.00         45.00            4.00               NA\n22         lv           43.70         34.60            7.70               NA\n23         lv           45.50         44.00            5.00               NA\n24         lv           45.00         42.00            5.00               NA\n25         lv           47.00         43.00            4.00               NA\n26         lv           43.00         39.00            6.00               NA\n27         lv           48.00         43.00            4.00               NA\n28         lv           42.90         39.00            4.70               NA\n29         lv           40.70         43.10            6.30               NA\n30         lv           47.00         43.00            4.00               NA\n31         lv           44.40         43.90            3.70               NA\n32         lv           43.00         44.00            5.00               NA\n33         lv           47.00         44.00            3.00               NA\n34         lv           47.00         45.00            3.00               NA\n35         lv           46.00         46.00            3.00               NA\n36         lv           43.20         38.30            5.90               NA\n37         lv           44.20         43.70            4.80               NA\n38         lv           46.00         43.00            4.00               NA\n39         lv           44.60         43.70            4.20               NA\n40         lv           45.50         37.30            5.40               NA\n41         lv           44.00         44.00            4.00               NA\n42         lv           45.00         40.00            5.00               NA\n43         lv           44.70         37.40            5.30               NA\n44         lv           44.00         44.10            3.90               NA\n45         lv           44.00         44.40            4.10               NA\n46         lv           42.00         45.00            4.00               NA\n47         lv           44.00         37.20            5.90               NA\n48         lv           45.00         40.00            5.00               NA\n49         lv           44.00         44.00            5.00               NA\n50         lv           37.69         35.07            6.18               NA\n51         lv           49.25         45.92              NA               NA\n52         lv           44.00         44.00            5.00               NA\n53         lv           49.06         47.82              NA               NA\n54         lv           45.00         39.00            5.00               NA\n55         lv           43.23         47.98              NA               NA\n56         lv           48.53         48.44              NA               NA\n57         lv           44.00         39.00            6.00               NA\n58         lv           42.63         48.16              NA               NA\n59         lv           48.78         48.22              NA               NA\n60         lv           42.56         47.96              NA               NA\n61         lv           48.81         48.14              NA               NA\n62         lv           44.00         39.00            6.00               NA\n63         rv           46.00         45.00            4.00               NA\n64         lv           43.38         46.89              NA               NA\n65         lv           43.00         40.00            6.00               NA\n66         lv           42.47         47.50              NA               NA\n67         rv           47.00         45.00            3.00               NA\n68         lv           42.36         47.83              NA               NA\n69         lv           42.00         40.00            5.00               NA\n70         lv           43.28         46.90              NA               NA\n   adjpoll_clinton adjpoll_trump adjpoll_johnson adjpoll_mcmullin  mu_hat\n1         45.20163      41.72430        4.626221               NA  0.0400\n2         43.34557      41.21439        5.175792               NA  0.0234\n3         42.02638      38.81620        6.844734               NA  0.0300\n4         45.65676      40.92004        6.069454               NA  0.0400\n5         46.84089      42.33184        3.726098               NA  0.0400\n6         49.02208      43.95631        3.057876               NA  0.0400\n7         45.11649      40.92722        4.341786               NA  0.0400\n8         43.58576      40.77325        5.365788               NA  0.0400\n9         42.92745      42.23545        6.316175               NA -0.0150\n10        44.21714      40.57082        4.068708               NA  0.0300\n11        47.57171      43.68125        5.556625               NA  0.0400\n12        48.86765      43.39600        4.838600               NA  0.0600\n13        42.83406      43.43819        4.780429               NA  0.0100\n14        45.13966      42.26495        3.679914               NA  0.0500\n15        45.32156      43.38579              NA               NA -0.0323\n16        47.63973      43.16080        4.761112               NA  0.0320\n17        47.01806      42.04561              NA               NA  0.0278\n18        46.28310      43.27167        6.904304               NA  0.0300\n19        45.65592      40.37888        3.677361               NA  0.0600\n20        45.56041      43.13745        4.418502               NA  0.0200\n21        49.14547      45.12675        4.253578               NA  0.0400\n22        43.13676      38.24491        5.957800               NA  0.0910\n23        46.70731      44.14856        4.979693               NA  0.0150\n24        45.23948      42.55291        3.928672               NA  0.0300\n25        45.28374      41.76419        4.635295               NA  0.0400\n26        43.12950      38.96855        6.757478               NA  0.0400\n27        46.34620      41.81491        4.620226               NA  0.0500\n28        43.74084      41.38541        4.411561               NA  0.0390\n29        42.51610      42.67482        5.607142               NA -0.0240\n30        45.38686      41.91655        4.548039               NA  0.0400\n31        46.41406      43.79416        2.837214               NA  0.0050\n32        44.89821      43.61472        4.316216               NA -0.0100\n33        45.39959      43.04363        3.456292               NA  0.0300\n34        45.40490      44.19081        3.354964               NA  0.0200\n35        44.40672      45.38988        3.222180               NA  0.0000\n36        43.34223      38.39563        6.565732               NA  0.0490\n37        46.16068      43.36544        4.101147               NA  0.0050\n38        46.94868      43.42593        4.889163               NA  0.0300\n39        46.61572      44.16262        2.969010               NA  0.0090\n40        45.64754      37.54281        5.964403               NA  0.0820\n41        44.66353      44.28981        4.331246               NA  0.0000\n42        45.22830      42.30433        3.770880               NA  0.0500\n43        44.84936      37.84188        5.731620               NA  0.0730\n44        46.01937      44.14133        2.935885               NA -0.0010\n45        46.02119      44.64041        3.003101               NA -0.0040\n46        42.67626      45.41689        4.239500               NA -0.0300\n47        44.14389      37.86409        6.197528               NA  0.0680\n48        45.31041      42.34422        3.779955               NA  0.0500\n49        44.67792      44.98535        4.871296               NA  0.0000\n50        43.11401      41.64819        5.238244               NA  0.0262\n51        47.36662      41.90514              NA               NA  0.0333\n52        44.68157      44.56407        5.138172               NA  0.0000\n53        47.15442      43.89729              NA               NA  0.0124\n54        45.37288      41.39494        3.764886               NA  0.0600\n55        44.98547      44.55796              NA               NA -0.0475\n56        46.59161      44.62636              NA               NA  0.0009\n57        44.41354      41.49658        4.692698               NA  0.0500\n58        44.39404      44.80749              NA               NA -0.0553\n59        46.79931      44.56131              NA               NA  0.0056\n60        44.30183      44.69964              NA               NA -0.0540\n61        46.77929      44.65897              NA               NA  0.0067\n62        44.42627      41.62366        4.600952               NA  0.0500\n63        46.04416      44.79836        4.454841               NA  0.0100\n64        45.08903      43.73871              NA               NA -0.0351\n65        43.43158      42.77084        4.499624               NA  0.0300\n66        44.13673      44.50366              NA               NA -0.0503\n67        47.02612      44.52409        3.647916               NA  0.0200\n68        43.97671      45.01132              NA               NA -0.0547\n69        42.43340      42.96991        3.366840               NA  0.0200\n70        44.84075      44.24033              NA               NA -0.0362\n         p          se        lower         upper\n1  0.52000 0.021206832 -0.001565391  0.0815653910\n2  0.51170 0.006132712  0.011379884  0.0354201164\n3  0.51500 0.021334733 -0.011816077  0.0718160772\n4  0.52000 0.016478037  0.007703048  0.0722969523\n5  0.52000 0.007746199  0.024817449  0.0551825509\n6  0.52000 0.027766261 -0.014421872  0.0944218716\n7  0.52000 0.026460164 -0.011861920  0.0918619205\n8  0.52000 0.027906686 -0.014697106  0.0946971055\n9  0.49250 0.030052273 -0.073902455  0.0439024552\n10 0.51500 0.035361533 -0.039308606  0.0993086056\n11 0.52000 0.029451989 -0.017725898  0.0977258977\n12 0.53000 0.036497747 -0.011535585  0.1315355848\n13 0.50500 0.032614773 -0.053924955  0.0739249547\n14 0.52500 0.019891614  0.011012436  0.0889875640\n15 0.48385 0.018333650 -0.068233954  0.0036339536\n16 0.51600 0.031465305 -0.029671998  0.0936719976\n17 0.51390 0.024797359 -0.020802824  0.0764028240\n18 0.51500 0.025964525 -0.020890468  0.0808904685\n19 0.53000 0.003767618  0.052615469  0.0673845313\n20 0.51000 0.025814724 -0.030596860  0.0705968600\n21 0.52000 0.032590300 -0.023876988  0.1038769882\n22 0.54550 0.020906295  0.050023662  0.1319763377\n23 0.50750 0.030052273 -0.043902455  0.0739024552\n24 0.51500 0.032829526 -0.034345871  0.0943458710\n25 0.52000 0.022703221 -0.004498313  0.0844983131\n26 0.52000 0.021093122 -0.001342519  0.0813425187\n27 0.52500 0.024330806  0.002311620  0.0976883802\n28 0.51950 0.003441726  0.032254217  0.0457457829\n29 0.48800 0.031210535 -0.085172648  0.0371726477\n30 0.52000 0.026525348 -0.011989682  0.0919896819\n31 0.50250 0.033370015 -0.060405230  0.0704052296\n32 0.49500 0.033276252 -0.075221455  0.0552214546\n33 0.51500 0.029462312 -0.027746131  0.0877461306\n34 0.51000 0.029266966 -0.037363252  0.0773632524\n35 0.50000 0.029086486 -0.057009513  0.0570095133\n36 0.52450 0.022217482  0.005453734  0.0925462656\n37 0.50250 0.035266840 -0.064123006  0.0741230063\n38 0.51500 0.028465770 -0.025792910  0.0857929099\n39 0.50450 0.031340688 -0.052427748  0.0704277478\n40 0.54100 0.022733080  0.037443163  0.1265568367\n41 0.50000 0.025819889 -0.050606982  0.0506069824\n42 0.52500 0.019647631  0.011490643  0.0885093572\n43 0.53650 0.023692335  0.026563023  0.1194369771\n44 0.49950 0.033961764 -0.067565057  0.0655650566\n45 0.49800 0.034059863 -0.070757331  0.0627573309\n46 0.48500 0.025808267 -0.080584204  0.0205842041\n47 0.53400 0.026074924  0.016893150  0.1191068502\n48 0.52500 0.019871918  0.011051041  0.0889489586\n49 0.50000 0.025819889 -0.050606982  0.0506069824\n50 0.51310 0.006410691  0.013635046  0.0387649537\n51 0.51665 0.025207676 -0.016107044  0.0827070444\n52 0.50000 0.025819889 -0.050606982  0.0506069824\n53 0.50620 0.026000549 -0.038561076  0.0633610765\n54 0.53000 0.019724776  0.021339440  0.0986605601\n55 0.47625 0.018437645 -0.083637785 -0.0113622153\n56 0.50045 0.026773967 -0.051576975  0.0533769749\n57 0.52500 0.019486167  0.011807113  0.0881928869\n58 0.47235 0.018266049 -0.091101457 -0.0194985430\n59 0.50280 0.027430309 -0.048163406  0.0593634056\n60 0.47300 0.018270409 -0.089810001 -0.0181899990\n61 0.50335 0.026889279 -0.046002987  0.0594029874\n62 0.52500 0.019512181  0.011756126  0.0882438739\n63 0.50500 0.013658276 -0.016770221  0.0367702213\n64 0.48245 0.018362837 -0.071091161  0.0008911606\n65 0.51500 0.019539013 -0.008296465  0.0682964652\n66 0.47485 0.018425700 -0.086414373 -0.0141856272\n67 0.51000 0.020261129 -0.019711813  0.0597118128\n68 0.47265 0.018217943 -0.090407168 -0.0189928318\n69 0.51000 0.019607691 -0.018431074  0.0584310740\n70 0.48190 0.017819886 -0.071126977 -0.0012730227\n\n\n\nMake a plot of the error, the difference between each poll’s estimate and the actual \\(mu=0.021\\) stratified by pollster.\nRedo the previous plot but only for pollsters that took five or more polls."
  },
  {
    "objectID": "19-models.html#class-competition",
    "href": "19-models.html#class-competition",
    "title": "19  Models",
    "section": "19.1 Class competition",
    "text": "19.1 Class competition\n\nlibrary(tidyverse)\nclean &lt;- function(x){\n  ret &lt;- parse_number(str_remove(x,\"%\"))\n  ifelse(ret &gt; 1, ret/100, ret)\n}\nodat &lt;- read_csv(\"~/Downloads/Poll Competition（回复） - 第 1 张表单回复 (1).csv\") |&gt;\n  setNames(c(\"stamp\", \"name\", \"estimate\", \"upper\", \"lower\", \"n\"))  \n\nRows: 31 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): 时间戳记, Name, Prediction, Interval upper bound, Interval lower bound\ndbl (1): Sample size\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndat &lt;- odat |&gt;\n  mutate(id = as.character(1:n())) |&gt;\n  mutate(across(c(estimate, upper, lower), clean)) |&gt;\n  filter(estimate &lt;= 1 & estimate &gt; 0.2)\n\ndat |&gt; ggplot(aes(id, estimate, ymin = lower, ymax = upper)) +\n  geom_errorbar() +\n  geom_point() +\n  geom_hline(yintercept = 0.471, lty = 2) +\n  coord_flip()\n\n\n\ndat |&gt; \n  filter(upper &gt;= 0.471 & lower &lt;= 0.471) |&gt;\n  mutate(size = upper - lower) |&gt;\n  arrange(size)\n\n# A tibble: 12 × 8\n   stamp                   name          estimate upper lower     n id      size\n   &lt;chr&gt;                   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 2023-10-11 上午01:14:54 Li Li            0.48  0.5   0.45    100 23    0.05  \n 2 2023-10-4 上午11:27:18  Laura Chen       0.45  0.48  0.42     30 3     0.06  \n 3 2023-10-4 下午02:20:58  Ellie Zhang      0.450 0.492 0.408   100 5     0.0837\n 4 2023-10-4 下午05:29:15  Rachel Sussm…    0.51  0.555 0.465   200 7     0.0906\n 5 2023-10-10 下午11:30:30 Yiran Fu         0.5   0.55  0.45     50 15    0.1   \n 6 2023-10-10 下午11:44:23 Lindo Nkambu…    0.44  0.524 0.386   200 17    0.138 \n 7 2023-10-10 下午10:29:47 Min              0.5   0.57  0.43     30 10    0.14  \n 8 2023-10-10 下午11:25:45 Alex Mellott     0.48  0.55  0.41     50 13    0.14  \n 9 2023-10-4 下午01:43:59  Camille Shao     0.46  0.55  0.4     300 4     0.15  \n10 2023-10-10 下午11:45:31 Zhimeng Liu      0.48  0.6   0.44     25 18    0.16  \n11 2023-10-11 上午12:56:12 Dailin Luo       0.5   0.6   0.4      10 22    0.2   \n12 2023-10-11 上午07:51:54 Nia Martinez…    0.52  0.71  0.32     15 27    0.39  \n\ndat |&gt; summarize(estimate =sum(estimate*n)/sum(n), n=sum(n))\n\n# A tibble: 1 × 2\n  estimate     n\n     &lt;dbl&gt; &lt;dbl&gt;\n1    0.487  2020"
  },
  {
    "objectID": "19-models.html#data-driven-models",
    "href": "19-models.html#data-driven-models",
    "title": "19  Models",
    "section": "19.2 Data-driven models",
    "text": "19.2 Data-driven models\n\n19.2.1 Poll aggregators\n\nmu &lt;- 0.039\nNs &lt;- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)\np &lt;- (mu + 1) / 2\n\npolls &lt;- map_df(Ns, function(N) {\n  x &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))\n  x_hat &lt;- mean(x)\n  se_hat &lt;- sqrt(x_hat * (1 - x_hat) / N)\n  list(estimate = 2 * x_hat - 1, \n    low = 2*(x_hat - 1.96*se_hat) - 1, \n    high = 2*(x_hat + 1.96*se_hat) - 1,\n    sample_size = N)\n}) |&gt; mutate(poll = seq_along(Ns))\n\nHere is a visualization showing the intervals the pollsters would have reported for the difference between Obama and Romney:\n\n\n\n\n\n\nsum(polls$sample_size)\n\n[1] 11269\n\n\nparticipants. Basically, we construct an estimate of the spread, let’s call it \\(\\mu\\), with a weighted average in the following way:\n\nmu_hat &lt;- polls |&gt; \n  summarize(avg = sum(estimate*sample_size) / sum(sample_size)) |&gt; \n  pull(avg)\n\nIur margin of error is\n\np_hat &lt;- (1 + mu_hat)/2; \nmoe &lt;- 2*1.96*sqrt(p_hat*(1 - p_hat)/sum(polls$sample_size))\nmoe\n\n[1] 0.01845451\n\n\nThus, we can predict that the spread will be 3.1 plus or minus 1.8, which not only includes the actual result we eventually observed on election night, but is quite far from including 0. Once we combine the 12 polls, we become quite certain that Obama will win the popular vote.\n\n\n\n\n\nHowever, this was just a simulation to illustrate the idea.\n\nlibrary(dslabs)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(state == \"U.S.\" & enddate &gt;= \"2016-10-31\" &\n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade)))\n\nWe add a spread estimate:\n\npolls &lt;- polls |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nWe have 49 estimates of the spread.\nThe expected value is the election night spread \\(\\mu\\) and the standard error is \\(2\\sqrt{p (1 - p) / N}\\). Assuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread is:\n\nmu_hat &lt;- polls |&gt; \n  summarize(mu_hat = sum(spread * samplesize) / sum(samplesize)) |&gt; \n  pull(mu_hat)\n\nand the standard error is:\n\np_hat &lt;- (mu_hat + 1)/2 \nmoe &lt;- 1.96 * 2 * sqrt(p_hat * (1 - p_hat) / sum(polls$samplesize))\nmoe\n\n[1] 0.006623178\n\n\nSo we report a spread of 1.43% with a margin of error of 0.66%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened?\nA histogram of the reported spreads shows a problem:\n\npolls |&gt; ggplot(aes(spread)) + geom_histogram(color = \"black\", binwidth = .01)\n\n\n\n\nThe data does not appear to be normally distributed and the standard error appears to be larger than 0.0066232. The theory is not working here and in the next section we describe a useful data-driven model.\n\n\n19.2.2 Beyond the simple sampling model\nNotice that data come various pollsters and some are taking several polls a week:\n\npolls |&gt; group_by(pollster) |&gt; summarize(n())\n\n# A tibble: 15 × 2\n   pollster                                                   `n()`\n   &lt;fct&gt;                                                      &lt;int&gt;\n 1 ABC News/Washington Post                                       7\n 2 Angus Reid Global                                              1\n 3 CBS News/New York Times                                        2\n 4 Fox News/Anderson Robbins Research/Shaw & Company Research     2\n 5 IBD/TIPP                                                       8\n 6 Insights West                                                  1\n 7 Ipsos                                                          6\n 8 Marist College                                                 1\n 9 Monmouth University                                            1\n10 Morning Consult                                                1\n11 NBC News/Wall Street Journal                                   1\n12 RKM Research and Communications, Inc.                          1\n13 Selzer & Company                                               1\n14 The Times-Picayune/Lucid                                       8\n15 USC Dornsife/LA Times                                          8\n\n\nLet’s visualize the data for the pollsters that are regularly polling:\n\n\n\n\n\nThis plot reveals an unexpected result. First, consider that the standard error predicted by theory for each poll is between 0.018 and 0.033:\n\npolls |&gt; group_by(pollster) |&gt; \n  filter(n() &gt;= 6) |&gt;\n  summarize(se = 2 * sqrt(p_hat * (1 - p_hat) / median(samplesize)))\n\n# A tibble: 5 × 2\n  pollster                     se\n  &lt;fct&gt;                     &lt;dbl&gt;\n1 ABC News/Washington Post 0.0265\n2 IBD/TIPP                 0.0333\n3 Ipsos                    0.0225\n4 The Times-Picayune/Lucid 0.0196\n5 USC Dornsife/LA Times    0.0183\n\n\nThis agrees with the within poll variation we see. However, there appears to be differences across the polls.\n\none_poll_per_pollster &lt;- polls |&gt; group_by(pollster) |&gt; \n  filter(enddate == max(enddate)) |&gt;\n  ungroup()\n\nHere is a histogram of the data for these 15 pollsters:\n\nqplot(spread, data = one_poll_per_pollster, binwidth = 0.01)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nAlthough we are no longer using a model with red (Republicans) and blue (Democrat) beads in an urn, our new model can also be thought of as an urn mode but containing poll results from all possible pollsters and think of our $N=$15 data points\n\\[X_1,\\dots X_N\\]\na as a random sample from this urn. To develop a useful model, we assume that the expected value of our urn is the actual spread \\(\\mu=2p-1\\), which implies that the sample average has expected value \\(\\mu\\).\nNow, because instead of 0s and 1s, our urn contains continuous numbers, the standard deviation of the urn is no longer\n\\[\\sqrt{p(1-p)}\\].\nSo our new statistical model is that\n\\[\nX_1, \\dots, X_N, \\text{E}(X) = \\mu \\text{ and }  \\text{SD}(X) = \\sigma\n\\].\nThe distribution, for now, is unspecified. But we consider \\(N\\) to be large enough to assume that the sample average\n\\[\n\\bar{X} = \\sum_{i=1}^N X_i\n\\]\nwith\n\\[\n\\mbox{E}(\\bar{X}) = \\mu\n\\]\nand\n\\[\n\\mbox{SE}(\\bar{X}) = \\sigma / \\sqrt{N}\n\\]\nand\n\\[\n\\bar{X} \\sim \\mbox{N}(\\mu, \\sigma / \\sqrt{N})\n\\]\n\n\n19.2.3 Estimating the standard deviation\nT \\[\ns = \\sqrt{ \\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2 }\n\\]\nThe sd function in R computes the sample standard deviation:\n\nsd(one_poll_per_pollster$spread)\n\n[1] 0.02419369\n\n\n\n\n19.2.4 Computing a confidence interval\nWe are now ready to form a new confidence interval based on our new data-driven model:\n\nresults &lt;- one_poll_per_pollster |&gt; \n  summarize(avg = mean(spread), \n            se = sd(spread) / sqrt(length(spread))) |&gt; \n  mutate(start = avg - 1.96 * se, \n         end = avg + 1.96 * se) \nround(results * 100, 1)\n\n  avg  se start end\n1 2.9 0.6   1.7 4.1\n\n\nOur confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the popular vote.\n\n\n19.2.5 The t-distribution\nThe statistic on which confidence intervals for \\(\\mu\\) are based is\n\\[\nZ = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{N}}\n\\]\nCLT tells us that Z is approximately normally distributed with expected value 0 and standard error 1. But in practice we don’t know \\(\\sigma\\) so we use:\n\\[\nt = \\frac{\\bar{X} - \\mu}{s/\\sqrt{N}}\n\\]\nThis is referred to a t-statistic. By substituting \\(\\sigma\\) with \\(s\\) we introduce some variability. The theory tells us that \\(t\\) follows a student t-distribution with \\(N-1\\) degrees of freedom. The degrees of freedom is a parameter that controls the variability via fatter tails:\n\n\n\n\n\nIf we are willing to assume the pollster effect data is normally distributed, based on the sample data \\(X_1, \\dots, X_N\\),\n\none_poll_per_pollster |&gt;\n  ggplot(aes(sample = scale(spread))) + stat_qq() +\n  geom_abline()\n\n\n\n\nthen \\(t\\) follows a t-distribution with \\(N-1\\) degrees of freedom. So perhaps a better confidence interval for \\(\\mu\\) is:\n\nz &lt;- qt(0.975,  nrow(one_poll_per_pollster) - 1)\none_poll_per_pollster |&gt; \n  summarize(avg = mean(spread), moe = z*sd(spread)/sqrt(length(spread))) |&gt; \n  mutate(start = avg - moe, end = avg + moe) \n\n# A tibble: 1 × 4\n     avg    moe  start    end\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 0.0290 0.0134 0.0156 0.0424\n\n\nA bit larger than the one using normal is\n\nqt(0.975, 14)\n\n[1] 2.144787\n\n\nis bigger than\n\nqnorm(0.975)\n\n[1] 1.959964\n\n\nThis results in slightly larger confidence interval than we obtained before:\n\n\n  start end\n1   1.6 4.2\n\n\nNote that using the t-distribution and the t-statistic is the basis for t-tests, widely used approach for computing p-values. To learn more about t-tests, you can consult any statistics textbook.\nThe t-distribution can also be used to model errors in bigger deviations that are more likely than with the normal distribution, as seen in the densities we previously saw.\n\npolls_us_election_2016 |&gt;\n  filter(state == \"Wisconsin\" &\n           enddate &gt;= \"2016-10-31\" & \n           (grade %in% c(\"A+\", \"A\", \"A-\", \"B+\") | is.na(grade))) |&gt;\n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) |&gt;\n  mutate(state = as.character(state)) |&gt;\n  left_join(results_us_election_2016, by = \"state\") |&gt;\n  mutate(actual = clinton/100 - trump/100) |&gt;\n  summarize(actual = first(actual), avg = mean(spread), \n            sd = sd(spread), n = n()) |&gt;\n  select(actual, avg, sd, n)\n\n  actual        avg         sd n\n1 -0.007 0.07106667 0.01041454 6"
  },
  {
    "objectID": "19-models.html#bayesian-models",
    "href": "19-models.html#bayesian-models",
    "title": "19  Models",
    "section": "19.3 Bayesian models",
    "text": "19.3 Bayesian models\nIn 2016 FiveThirtyEight showed this chart depicting distributions for the percent of the popular vote for each candidate:\n\n\n\n\n\nThe colored areas represent values with an 80% chance of including the actual result, according to the FiveThirtyEight model.\n\n\n\n\nBut what does this mean in the context of the theory we have covered in which these percentages are considered fixed.\n\n19.3.1 Priors, posteriors and and credible intervals\nIn the previous chapter we an estimate and margin of error for the difference in popular votes between Hillary Clinton and Donald Trump, which we denoted with \\(\\mu\\). The estimate was between 2 and 3 percent and the confidence interval did not include 0. A forecaster would use this to predict Hillary Clinton would win the popular vote. But to make a probabilistic statement about winning the election, we need to use a Bayesian.\nWe start the Bayesian approach by quantifying our knowledge before seeing any data. This is done using a probability distribution refereed to as a prior. For our example we could write:\n\\[\n\\mu \\sim N(\\theta, \\tau)\n\\]\nWe can think of \\(\\theta\\) as our best guess for the popular vote difference had we not seen any polling data and we can think of \\(\\tau\\) as quantifying how certain we feel about this guess. Generally, if we have expert knowledge related to \\(\\mu\\), we can try to quantify it with the prior disribution. In the case of election polls, experts use fundamentals, which include, for example, the state of the economy, to develop prior distributions. The data is used to update our initial guess or prior belief. This can be done mathematically if we define the distribution for the observed data, for any given \\(\\mu\\). In our particular example we would write down a model the average of our polls, which is the same as before:\n\\[\n\\bar{X} \\mid \\mu \\sim N(\\mu, \\sigma/\\sqrt{N})\n\\]\nAs before, \\(\\sigma\\) describes randomness due to sampling and the pollster effects. In the Bayesian contexts, this is referred to as the sampling distribution. Note that we write the conditional \\(\\bar{X} \\mid \\mu\\) becuase \\(\\mu\\) is now considered a random variable.\nWe do not show the derivations here, but we can now use Calculus and a version fo Bayes Theorem foto derive the distribution of \\(\\mu\\) conditional of the data, refered to as the posterior distribution. Specifcially we can show the \\(\\mu \\mid \\bar{X}\\) follows a normal distribution with expected value:\n\\[\n\\begin{aligned}\n\\mbox{E}(\\mu \\mid \\bar{X}) &= B \\theta + (1-B) \\bar{X}\\\\\n&= \\theta + (1-B)(\\bar{X}-\\theta)\\\\\n\\mbox{with } B &= \\frac{\\sigma^2/N}{\\sigma^2/N+\\tau^2}\n\\end{aligned}\n\\] and standard error :\n\\[\n\\mbox{SE}(\\mu \\mid \\bar{X})^2 = \\frac{1}{1/\\sigma^2+1/\\tau^2}.\n\\]\nNote that the expected value is a weighted average of our prior guess \\(\\theta\\) and the observed data \\(\\bar{X}\\). The weight depends on how certain we are about our prior belief, quantified by \\(\\tau\\), and the precision \\(\\sigma/N\\) of the summary of our observed data. This weighted average is sometimes referred to as shrinking because it shrinks estimates towards a prior value.\nThese quantities useful for updating our beliefs. Specifically, we use the posterior distribution not only to compute the expected value of \\(\\mu\\) given the observed data, but for any probability \\(\\alpha\\) we can construct intervals, centered at our estimate and with \\(\\alpha\\) chance of ocurring.\nTo compute a posterior distribution and construct a credible interval, we define a prior distribution with mean 0% and standard error 3.5% which can be interpreted as: before seing polling data, we don’t think any candidate has the advantage and a difference of up to 7% either way is possible. We compute the posterior distribution using the equations above:\n\ntheta &lt;- 0\ntau &lt;- 0.035\nsigma &lt;- results$se\nx_bar &lt;- results$avg\nB &lt;- sigma^2 / (sigma^2 + tau^2)\n\nposterior_mean &lt;- B*theta + (1 - B)*x_bar\nposterior_se &lt;- sqrt(1/(1/sigma^2 + 1/tau^2))\n\nposterior_mean\n\n[1] 0.02808534\n\nposterior_se\n\n[1] 0.006149604\n\n\nBecause we know the posterior distribution in normal, we can consturct a credible interval like this:\n\nposterior_mean + c(-1, 1) * qnorm(0.975) * posterior_se\n\n[1] 0.01603234 0.04013834\n\n\nFurthermore, we can now make the probabilitic statement we could not make with the frequentists approach by computing the posterior probability of Hillary winning the popular vote. Specifically, \\(\\mbox{Pr}(\\mu&gt;0 \\mid \\bar{X})\\) can be computed like this:\n\n1 - pnorm(0, posterior_mean, posterior_se)\n\n[1] 0.9999975\n\n\nThis says we are 100% sure Clinton will win the popular vote, which seems too overconfident. Also, it is not in agreement with FiveThirtyEight’s 81.4%. What explains this difference? There is a level of uncertainty that we are not yet describing, and we will get back to that later."
  },
  {
    "objectID": "19-models.html#hierarchichal-models",
    "href": "19-models.html#hierarchichal-models",
    "title": "19  Models",
    "section": "19.4 Hierarchichal Models",
    "text": "19.4 Hierarchichal Models\nHierarchical models are useful for quantifying different levels of variability or uncertainty. One can use them using a Bayesian or a Frequentist framework. However because in the Frequentist framework they often extend a model with a fixed parameter by assuming the parameter is actually random, the model description includes two distribution that look like the prior and a sampling distribution used in the Bayesian framework, making the resulting summaries very similar or even equal to what is obtained with a Bayesian context. A key difference between the Bayesian and the Frequentist hierarchical model approach is that in the latter we use data to construct priors rather than treat priors as a quantification of prior expert knowledge. Here illustrate the use of hiereachical models with an example from sports, in which dedicated fans, intuitively apply the ideas of hierarchical models to manage expectations when a new player is of to an exceptionally good start.\n\n19.4.1 Case study: election forecasting\nSince the 2008 elections, organizations other than FiveThirtyEight have started their own election forecasting groups that also aggregate polling data and uses statistical models to make predictions. However, in 2016 forecasters underestimated Trump’s chances of winning greatly. The day before the election the New York Times reported the following probabilities for Hillary Clinton winning the presidency:\n\n\n\n\n\n\nNYT\n538\nHuffPost\nPW\nPEC\nDK\nCook\nRoth\n\n\n\n\nWin Prob\n85%\n71%\n98%\n89%\n&gt;99%\n92%\nLean Dem\nLean Dem\n\n\n\n\n\n\n\nFor example, the Princeton Election Consortium (PEC) gave Trump less than 1% chance of winning, while the Huffington Post gave him a 2% chance. In contrast, FiveThirtyEight had Trump’s probability of winning at 29%, substantially higher than the others. In fact, four days before the election FiveThirtyEight published an article titled Trump Is Just A Normal Polling Error Behind Clinton\nSo why did FiveThirtyEight’s model fair so much better than others? How could PEC and Huffington Post get it so wrong if they were using the same data? In this chapter we describe how FiveThirtyEight used a hierarchical model to correctly account for key sources of variability and outperform all other forecasters. For illustrative purposes we will cotinue examining our popular vote example. In the final section we then describe the more complex approach used to forecast the electoral college result.\n\n\n19.4.2 The general bias\nIn the previous chapter we computed the posterior probability of Hillary Clinton winning the popular vote with a standard Bayesian analysis and found it to be very close to 100%. However, FiveThirtyEight gave her a 81.4% chance[^models-4]. What explains this difference? Below we describe the general bias, another source of variability, included in the FiveThirtyEight model, that accounts for the difference.\nAfter elections are over, one can look at the difference between pollster predictions and actual result. An important observation that our initial models did not take into account is that it is common to see a general bias that affects most pollsters in the same way making the observed data correlated. There is no agreed upon explanation for this, but we do observe it in historical data: in one election, the average of polls favors Democrats by 2%, then in the following election they favor Republicans by 1%, then in the next election there is no bias, then in the following one Republicans are favored by 3%, and so on. In 2016, the polls were biased in favor of the Democrats by 1-2%.\nHowever, although we know this bias term affects our polls, we have no way of knowing what this bias is until election night. So we can’t correct our polls accordingly. What we can do is include a term in our model that accounts for the variability.\n\n\n19.4.3 Mathematical representations of the hierarchical model\nSuppose we are collecting data from one pollster and we assume there is no general bias. The pollster collects several polls with a sample size of \\(N\\), so we observe several measurements of the spread \\(X_1, \\dots, X_J\\). Suppose the real proportion for Hillary is \\(p\\) and the difference is \\(\\mu\\). The urn model theory tells us that these random variables are normally distributed with expected value \\(\\mu\\) and standard error \\(2 \\sqrt{p(1-p)/N}\\):\n\\[\nX_j \\sim \\mbox{N}\\left(\\mu, \\sqrt{p(1-p)/N}\\right)\n\\]\nWe use the index \\(j\\) to represent the different polls conducted by this pollster. Here is a simulation for six polls assuming the spread is 2.1 and \\(N\\) is 2,000:\n\nset.seed(3)\nJ &lt;- 6\nN &lt;- 2000\nmu &lt;- .021\np &lt;- (mu + 1)/2\nX &lt;- rnorm(J, mu, 2 * sqrt(p * (1 - p) / N))\n\nNow suppose we have \\(J=6\\) polls from each of \\(I=5\\) different pollsters. For simplicity, let’s say all polls had the same sample size \\(N\\). The urn model tell us the distribution is the same for all pollsters so to simulate data, we use the same model for each:\n\nI &lt;- 5\nJ &lt;- 6\nN &lt;- 2000\nX &lt;- sapply(1:I, function(i){\n  rnorm(J, mu, 2 * sqrt(p * (1 - p) / N))\n})\n\nAs expected, the simulated data does not really seem to capture the features of the actual data because it does not account for pollster-to-pollster variability:\n\n\n\n\n\nTo fix this, we need to represent the two levels of variability and we need two indexes, one for pollster and one for the polls each pollster takes. We use \\(X_{ij}\\) with \\(i\\) representing the pollster and \\(j\\) representing the \\(j\\)-th poll from that pollster. The model is now augmented to include pollster effects \\(h_i\\), referred to as house effects by FiveThirtyEight, with standard deviation \\(\\sigma_h\\):\n\\[\n\\begin{aligned}\nh_i &\\sim \\mbox{N}\\left(0, \\sigma_h\\right)\\\\\nX_{i,j} \\mid h_i &\\sim \\mbox{N}\\left(\\mu + h_i, \\sqrt{p(1-p)/N}\\right)\n\\end{aligned}\n\\]\nTo simulate data from a specific pollster, we first need to draw an \\(h_i\\) and the generate individual poll data after adding this effect. Here is how we would do it for one specific pollster. We assume \\(\\sigma_h\\) is 0.025:\n\nI &lt;- 5\nJ &lt;- 6\nN &lt;- 2000\nmu &lt;- .021\np &lt;- (mu + 1) / 2\nh &lt;- rnorm(I, 0, 0.025)\nX &lt;- sapply(1:I, function(i){\n  mu + h[i] + rnorm(J, 0, 2 * sqrt(p * (1 - p) / N))\n})\n\nThe simulated data now looks more like the actual data:\n\n\n\n\n\nNote that \\(h_i\\) is common to all the observed spreads from a specific pollster. Different pollsters have a different \\(h_i\\), which explains why we can see the groups of points shift up and down from pollster to pollster.\nNow, in the model above, we assume the average house effect is 0. We think that for every pollster biased in favor of our party, there is another one in favor of the other and assume the standard deviation is \\(\\sigma_h\\). But historically we see that every election has a general bias affecting all polls. We can observe this with the 2016 data, but if we collect historical data, we see that the average of polls misses by more than models like the one above predict. To see this, we would take the average of polls for each election year and compare it to the actual value. If we did this, we would see a difference with a standard deviation of between 2-3%. To incorporate this into the model, we can add another level account for this variability:\n\\[\n\\begin{aligned}\nb &\\sim \\mbox{N}\\left(0, \\sigma_b\\right)\\\\\nh_j \\mid \\, b &\\sim \\mbox{N}\\left(b, \\sigma_h\\right)\\\\\nX_{i,j} | \\, h_j, b &\\sim \\mbox{N}\\left(\\mu + h_j, \\sqrt{p(1-p)/N}\\right)\n\\end{aligned}\n\\]\nThis model accounts for three levels of variability: 1) variability in the bias observed from election to election, quantified by \\(\\sigma_b\\), 2) pollster-to-pollster or house effect variability, quantified by \\(\\sigma_h\\), and 3) poll sampling variability, which we can derive to be \\(\\sqrt(p(1-p)/N)\\).\nNote that not including a term like \\(b\\) in the models, is what led many forecasters to be overconfident. This random variable changes from election to election, but for any given election, it is the same for all pollsters and polls within on election (note it does not have an index). This implies we can’t estimate \\(\\sigma_h\\) with data from just one election. It also implies that the random variables \\(X_{i,j}\\) for a fixed election year share the same \\(b\\) and are therefore correlated.\nOne way to interpret \\(b\\) is as the difference between the average of all polls from all pollsters and the actual result of the election. Because we don’t know the actual result until after the election, we can’t estimate \\(b\\) until after the election.\n\n\n19.4.4 Computing a posterior probability\nNow let’s fit the model above to data. We will use the same data used in the previous chapters and saved in one_poll_per_pollster.\nHere we have just one poll per pollster so we will drop the \\(j\\) index and represent the data as before with \\(X_1, \\dots, X_I\\). As a reminder we have data from \\(I=15\\) pollsters. Based on the model assumptions described above, we can mathematically show that the average \\(\\bar{X}\\)\n\nx_bar &lt;- mean(one_poll_per_pollster$spread)\n\nhas expected value \\(\\mu\\), thus it provides an unbiased estimate of the outcome of interest. However, how precise is this estimate? Can we use the observed stample standard deviation to construct an estimate of the standard error of \\(\\bar{X}\\)?\nIt turns out that, because the \\(X_i\\) are correlated, estimating the standard error is more complex than what we have described up to now. Specifically, using advanced statistical calculations not shown here, we can show that the typical variance (standard error squared) estimate\n\ns2 &lt;- with(one_poll_per_pollster, sd(spread)^2 / length(spread))\n\nwill consistently underestimate the true standard error by about \\(\\sigma_b^2\\). And, as mentioned earlier, to estimate \\(\\sigma_b\\), we need data from several elections. By collecting and analyzing polling data from several elections, FiveThirtyEight estimates this variability and find that \\(\\sigma_b \\approx 0.025\\). We can therefore greatly improve our standard error estimate by adding this quantity:\n\nsigma_b &lt;- 0.025\nse &lt;- sqrt(s2 + sigma_b^2)\n\nIf we redo the Bayesian calculation taking this variability into account, we get a result much closer to FiveThirtyEight’s:\n\nmu &lt;- 0\ntau &lt;- 0.035\nB &lt;- se^2 / (se^2 + tau^2)\nposterior_mean &lt;- B*mu + (1-B)*x_bar\nposterior_se &lt;- sqrt( 1/ (1/se^2 + 1/tau^2))\n\n1 - pnorm(0, posterior_mean, posterior_se)\n\n[1] 0.8174373\n\n\nNote that by accounting for the general bias term, our Bayesian analysis now produces a posterior probability similar to that reported by FiveThirtyEight.\n\n\n19.4.5 Predicting the electoral college\nUp to now we have focused on the popular vote. But in the United States, elections are not decided by the popular vote but rather by what is known as the electoral college. Each state gets a number of electoral votes that depends, in a somewhat complex way, on the population size of the state. Here are the top 5 states ranked by electoral votes in 2016.\n\nresults_us_election_2016 |&gt; top_n(5, electoral_votes)\n\n         state electoral_votes clinton trump others\n1   California              55    61.7  31.6    6.7\n2        Texas              38    43.2  52.2    4.5\n3      Florida              29    47.8  49.0    3.2\n4     New York              29    59.0  36.5    4.5\n5     Illinois              20    55.8  38.8    5.4\n6 Pennsylvania              20    47.9  48.6    3.6\n\n\nWith some minor exceptions we don’t discuss, the electoral votes are won all or nothing. For example, if you won California in 2016 by just 1 vote, you still get all 55 of its electoral votes. This means that by winning a few big states by a large margin, but losing many small states by small margins, you can win the popular vote and yet lose the electoral college. This happened in 1876, 1888, 2000, and 2016. The idea behind this is to avoid a few large states having the power to dominate the presidential election.\n\nMany people in the US consider the electoral college unfair and would like to see it abolished in favor of the popular vote.\n\nWe are now ready to predict the electoral college result for 2016. We start by aggregating results from a poll taken during the last week before the election. We use the grepl, which finds strings in character vectors, to remove polls that are not for entire states.\n\nresults &lt;- polls_us_election_2016 |&gt;\n  filter(state!=\"U.S.\" & \n           !grepl(\"CD\", state) & \n           enddate &gt;=\"2016-10-31\" & \n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade))) |&gt;\n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) |&gt;\n  group_by(state) |&gt;\n  summarize(avg = mean(spread), sd = sd(spread), n = n()) |&gt;\n  mutate(state = as.character(state))\n\nHere are the five closest races according to the polls:\n\nresults |&gt; arrange(abs(avg))\n\n# A tibble: 47 × 4\n   state               avg     sd     n\n   &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n 1 Florida         0.00356 0.0163     7\n 2 North Carolina -0.0073  0.0306     9\n 3 Ohio           -0.0104  0.0252     6\n 4 Nevada          0.0169  0.0441     7\n 5 Iowa           -0.0197  0.0437     3\n 6 Michigan        0.0209  0.0203     6\n 7 Arizona        -0.0326  0.0270     9\n 8 Pennsylvania    0.0353  0.0116     9\n 9 New Mexico      0.0389  0.0226     6\n10 Georgia        -0.0448  0.0238     4\n# ℹ 37 more rows\n\n\nWe now introduce the command left_join that will let us easily add the number of electoral votes for each state from the dataset us_electoral_votes_2016. Here, we simply say that the function combines the two datasets so that the information from the second argument is added to the information in the first:\n\nresults &lt;- left_join(results, results_us_election_2016, by = \"state\")\n\nNotice that some states have no polls because the winner is pretty much known:\n\nresults_us_election_2016 |&gt; filter(!state %in% results$state) |&gt; \n  pull(state)\n\n[1] \"Rhode Island\"         \"Alaska\"               \"Wyoming\"             \n[4] \"District of Columbia\"\n\n\nNo polls were conducted in DC, Rhode Island, Alaska, and Wyoming because Democrats are sure to win in the first two and Republicans in the last two.\nBecause we can’t estimate the standard deviation for states with just one poll, we will estimate it as the median of the standard deviations estimated for states with more than one poll:\n\nresults &lt;- results |&gt;\n  mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))\n\nTo make probabilistic arguments, we will use a Monte Carlo simulation. For each state, we apply the Bayesian approach to generate an election day \\(\\mu\\). We could construct the priors for each state based on recent history. However, to keep it simple, we assign a prior to each state that assumes we know nothing about what will happen. Since from election year to election year the results from a specific state don’t change that much, we will assign a standard deviation of 2% or \\(\\tau=0.02\\). For now, we will assume, incorrectly, that the poll results from each state are independent. The code for the Bayesian calculation under these assumptions looks like this:\n\nmu &lt;- 0\ntau &lt;- 0.02\nresults |&gt; mutate(sigma = sd/sqrt(n), \n                   B = sigma^2 / (sigma^2 + tau^2),\n                   posterior_mean = B * mu + (1 - B) * avg,\n                   posterior_se = sqrt(1/ (1/sigma^2 + 1/tau^2)))\n\n# A tibble: 47 × 12\n   state          avg      sd     n electoral_votes clinton trump others   sigma\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;           &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 Alabama   -0.149   2.53e-2     3               9    34.4  62.1    3.6 1.46e-2\n 2 Arizona   -0.0326  2.70e-2     9              11    45.1  48.7    6.2 8.98e-3\n 3 Arkansas  -0.151   9.90e-4     2               6    33.7  60.6    5.8 7.00e-4\n 4 Californ…  0.260   3.87e-2     5              55    61.7  31.6    6.7 1.73e-2\n 5 Colorado   0.0452  2.95e-2     7               9    48.2  43.3    8.6 1.11e-2\n 6 Connecti…  0.0780  2.11e-2     3               7    54.6  40.9    4.5 1.22e-2\n 7 Delaware   0.132   3.35e-2     2               3    53.4  41.9    4.7 2.37e-2\n 8 Florida    0.00356 1.63e-2     7              29    47.8  49      3.2 6.18e-3\n 9 Georgia   -0.0448  2.38e-2     4              16    45.9  51      3.1 1.19e-2\n10 Hawaii     0.186   2.10e-2     1               4    62.2  30      7.7 2.10e-2\n# ℹ 37 more rows\n# ℹ 3 more variables: B &lt;dbl&gt;, posterior_mean &lt;dbl&gt;, posterior_se &lt;dbl&gt;\n\n\nThe estimates based on posterior do move the estimates towards 0, although the states with many polls are influenced less. This is expected as the more poll data we collect, the more we trust those results:\n\n\n\n\n\nNow we repeat this 10,000 times and generate an outcome from the posterior. In each iteration, we keep track of the total number of electoral votes for Clinton. Remember that Trump gets 270 minus the votes for Clinton. Also note that the reason we add 7 in the code is to account for Rhode Island and D.C.:\n\nB &lt;- 10000\nmu &lt;- 0\ntau &lt;- 0.02\nclinton_EV &lt;- replicate(B, {\n  results |&gt; mutate(sigma = sd/sqrt(n), \n                   B = sigma^2 / (sigma^2 + tau^2),\n                   posterior_mean = B * mu + (1 - B) * avg,\n                   posterior_se = sqrt(1 / (1/sigma^2 + 1/tau^2)),\n                   result = rnorm(length(posterior_mean), \n                                  posterior_mean, posterior_se),\n                   clinton = ifelse(result &gt; 0, electoral_votes, 0)) |&gt; \n    summarize(clinton = sum(clinton)) |&gt; \n    pull(clinton) + 7\n})\n\nmean(clinton_EV &gt; 269)\n\n[1] 0.998\n\n\nThis model gives Clinton over 99% chance of winning. A similar prediction was made by the Princeton Election Consortium. We now know it was quite off. What happened?\nThe model above ignores the general bias and assumes the results from different states are independent. After the election, we realized that the general bias in 2016 was not that big: it was between 1 and 2%. But because the election was close in several big states and these states had a large number of polls, pollsters that ignored the general bias greatly underestimated the standard error. Using the notation we introduce, they assumed the standard error was \\(\\sqrt{\\sigma^2/N}\\) which with large N is quite smaller than the more accurate estimate \\(\\sqrt{\\sigma^2/N + \\sigma_b^2}\\). FiveThirtyEight, which models the general bias in a rather sophisticated way, reported a closer result. We can simulate the results now with a bias term. For the state level, the general bias can be larger so we set it at \\(\\sigma_b = 0.03\\):\n\ntau &lt;- 0.02\nbias_sd &lt;- 0.03\nclinton_EV_2 &lt;- replicate(1000, {\n  results |&gt; mutate(sigma = sqrt(sd^2/n  + bias_sd^2),  \n                   B = sigma^2 / (sigma^2 + tau^2),\n                   posterior_mean = B*mu + (1-B)*avg,\n                   posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2)),\n                   result = rnorm(length(posterior_mean), \n                                  posterior_mean, posterior_se),\n                   clinton = ifelse(result&gt;0, electoral_votes, 0)) |&gt; \n    summarize(clinton = sum(clinton) + 7) |&gt; \n    pull(clinton)\n})\nmean(clinton_EV_2 &gt; 269)\n\n[1] 0.848\n\n\nThis gives us a much more sensible estimate. Looking at the outcomes of the simulation, we see how the bias term adds variability to the final results.\n\n\n\n\n\nFiveThirtyEight includes many other features we do not include here. One is that they model variability with distributions that have high probabilities for extreme events compared to the normal. One way we could do this is by changing the distribution used in the simulation from a normal distribution to a t-distribution. FiveThirtyEight predicted a probability of 71%."
  },
  {
    "objectID": "19-models.html#forecasting",
    "href": "19-models.html#forecasting",
    "title": "19  Models",
    "section": "19.5 Forecasting",
    "text": "19.5 Forecasting\nForecasters like to make predictions well before the election. The predictions are adapted as new polls come out. However, an important question forecasters must ask is: how informative are polls taken several weeks before the election about the actual election? Here we study the variability of poll results across time.\nTo make sure the variability we observe is not due to pollster effects, let’s study data from one pollster:\n\none_pollster &lt;- polls_us_election_2016 |&gt; \n  filter(pollster == \"Ipsos\" & state == \"U.S.\") |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nSince there is no pollster effect, then perhaps the theoretical standard error matches the data-derived standard deviation. We compute both here:\n\nse &lt;- one_pollster |&gt; \n  summarize(empirical = sd(spread), \n            theoretical = 2 * sqrt(mean(spread) * (1 - mean(spread)) /\n                                     min(samplesize)))\nse\n\n   empirical theoretical\n1 0.04025194  0.03256719\n\n\nBut the empirical standard deviation is higher than the highest possible theoretical estimate. Furthermore, the spread data does not look normal as the theory would predict:\n\n\n\n\n\nThe models we have described include pollster-to-pollster variability and sampling error. But this plot is for one pollster and the variability we see is certainly not explained by sampling error. Where is the extra variability coming from? The following plots make a strong case that it comes from time fluctuations not accounted for by the theory that assumes \\(p\\) is fixed:\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSome of the peaks and valleys we see coincide with events such as the party conventions, which tend to give the candidate a boost. We can see the peaks and valleys are consistent across several pollsters:\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis implies that, if we are going to forecast, our model must include a term to accounts for the time effect. We need to write a model including a bias term for time, denote it \\(b_t\\). The standard deviation of \\(b_t\\) would depend on \\(t\\) since the closer we get to election day, the closer to 0 this bias term should be.\nPollsters also try to estimate trends from these data and incorporate these into their predictions. We can model the time trend \\(b_t\\) with a smooth function. We usually see the trend estimte not for the difference, but for the actual percentages for each candidate like this:\n\n\n\n\n\nOnce a model like the one above is selected, we can use historical and present data to estimate all the necessary parameters to make predictions. There is a variety of methods for estimating trends which we discuss in the Machine Learning part."
  },
  {
    "objectID": "19-models.html#exercises",
    "href": "19-models.html#exercises",
    "title": "19  Models",
    "section": "19.6 Exercises",
    "text": "19.6 Exercises\nWe have been using urn models to motivate the use of probability models. Most data science applications are not related to data obtained from urns. More common are data that come from individuals. The reason probability plays a role here is because the data come from a random sample. The random sample is taken from a population and the urn serves as an analogy for the population.\nLet’s revisit the heights dataset. Suppose we consider the males in our course the population.\n\nlibrary(dslabs)\nx &lt;- heights |&gt; filter(sex == \"Male\") |&gt;\n  pull(height)\n\n\nMathematically speaking, x is our population. Using the urn analogy, we have an urn with the values of x in it. What are the average and standard deviation of our population?\nCall the population average computed above \\(\\mu\\) and the standard deviation \\(\\sigma\\). Now take a sample of size 50, with replacement, and construct an estimate for \\(\\mu\\) and \\(\\sigma\\).\nWhat does the theory tell us about the sample average \\(\\bar{X}\\) and how it is related to \\(\\mu\\)?\n\n\nIt is practically identical to \\(\\mu\\).\nIt is a random variable with expected value \\(\\mu\\) and standard error \\(\\sigma/\\sqrt{N}\\).\nIt is a random variable with expected value \\(\\mu\\) and standard error \\(\\sigma\\).\nContains no information.\n\n\nSo how is this useful? We are going to use an oversimplified yet illustrative example. Suppose we want to know the average height of our male students, but we only get to measure 50 of the 708. We will use \\(\\bar{X}\\) as our estimate. We know from the answer to exercise 3 that the standard estimate of our error \\(\\bar{X}-\\mu\\) is \\(\\sigma/\\sqrt{N}\\). We want to compute this, but we don’t know \\(\\sigma\\). Based on what is described in this section, show your estimate of \\(\\sigma\\).\nNow that we have an estimate of \\(\\sigma\\), let’s call our estimate \\(s\\). Construct a 95% confidence interval for \\(\\mu\\).\nNow run a Monte Carlo simulation in which you compute 10,000 confidence intervals as you have just done. What proportion of these intervals include \\(\\mu\\)?\nUse the qnorm and qt functions to generate quantiles. Compare these quantiles for different degrees of freedom for the t-distribution. Use this to motivate the sample size of 30 rule of thumb.\nIn 1999, in England, Sally Clark was found guilty of the murder of two of her sons. Both infants were found dead in the morning, one in 1996 and another in 1998. In both cases, she claimed the cause of death was sudden infant death syndrome (SIDS). No evidence of physical harm was found on the two infants so the main piece of evidence against her was the testimony of Professor Sir Roy Meadow, who testified that the chances of two infants dying of SIDS was 1 in 73 million. He arrived at this figure by finding that the rate of SIDS was 1 in 8,500 and then calculating that the chance of two SIDS cases was 8,500 \\(\\times\\) 8,500 \\(\\approx\\) 73 million. Which of the following do you agree with?\n\n\nSir Meadow assumed that the probability of the second son being affected by SIDS was independent of the first son being affected, thereby ignoring possible genetic causes. If genetics plays a role then: \\(\\mbox{Pr}(\\mbox{second case of SIDS} \\mid \\mbox{first case of SIDS}) &gt; \\mbox{P}r(\\mbox{first case of SIDS})\\).\nNothing. The multiplication rule always applies in this way: \\(\\mbox{Pr}(A \\mbox{ and } B) =\\mbox{Pr}(A)\\mbox{Pr}(B)\\)\nSir Meadow is an expert and we should trust his calculations.\nNumbers don’t lie.\n\n\nUntil recentely, Florida was one of the most closely watched states in the U.S. election because it has many electoral votes, and the election was generally close, and Florida was a swing state that can vote either way. Create the following table with the polls taken during the last two weeks:\n\n\nlibrary(tidyverse)\nlibrary(dslabs)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(state == \"Florida\" & enddate &gt;= \"2016-11-04\" ) |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nTake the average spread of these polls. The CLT tells us this average is approximately normal. Calculate an average and provide an estimate of the standard error. Save your results in an object called results.\n\nNow assume a Bayesian model that sets the prior distribution for Florida’s election night spread \\(\\mu\\) to be Normal with expected value \\(\\theta\\) and standard deviation \\(\\tau\\). What are the interpretations of \\(\\theta\\) and \\(\\tau\\)?\n\n\n\\(\\theta\\) and \\(\\tau\\) are arbitrary numbers that let us make probability statements about \\(\\mu\\).\n\\(\\theta\\) and \\(\\tau\\) summarize what we would predict for Florida before seeing any polls. Based on past elections, we would set \\(\\mu\\) close to 0 because both Republicans and Democrats have won, and \\(\\tau\\) at about \\(0.02\\), because these elections tend to be close.\n\\(\\theta\\) and \\(\\tau\\) summarize what we want to be true. We therefore set \\(\\theta\\) at \\(0.10\\) and \\(\\tau\\) at \\(0.01\\).\nThe choice of prior has no effect on Bayesian analysis.\n\n\nThe CLT tells us that our estimate of the spread \\(\\hat{\\mu}\\) has normal distribution with expected value \\(\\mu\\) and standard deviation \\(\\sigma\\) calculated in problem 6. Use the formulas we showed for the posterior distribution to calculate the expected value of the posterior distribution if we set \\(\\theta = 0\\) and \\(\\tau = 0.01\\).\nNow compute the standard deviation of the posterior distribution.\nUsing the fact that the posterior distribution is normal, create an interval that has a 95% probability of occurring centered at the posterior expected value. Note that we call these credible intervals.\nAccording to this analysis, what was the probability that Trump wins Florida?\nNow use sapply function to change the prior variance from seq(0.005, 0.05, len = 100) and observe how the probability changes by making a plot.\nIn April 2013, José Iglesias, a professional baseball player was starting his career. He was performing exceptionally well. Specifically, he had a batting average (AVG) of .450. The batting average statistic is one way of measuring success. Roughly speaking, it tells us the success rate when batting. José had 9 successes out of 20 tries. An AVG of .450 means José has been successful 45% of the times he has batted which is rather high, historically speaking: no one has finished a season with an AVG of .400 or more since Ted Williams did it in 1941! We want to predict José’s batting average at the end of the season after players have about 500 tries or at bats. With the frequentist techniques we have no choice but to predict that his AVG will be .450 at the end of the season. Compute a confidence interval for the success rate.\nDespite the frequentist prediction of \\(.450\\) not a single baseball enthusiast would make this prediction. Why is this? One reason is that they now the estimate has much uncertainty. However, the main reason is that they are implicitly using a hierarchical model that factors in information from years of following baseball. Use the following code to explore the distribution of batting averages in the three seasons prior to 2013 and describe what this tells us.\n\n\nSo is José lucky or is he the best batter seen in the last 50 years? Perhaps it’s a combination of both luck and talent. But how much of each? If we become convinced that he is lucky, we should trade him to a team that trusts the .450 observation and is maybe overestimating his potential. The hierarchical model provides a mathematical description of how we came to see the observation of .450. First, we pick a player at random with an intrinsic ability summarized by, for example, \\(\\mu\\). Then we see 20 random outcomes with success probability \\(\\mu\\). What model would you use for the first level of your hierarchical model?\nDescribe the second level of the hierarchical model.\nApply the hierarchical model to José’s data. Suppose we want to predict his innate ability in the form of his true batting average \\(\\mu\\). Write down the distributions of the hierarchical model.\nWe now are ready to compute a the distribution of \\(\\mu\\) conditioned on the observed data \\(\\bar{X}\\). Compute the expected value of \\(\\mu\\) given the current average \\(\\bar{X}\\) and provide an intuitive explanation for the mathematical formula.\nWe started with a frequentist 95% confidence interval that ignored data from other players and summarized just José’s data: .450 \\(\\pm\\) 0.220. Construct a credible interval for \\(\\mu\\) based on the hierarchical model.\nThe credible interval suggests that if another team is impressed by the .450 observation, we should consider trading José as we are predicting he will be just slightly above average. Interestingly, the Red Sox traded José to the Detroit Tigers in July. Here are the José Iglesias batting averages for the next five months:\n\n\n\n\nMonth\nAt Bat\nHits\nAVG\n\n\n\n\nApril\n20\n9\n.450\n\n\nMay\n26\n11\n.423\n\n\nJune\n86\n34\n.395\n\n\nJuly\n83\n17\n.205\n\n\nAugust\n85\n25\n.294\n\n\nSeptember\n50\n10\n.200\n\n\nTotal w/o April\n330\n97\n.293\n\n\n\nWhich of the two approaches provided a better prediciton?"
  },
  {
    "objectID": "pset2.html#introduction",
    "href": "pset2.html#introduction",
    "title": "Problem Set 2",
    "section": "Introduction",
    "text": "Introduction\nFor this assignment, you’ll delve into data wrangling, statistical inference, and linear modeling that was used by academics to gain a deeper understanding of the efforts made to estimate the indirect death toll in Puerto Rico following Hurricane María. Begin by reviewing this comprehensive timeline and summary. Initially, we’ll use data wrangling techniques to extract information from documents released by organizations that had early access to the mortality registry data. Following that, we’ll work with the mortality registry data that has since been publicly disclosed by the government. To determine mortality rates, it’s essential to acquire data on population size, categorized by age and sex. We’ll achieve this by utilizing APIs provided by the US Census.\nThese are the libraries you will need and the only ones you are allowed to load\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(lubridate)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(pdftools)\nlibrary(janitor)\nlibrary(httr2)\nlibrary(excessmort)\nlibrary(jsonlite)\nlibrary(purrr)\n\nYou don’t need these but we will allow you to load them:\n\nlibrary(ggthemes)\nlibrary(ThemePark)\nlibrary(ggrepel)\n\nReminders:\n\nAdd a title to all your graphs.\nAdd a label to the x and y axes when not obvious what they are showing.\nThink about transformations that convey the message in clearer fashion."
  },
  {
    "objectID": "pset2.html#preparation",
    "href": "pset2.html#preparation",
    "title": "Problem Set 2",
    "section": "Preparation",
    "text": "Preparation\nCreate a directory for this homework. In this directory create two subdirectories: data and rdas. You will also create a get-population.R file where you will have the code to download and wrangle population data from the US Census."
  },
  {
    "objectID": "pset2.html#wrangling",
    "href": "pset2.html#wrangling",
    "title": "Problem Set 2",
    "section": "Wrangling",
    "text": "Wrangling\n\nIn December 2017 a preprint was published that includes data from the mortality registry. It is a Word document that you can download from https://osf.io/preprints/socarxiv/s7dmu/download. Save a PDF copy of this document to your data directory.\n\n\nurl &lt;- \"https://osf.io/preprints/socarxiv/s7dmu/download\"\nfn &lt;- tempfile(fileext = \".docx\")\ndownload.file(url, destfile = fn)\nsystem2(\"open\", fn) \n# save as PDF\n\n\nRead in the PFD file into R and create a data frame with the data in Table 1 of the paper. The data frame should be tidy with columns months, year, and deaths. Your data frame need not include the confidence intervals or averages.\n\n\nfilename &lt;- \"data/santoslozada-howard-2017-preprint.pdf\"\ntxt &lt;- pdf_text(filename)[4]\ntmp &lt;- str_split(txt, \"\\n\")[[1]][2:14] |&gt;\n  str_replace_all(\"\\\\s([A-Z])\", \"\\\\1\") %&gt;%\n  str_replace(\"\\\\s-\\\\s\", \"-\") %&gt;%\n  str_split(\"\\\\s+\", simplify = TRUE) \ntmp[1,1] &lt;- \"month\"\ndat &lt;- tmp |&gt; \n  row_to_names(1) |&gt; \n  as.data.frame() |&gt;\n  select(month, `2010`:`2016`) |&gt;\n  pivot_longer(-month, names_to = \"year\", values_to = \"deaths\") |&gt;\n  mutate(month = match(month, month.name),\n           year = factor(year), deaths = parse_number(deaths))\n\n\nFor each month compute the average and a 95% confidence interval to reproduce Figure 3 in the preprint. Make sure to show the month names on the x-axis, not numbers. Hint: Save the graph to an object to make an upcoming exercise easier.\n\n\np &lt;- dat |&gt; group_by(month) |&gt;\n  summarize(avg = mean(deaths), sd = sd(deaths), n = n(), \n            lower = avg - qt(0.975, n - 1)*sd/sqrt(n),\n            upper = avg + qt(0.975, n - 1)*sd/sqrt(n)) |&gt;\n  ggplot(aes(month, avg, ymin = lower, ymax = upper)) +\n  geom_errorbar() +\n  geom_point(color = \"red\") +\n  scale_x_continuous(breaks = 1:12, labels = month.abb) +\n  labs(title = \"Average deaths and 95% confidence intervals by month in Puerto Rico, 2010-2016\",\n       y = \"Mean and 95% C.I.\", x = \"Month\") +\n  theme_bw() \np \n\n\n\n\n\nThe model here seems to be that the observed death for month \\(i\\) and year \\(j\\) is\n\n\\[\nY_{ij} = \\mu_i + \\varepsilon_{ij}\n\\]\nwith \\(\\text{Var}(\\varepsilon_{ij}) = \\sigma^2_i\\). The preprint reports the September and October 2017 deaths as 2,987 and 3,043. Create a data frame called dat_2017 with these two values and include an estimate for the standard error of this random variable. Hint: Look at the model and use data from 2010-2016 to estimate \\(\\sigma_i\\).\n\ndat_2017 &lt;- dat |&gt; filter(month %in% c(9,10)) |&gt;\n  group_by(month) |&gt;\n  summarize(sd = sd(deaths)) |&gt;\n  bind_cols(data.frame(avg = c(2987, 3043))) |&gt;\n  mutate(upper = avg + 1.96*sd, lower = avg - 1.96*sd)\n\n\nMake a plot now that includes the two points for 2017 and the 1.96 standard errors bars around them. Are the deaths statistically significantly different than the expected based on 2010-2016 data?\n\n\np + geom_point(data = dat_2017, color = \"blue\") + geom_errorbar(data = dat_2017) +\n  labs(subtitle = \"Observed counts for 2017 are in blue\")\n\n\n\n\n\nOn December 8, 2017 the New York Times publishes an article with daily counts. They share the data that was provided to them by the Mortality Registry. It is PDF you can obtain here. Read the PDF into R and extract the daily counts. Save the results to a data frame called dat with columns data and deaths. Make sure the data frame is ordered by date.\n\n\nurl &lt;- \"https://github.com/c2-d2/pr_mort_official/raw/master/data/Mortalidad-RegDem-2015-17-NYT-part1.pdf\"\npdf &lt;- pdf_text(url) |&gt; str_split(\"\\n\")\ndat &lt;- lapply(pdf, function(s){\n  s &lt;- str_trim(s)\n  s &lt;- str_remove_all(s, \"Registro Demográfico - División de Calidad y Estadísticas Vitales\")\n  header_index &lt;- str_which(s, \"2015\")[1]\n  tmp &lt;- str_split(s[header_index], \"\\\\s+\", simplify = TRUE) |&gt; str_remove_all(\"\\\\*\") |&gt;\n    str_replace_all(\"Y(201\\\\d)\", \"\\\\1\")\n  month &lt;- tmp[1]\n  header &lt;- tmp[-c(1,5)]\n  tail_index  &lt;- str_which(s, \"Total\")\n  n &lt;- str_count(s, \"\\\\d+\")\n  out &lt;- c(1:header_index, ## take out first lines\n           which(n &lt;= 3), ## lines with just one number (plot y-axis ) or 3 (legend)\n           which(n &gt;= 20 & n &lt;= 31), ## take out lines with numbers from plot x-axis\n           tail_index:length(s)) ## take out lines at end\n  if (month == \"FEB\") {\n   feb29 &lt;- s[str_detect(s, \"^29\\\\s+\")] |&gt; str_remove(\"29\\\\s+\") |&gt; parse_number()\n  }\n  s &lt;- s[-out] |&gt;  \n    str_remove_all(\"[^\\\\d\\\\s]\") |&gt; ## remove things that are not digits or space\n    str_trim() |&gt; \n    str_split_fixed(\"\\\\s+\", n = 6)  ## split by any space\n  \n  if (month == \"DEC\") {\n    header &lt;- header[1:2]\n    s &lt;- s[,1:3]\n  } else {\n    s &lt;- s[,1:4]\n  }\n  colnames(s) &lt;- c(\"day\", header)\n  \n  s &lt;- s |&gt; as_tibble() |&gt; \n    mutate(month = month, day = as.numeric(day)) |&gt;\n    pivot_longer(-c(day, month), names_to = \"year\", values_to = \"deaths\") |&gt;\n    mutate(deaths = as.numeric(deaths), month = str_to_title(month)) |&gt;\n    mutate(month = if_else(month == \"Ago\", \"Aug\", month)) |&gt;\n    mutate(month = match(month, month.abb)) |&gt;\n    mutate(date = make_date(year, month, day)) |&gt;\n    select(date, deaths) |&gt;\n    arrange(date)\n\n  if (month == \"FEB\") {\n    s &lt;- bind_rows(s, data.frame(date = make_date(2016, 2, 29), deaths = feb29)) \n  }\n \n   return(s)\n})\ndat &lt;- do.call(\"bind_rows\", dat) |&gt; arrange(date)\n\n\nPlot the deaths versus dates and describe what you see towards the end for 2017.\n\n\ndat |&gt; ggplot(aes(date, deaths)) + geom_point() + \n  labs(title = \"Deaths extracted from PDF shared with NYT\", x = \"Date\", y = \"Deaths\")\n\n\n\n\n\nThe reason you see a drop at the end is because it takes time to officially register deaths. It takes about 45 days for 99% of the data to be added. Remove the last 45 days and remake the plot, but this time showing deaths against day of the year (1 through 365 or 366) with color highlighting what happened after the hurricane. Do not include a legend.\n\n\ndat |&gt; filter(date &lt;= max(date) - days(45)) |&gt;\n  ggplot(aes(yday(date), deaths, color = date &lt; make_date(2017, 9, 20))) + \n  geom_point(show.legend = FALSE) +\n  theme_bw() +\n  labs(title = \"Deaths extracted from PDF shared with NYT\", \n       subtitle = \"Data for days after the hurricane are shown in red\", x = \"Date\", y = \"Deaths\")"
  },
  {
    "objectID": "pset2.html#us-census-apis",
    "href": "pset2.html#us-census-apis",
    "title": "Problem Set 2",
    "section": "US Census APIs",
    "text": "US Census APIs\nIn June 2018, data was finally made public. This dataset gives you deaths by age group and sex obtained more recently from the Mortality Registry. In preparation for the analysis of these data, we will obtain population estimates from the US Census by age and gender.\nWe will be using two different APIs as that is how the Census makes the data available. Important to note that in one of these APIs, all ages 85 or above are grouped into one group. Observe that we use estimates as of July 1st.\nIf you wish to skip this section (though you will lose points), you can obtain the already wrangled population data here.\n\nFirst step is to obtain a census key. You can request one here https://api.census.gov/data/key_signup.html. Once you have a key create a file in your directory called census-key.R that simply defines the variable census_key to be your personal key. Do not share this key publicly. The quarto file you turn in should not show your census key, instead it should source a file called census-key.R to define the variable. We will have a file on our end with our key so your script can knit.\nOnce you have your key you can use the httr2 package to download the data directly from the Census data base. We will start downloading the intercensus data from 2000-2009 (data dictionary here). We will download it only for Puerto Rico which has region ID 72. The following code downloads the data.\n\n\nurl &lt;- \"https://api.census.gov/data/2000/pep\"\nsource(\"census-key.R\")\nendpoint &lt;- paste0(\"int_charage?get=POP,SEX,AGE,DATE_&for=state:72&key=\", census_key)\nresponse &lt;- request(url) |&gt; \n  req_url_path_append(endpoint) |&gt;\n  req_perform()  \n\nThe data is now included in response and you can access it using the resp functions in httr2. Examine the results you obtain when applying resp_body_string. Write code to convert this into a data frame with columns names year, sex, age, and population and call it pop1. Hint: Use the function fromJSON from the jsonlite package. The functions row_to_names and clean_names from the janitor package might also be handy. Use the codebook to understand how the date column relates to year.\n\npop1 &lt;- response |&gt;\n  resp_body_string() |&gt; \n  fromJSON(flatten = TRUE) |&gt;\n  as.data.frame() |&gt;\n  row_to_names(1) |&gt;\n  clean_names() |&gt;\n  mutate(across(everything(), parse_number)) |&gt;\n  filter(age != 999 & sex != 0 & between(date , 2, 11)) |&gt;\n  mutate(sex = factor(sex, labels = c(\"M\", \"F\")), year =  2000 + date - 2) |&gt; \n  select(-c(date, state))\n\n\nNow we will obtain data for 2010-2019. The intercensus data is not available so we will use Vintage 2019 data (data dictionary here). We can follow a similar procedure but with the following API and endpoints:\n\n\nurl &lt;- \"https://api.census.gov/data/2019/pep\"\nsource(\"census-key.R\")\nendpoint &lt;- paste0(\"charage?get=POP,SEX,AGE,DATE_CODE&for=state:72&key=\", census_key)\n\nDownload the data and write code to convert this into a data frame with columns names year, sex, age, and population and call it pop2.\n\nresponse &lt;- request(url) |&gt; \n  req_url_path_append(endpoint) |&gt;\n  req_perform()  \n\npop2 &lt;- response |&gt;\n  resp_body_string() |&gt; \n  fromJSON(flatten = TRUE) |&gt;\n  as.data.frame() |&gt;\n  row_to_names(1) |&gt;\n  clean_names() |&gt;\n  mutate(across(everything(), parse_number)) |&gt;\n  filter(age != 999 & sex != 0 & between(date_code , 3, 12)) |&gt;\n  mutate(sex = factor(sex, labels = c(\"M\", \"F\")), year = 2010 + date_code - 3) |&gt; \n  select(-c(date_code, state)) \n\n\nCombine the data frames pop1 and pop2 created in the previous exercises to form one population data frame called population and including all year. Make sure the 85+ category is correctly computed on the two datasets. Save it to a file called population.rds in your rds.\n\n\npop2 &lt;- pop2 |&gt;\n  mutate(age = if_else(age &gt; 85, 85, age)) |&gt;\n  group_by(sex, age, year) |&gt;\n  summarize(pop = sum(pop), .groups = \"drop\")\n\npopulation &lt;- bind_rows(pop1, pop2)\n\n#saveRDS(population, file = \"rdas/population.rds\")"
  },
  {
    "objectID": "pset2.html#daily-count-data",
    "href": "pset2.html#daily-count-data",
    "title": "Problem Set 2",
    "section": "Daily count data",
    "text": "Daily count data\nLet’s repeat the analysis done in the preprint but now using 2002-2016 data and, to better see the effect of the hurricane, let’s use weekly instead of monthly and start our weeks on the day the hurricane hit.\nYou can load the data from the excessmort package.\n\ndata(\"puerto_rico_counts\")\n\n\nDefine an object counts by wrangling puerto_rico_counts to 1) include data only from 20022017, 2) remove the population column, and 3) to match our population, combine the counts for those 85 and older together.\n\n\ncounts &lt;- puerto_rico_counts |&gt;\n  filter(between(year(date), 2002, 2017)) |&gt;\n  select(-population) |&gt;\n  mutate(agegroup = fct_collapse(agegroup, \"85-Inf\" = c(\"85-89\", \"90-94\", \"95-99\", \"100-Inf\"))) |&gt;\n  group_by(date, sex, agegroup) |&gt;\n  summarize(outcome = sum(outcome), .groups = \"drop\")\n\n\nCollapse the population data so that it combines agegroups like counts. Also change the sex column so that it matches counts as well.\n\n\ncuts &lt;- c(seq(0, 85, 5), Inf)\nlabels &lt;- paste0(head(cuts, -1), \"-\", tail(cuts, -1) - 1)\n#population &lt;- readRDS(\"rdas/population.rds\") |&gt;\npopulation &lt;- population |&gt;\n  mutate(agegroup = cut(age, cuts, labels = labels, include.lowest = TRUE, right = FALSE),\n         sex = fct_recode(sex, male = \"M\", female = \"F\")) |&gt;\n  group_by(year, sex, agegroup) |&gt;\n  summarize(population = sum(pop), .groups = \"drop\")\n\n\nAdd a population column to counts using the population data frame you just created.\n\n\ncounts &lt;- counts |&gt; \n  mutate(year = year(date)) |&gt;\n  left_join(population, by = c(\"year\", \"sex\", \"agegroup\"))\n\n\nUse R to determine what day of the week did María make landfall in PR.\n\n\nmaria &lt;- make_date(2017, 9, 20)\nwday(maria, label = TRUE)\n\n[1] Wed\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n\n\nRedefine the date column to be the start of the week that day is part of. Use the day of the week María made landfall as the first day. Now collapse the data frame to weekly data by redefining outcome to have the total deaths that week for each sex and agegroup. Remove weeks that have less the 7 days. Finally, add a column with the MMWR week. Name the resulting data frame weekly_counts\n\n\nweekly_counts &lt;- counts |&gt; \n  mutate(date = floor_date(date, week_start = 3, unit = \"week\")) |&gt;\n  group_by(date, sex, agegroup) |&gt;\n  summarize(outcome = sum(outcome), population = population[1], n = n(), .groups = \"drop\") |&gt;\n  filter(n == 7) |&gt;\n  select(-n) |&gt;\n  mutate(week = epiweek(date))\n\n\nMake a per-week version of the plot we made for monthly totals. Make a boxplot for each week based on the 20022016 data, then add red points for 2017. Comment on the possibility that indirect effect went past October.\n\n\ntotals &lt;- weekly_counts |&gt; \n  group_by(date, week) |&gt;\n  summarize(outcome = sum(outcome), .groups = \"drop\") \n\ntotals |&gt; filter(year(date)&lt;2007) |&gt;\n  ggplot(aes(week, outcome, group = week)) +\n  geom_boxplot() +\n  geom_point(data = filter(totals, year(date) == 2017), color = \"red\")\n\n\n\n\n\nIf we look at 2017 data before September and compare each week to the average from 20022016. What percent are below the median?\n\n\ntotals |&gt; \n  filter(year(date) &lt; 2017) |&gt;\n  group_by(week) |&gt; \n  summarize(avg = median(outcome)) |&gt;\n  left_join(filter(totals, year(date) == 2017), by = \"week\") |&gt;\n  filter(date &lt; make_date(2017,9,1)) |&gt;\n  summarize(mean(outcome &lt; avg))\n\n# A tibble: 1 × 1\n  `mean(outcome &lt; avg)`\n                  &lt;dbl&gt;\n1                   0.6\n\n\n\nWhy are 2017 totals somewhat below-average? Plot the population in millions against date. What do you see?\n\n\nweekly_counts |&gt; filter(week == 1) |&gt;\n  group_by(date) |&gt;\n  summarize(population = sum(population)) |&gt;\n  ggplot(aes(date, population/10^6)) + \n  geom_line() +\n  labs(title = \"Population of Puerto Rico: 20022017\",\n       x = \"Date\", y = \"Population (millions)\") +\n  theme_bw()\n\n\n\n\n\nWhen comparing mortalisty across populations of different sizes, we need to look at rates not totals. Because the population is decreasing, this is particularly important. Redo the boxplots but for rates instead of totals.\n\n\nrates &lt;- weekly_counts |&gt; \n  group_by(date, week) |&gt;\n  summarize(rates = sum(outcome)/sum(population), .groups = \"drop\") \n\nrates |&gt; filter(year(date) &lt; 2007) |&gt;\n  ggplot(aes(week, rates, group = week)) +\n  geom_boxplot() +\n  geom_point(data = filter(rates, year(date) == 2017), color = \"red\")\n\n\n\n\n\nNow the rates are all way above average! What is going on? Compute and plot the population sizes against year for each sex of the following age groups: 0-19, 20-39, 40-59, 60+. Describe what you see in this plot then explain why 2017 has higher average death rates.\n\n\nlibrary(forcats)\nweekly_counts |&gt; \n  filter(week == 1) |&gt;\n  mutate(agegroup = fct_collapse(agegroup, \n                                          \"0-19\" = c(\"0-4\",\"5-9\", \"10-14\", \"15-19\"),\n                                          \"20-39\" = c( \"20-24\", \"25-29\", \"30-34\", \"35-39\"),\n                                          \"40-59\" = c(\"40-44\", \"45-49\", \"50-54\", \"55-59\"),\n                                          other_level = \"60+\")) |&gt;\n  group_by(date, agegroup, sex) |&gt;\n  summarize(population = sum(population), .groups = \"drop\") |&gt;\n  ggplot(aes(date, population/10^5, color = sex)) +\n  geom_line() + \n  facet_wrap(~agegroup) +\n  theme_bw()\n\n\n\n\n\nCompute the death rates (deaths per 1,000 per year) by the agegroups for each year 20022016. Use a transformation of the y-axis that permits us to see the data clearly. Make a separate plot for males and females. Describe in two sentences what you learn.\n\n\nweekly_counts |&gt; \n  filter(year(date) &lt; 2017) |&gt;\n  mutate(year = year(date)) |&gt;\n  group_by(year, agegroup, sex) |&gt;\n  summarize(rate = mean(outcome)/population[1]*1000*52, .groups = \"drop\") |&gt;\n  ggplot(aes(year, rate, color = agegroup)) +\n  geom_line() +\n  scale_y_log10() +\n  facet_wrap(~sex)\n\n\n\n\n\nRepeat the above but use facet_wrap with scales = \"free_y\" to get a closer look at the patterns for each age group. In this case use color to distinguish the sexes. Describe the pattern observed for the death rate over time.\n\n\nweekly_counts |&gt; \n  mutate(year = year(date)) |&gt;\n  filter(year &lt; 2017) |&gt;\n  group_by(year, agegroup, sex) |&gt;\n  summarize(rate = mean(outcome)/population[1]*1000*52, .groups = \"drop\") |&gt;\n  ggplot(aes(year, rate, color = sex)) +\n  geom_line() +\n  scale_y_log10() +\n  facet_wrap(~agegroup, scales = \"free_y\")"
  },
  {
    "objectID": "pset2.html#linear-models",
    "href": "pset2.html#linear-models",
    "title": "Problem Set 2",
    "section": "Linear models",
    "text": "Linear models\n\nWe are going fit a linear model to account for the trend in death rates to obtain an more appropriate expected death rate for each agegroup and sex. Because we are fitting a linear model, it is preferable to have normally distributed data. We want the number of deaths per week to be larger than 10 for each group. Compute the average number of deaths per week by agegroup and sex for 2016. Based on these data, what agegroups do you recommend we combine?\n\n\ncounts |&gt; filter(year(date) == 2016) |&gt;\n  group_by(agegroup, sex) |&gt;\n  summarize(value = sum(outcome)/52, .groups = \"drop\") |&gt; \n  pivot_wider(names_from = sex)\n\n# A tibble: 18 × 3\n   agegroup female   male\n   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1 0-4       2.37   2.52 \n 2 5-9       0.308  0.173\n 3 10-14     0.115  0.231\n 4 15-19     0.288  1.62 \n 5 20-24     0.596  4.52 \n 6 25-29     1.19   4.67 \n 7 30-34     1.42   4.5  \n 8 35-39     2.19   5.40 \n 9 40-44     3.23   5.27 \n10 45-49     4.25   9.17 \n11 50-54     6.96  13.4  \n12 55-59    10.7   18.9  \n13 60-64    14.1   23.8  \n14 65-69    19.6   31.2  \n15 70-74    25.6   36.3  \n16 75-79    31.7   38.5  \n17 80-84    39.2   38.7  \n18 85-Inf   96.7   70.0  \n\n\n\nCreate a new dataset called dat that collapses the counts into agegroups with enough deaths to fit a linear model. Remove any week with MMWR week 53 and add a column t that includes the number of weeks since the first week in the first year.\n\n\ndat &lt;- weekly_counts |&gt;\n   mutate(agegroup = fct_collapse(agegroup, \n                                  \"0-44\" = c(\"0-4\",\"5-9\", \"10-14\", \"15-19\", \"20-24\", \n                                             \"25-29\", \"30-34\", \"35-39\", \"40-44\"),\n                                  \"45-54\" = c(\"45-49\", \"50-54\"))) |&gt;\n  group_by(date, agegroup, sex) |&gt;\n  summarize(population = sum(population), rate = sum(outcome)/population, .groups = \"drop\") |&gt;\n  mutate(week = epiweek(date)) |&gt;\n  filter(week &lt;= 52) |&gt;\n  mutate(t = (as.numeric(date) - as.numeric(min(date)))/7)\n\n\nWrite a function that receives a data frame tab, fits a linear model with a line for the time trend, and returns a data frame with 2017 data including a prediction.\n\n\nfit &lt;- function(tab){\n  fit &lt;- lm(rate ~ t + as.factor(week), data = filter(tab, year(date) &lt; 2017))\n  newdata &lt;- filter(tab, year(date) == 2017)\n  pred &lt;- predict(fit, se.fit = TRUE, newdata = newdata)\n  newdata$sd &lt;- summary(fit)$sigma\n  newdata$exp &lt;- pred$fit\n  newdata$se &lt;- pred$se.fit\n  return(newdata)\n}\n\n\nUse the group_modify function to fit this model to each sex and agegroup. Save the results in res.\n\n\nres &lt;- dat |&gt; \n  group_by(agegroup, sex) |&gt;\n  group_modify(~fit(.x)) |&gt;\n  ungroup()\n\n\nFor agegroup and by sex, plot the expected counts for each week with an error bar showing two standard deviations and in red the observed counts. Does the model appear to fit? Hint: Look to see if the red dots are inside the intervals before the hurricane.\n\n\nres |&gt; \n  ggplot(aes(week, exp*population)) +\n  geom_errorbar(aes(ymin = (exp - 1.96*sd)*population, ymax = (exp + 1.96*sd)*population)) +\n  geom_point() +\n  geom_point(aes(week, rate*population), color = \"red\") +\n  facet_grid(agegroup~sex, scales = \"free_y\") \n\n\n\n\n\nNow estimate weekly excess deaths for 2017 based on the rates esimated from 20022016 but the population sizes of 2017. Compare this to estimated standard deviation observed from year to year once we account for trends.\n\n\nexcess &lt;- res |&gt; \n  group_by(week) |&gt;\n  summarize(exp = sum(population*exp),\n            obs = sum(population*rate),\n            sd = sqrt(sum(population^2*sd^2))) |&gt;\n  mutate(excess = obs - exp) \n\nsd &lt;- unique(excess$sd)\nexcess |&gt;\n  ggplot(aes(week, excess)) +\n  geom_point() +\n  geom_hline(yintercept = 0, lty = 2) +\n  geom_hline(yintercept = c(-2,2)*sd, lty = 2, color = \"red\") +\n  theme_bw()\n\n\n\n\n\nPlot cummulative excess death for 2017 including a standard error.\n\n\nexcess |&gt;\n  mutate(excess = cumsum(excess), sd = sqrt(cumsum(sd^2))) |&gt;\n  ggplot(aes(week, excess)) +\n  geom_ribbon(aes(ymin = excess - 2*sd, ymax = excess + 2*sd), alpha = 0.5) +\n  geom_line() +\n  geom_hline(yintercept = 0, lty = 2) +\n  theme_bw()"
  },
  {
    "objectID": "20-regression.html#case-study-is-height-hereditary",
    "href": "20-regression.html#case-study-is-height-hereditary",
    "title": "20  Regression",
    "section": "20.1 Case study: is height hereditary?",
    "text": "20.1 Case study: is height hereditary?\n\nlibrary(tidyverse)\nlibrary(HistData)\n\nset.seed(1983)\ngalton_heights &lt;- GaltonFamilies |&gt;\n  filter(gender == \"male\") |&gt;\n  group_by(family) |&gt;\n  sample_n(1) |&gt;\n  ungroup() |&gt;\n  select(father, childHeight) |&gt;\n  rename(son = childHeight)\n\nSuppose we were asked to summarize the father and son data. Since both distributions are well approximated by the normal distribution, we could use the two averages and two standard deviations as summaries:\n\ngalton_heights |&gt; \n  summarize(mean(father), sd(father), mean(son), sd(son))\n\n# A tibble: 1 × 4\n  `mean(father)` `sd(father)` `mean(son)` `sd(son)`\n           &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1           69.1         2.55        69.2      2.71\n\n\nHowever, this summary fails to describe an important characteristic of the data: the trend that the taller the father, the taller the son.\n\ngalton_heights |&gt; ggplot(aes(father, son)) + \n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "20-regression.html#sec-corr-coef",
    "href": "20-regression.html#sec-corr-coef",
    "title": "20  Regression",
    "section": "20.2 The correlation coefficient",
    "text": "20.2 The correlation coefficient\nThe correlation coefficient is defined for a list of pairs \\((x_1, y_1), \\dots, (x_n,y_n)\\) as the average of the product of the standardized values:\n\\[\n\\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\n\\]\n\nrho &lt;- mean(scale(x) * scale(y))\n\nLet’s see why this makes sense\n\n\n\n\n\nThe correlation coefficient is always between -1 and 1. We can show this mathematically: consider that we can’t have higher correlation than when we compare a list to itself (perfect correlation) and in this case the correlation is:\n\\[\n\\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)^2 =\n\\frac{1}{\\sigma_x^2} \\frac{1}{n} \\sum_{i=1}^n \\left( x_i-\\mu_x \\right)^2 =\n\\frac{1}{\\sigma_x^2} \\sigma^2_x =\n1\n\\]\nA similar derivation, but with \\(x\\) and its exact opposite, proves the correlation has to be bigger or equal to -1.\nFor other pairs, the correlation is in between -1 and 1. The correlation, computed with the function cor, between father and son’s heights is about 0.5:\n\ngalton_heights |&gt; summarize(r = cor(father, son)) |&gt; pull(r)\n\n[1] 0.4334102\n\n\n\nFor reasons similar to those explained in Section Section 19.2.3 for the standard deviation, cor(x,y) divides by length(x)-1 rather than length(x).\n\nTo see what data looks like for different values of \\(\\rho\\), here are six examples of pairs with correlations ranging from -0.9 to 0.99:\n\n\n\n\n\n\n20.2.1 Sample correlation is a random variable\nLet’s consider our Galton heights to be the population and take random samples of size 25:\n\nr &lt;- sample_n(galton_heights, 25, replace = TRUE) |&gt; \n  summarize(r = cor(father, son)) |&gt; pull(r)\n\n`r`` is a random variable. We can run a Monte Carlo simulation to see its distribution:\n\nB &lt;- 1000\nN &lt;- 25\nr &lt;- replicate(B, {\n  sample_n(galton_heights, N, replace = TRUE) |&gt; \n    summarize(r = cor(father, son)) |&gt; \n    pull(r)\n})\nhist(r, breaks = 20)\n\n\n\n\nWe see that the expected value of `r`` is the population correlation:\n\nmean(r)\n\n[1] 0.4316779\n\n\nand that it has a relatively high standard error relative to the range of values R can take:\n\nsd(r)\n\n[1] 0.1652205\n\n\nSo, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty.\nDoes CLT apply?\nThe SE can be shown to be\n\\[\n\\sqrt{\\frac{1-r^2}{N-2}}\n\\]\nIn our example, \\(N=25\\) does not seem to be large enough to make the approximation a good one:\n\nggplot(aes(sample = r), data = data.frame(r)) + \n  stat_qq() + \n  geom_abline(intercept = mean(r), slope = sqrt((1 - mean(r)^2)/(N - 2)))\n\n\n\n\nIf you increase \\(N\\), you will see the distribution converging to normal.\n\nB &lt;- 1000\nN &lt;- 50\nr &lt;- replicate(B, {\n  sample_n(galton_heights, N, replace = TRUE) |&gt; \n    summarize(r = cor(father, son)) |&gt; \n    pull(r)\n})\nggplot(aes(sample = r), data = data.frame(r)) + \n  stat_qq() + \n  geom_abline(intercept = mean(r), slope = sqrt((1 - mean(r)^2)/(N - 2)))\n\n\n\n\n\n\n20.2.2 Correlation is not always a useful summary\nCorrelation is not always a good summary of the relationship between two variables. The following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point. All these pairs have a correlation of 0.82:\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHere are some other fun examples:\n\nlibrary(datasauRus)\n\nggplot(datasaurus_dozen, aes(x = x, y = y, color = dataset)) +\n  geom_point() +\n  facet_wrap(~dataset)\n\n\n\n\nCorrelation is only meaningful in a particular context. To help us understand when it is that correlation is meaningful as a summary statistic, we will return to the example of predicting a son’s height using his father’s height. This will help motivate and define linear regression. We start by demonstrating how correlation can be useful for prediction."
  },
  {
    "objectID": "20-regression.html#sec-conditional-expectation",
    "href": "20-regression.html#sec-conditional-expectation",
    "title": "20  Regression",
    "section": "20.3 Conditional expectations",
    "text": "20.3 Conditional expectations\nSuppose we are asked to guess the height of a randomly selected son and we don’t know his father’s height. How do we do this?\nMathematically, we can show that the conditional expectation\n\\[\nf(x) = \\mbox{E}(Y \\mid X = x)\n\\]\nminimizes the expected squared error (MSE):\n\\[\n\\mbox{E}[\\{Y -f(X)\\}^2]\n\\]\nwith \\(x\\) representing the fixed value that defines that subset, for example 72 inches.\nWe denote the standard deviation of the strata with\n\\[\n\\mbox{SD}(Y \\mid X = x) = \\sqrt{\\mbox{Var}(Y \\mid X = x)}\n\\]\nBecause the conditional expectation \\(E(Y\\mid X=x)\\) is the best predictor for the random variable \\(Y\\) for an individual in the strata defined by \\(X=x\\), many data science challenges reduce to estimating this quantity. The conditional standard deviation quantifies the precision of the prediction.\nHowever, we often have a limited number of points to estimate this conditional expectation. For example\n\nsum(galton_heights$father == 72)\n\n[1] 8\n\n\nfathers that are exactly 72-inches. If we change the number to 72.5, we get even fewer data points:\n\nsum(galton_heights$father == 72.5)\n\n[1] 1\n\n\nA practical way to improve these estimates of the conditional expectations, is to define strata of with similar values of \\(x\\). In our example, we can round father heights to the nearest inch and assume that they are all 72 inches. If we do this, we end up with the following prediction for the son of a father that is 72 inches tall:\n\nconditional_avg &lt;- galton_heights |&gt; \n  filter(round(father) == 72) |&gt;\n  summarize(avg = mean(son)) |&gt; \n  pull(avg)\nconditional_avg\n\n[1] 70.5\n\n\nNote that a 72-inch father is taller than average but smaller than 72. We see the same for someone slightly shorted than average:\n\nconditional_avg &lt;- galton_heights |&gt; \n  filter(round(father) == 67) |&gt;\n  summarize(avg = mean(son)) |&gt; \n  pull(avg)\nconditional_avg\n\n[1] 68.71818\n\n\nThe sons of have regressed some to the average height. We notice that the reduction in how many SDs taller is about 0.5, which happens to be the correlation. As we will see in a later section, this is not a coincidence.\nIf we want to make a prediction of any height, not just 72, we could apply the same approach to each strata. Stratification followed by boxplots lets us see the distribution of each group:\n\ngalton_heights |&gt; \n  mutate(father_strata = factor(round(father))) |&gt; \n  ggplot(aes(father_strata, son)) + \n  geom_boxplot() + \n  geom_point()\n\n\n\n\nNot surprisingly, the centers of the groups are increasing with height. Furthermore, these centers appear to follow a linear relationship. Below we plot the averages of each group. If we take into account that these averages are random variables with standard errors, the data is consistent with these points following a straight line:\n\n\n\n\n\nThe fact that these conditional averages follow a line is not a coincidence. In the next section, we explain that the line these averages follow is what we call the regression line, which improves the precision of our estimates. However, it is not always appropriate to estimate conditional expectations with the regression line so we also describe Galton’s theoretical justification for using the regression line."
  },
  {
    "objectID": "20-regression.html#the-regression-line",
    "href": "20-regression.html#the-regression-line",
    "title": "20  Regression",
    "section": "20.4 The regression line",
    "text": "20.4 The regression line\nIf we are predicting a random variable \\(Y\\) knowing the value of another \\(X=x\\) using a regression line, then we predict that for every standard deviation, \\(\\sigma_X\\), that \\(x\\) increases above the average \\(\\mu_X\\), our prediction \\(\\hat{Y}\\) increase \\(\\rho\\) standard deviations \\(\\sigma_Y\\) above the average \\(\\mu_Y\\) with \\(\\rho\\) the correlation between \\(X\\) and \\(Y\\). The formula for the regression is therefore:\n\\[\n\\left( \\frac{\\hat{Y}-\\mu_Y}{\\sigma_Y} \\right) = \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right)\n\\]\nWe can rewrite it like this:\n\\[\n\\hat{Y} = \\mu_Y + \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right) \\sigma_Y\n\\]\nNote that if the correlation is positive and lower than 1, our prediction is closer, in standard units, to the average height than the value used to predict, \\(x\\), is to the average of the \\(x\\)s. This is why we call it regression: the son regresses to the average height. In fact, the title of Galton’s paper was: Regression toward mediocrity in hereditary stature. To add regression lines to plots, we will need the above formula in the form:\n\\[\n\\hat{Y} = b + mx \\mbox{ with slope } m = \\rho \\frac{\\sigma_y}{\\sigma_x} \\mbox{ and intercept } b=\\mu_y - m \\mu_x\n\\]\nHere we add the regression line to the original data:\n\nmu_x &lt;- mean(galton_heights$father)\nmu_y &lt;- mean(galton_heights$son)\ns_x &lt;- sd(galton_heights$father)\ns_y &lt;- sd(galton_heights$son)\nr &lt;- cor(galton_heights$father, galton_heights$son)\n\ngalton_heights |&gt; \n  ggplot(aes(father, son)) + \n  geom_point(alpha = 0.5) +\n  geom_abline(slope = r * s_y/s_x, intercept = mu_y - r * s_y/s_x * mu_x) \n\n\n\n\nThe regression formula implies that if we first standardize the variables, that is subtract the average and divide by the standard deviation, then the regression line has intercept 0 and slope equal to the correlation \\(\\rho\\). You can make same plot, but using standard units like this:\n\ngalton_heights |&gt; \n  ggplot(aes(scale(father), scale(son))) + \n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = 0, slope = r)"
  },
  {
    "objectID": "20-regression.html#regression-improves-precision",
    "href": "20-regression.html#regression-improves-precision",
    "title": "20  Regression",
    "section": "20.5 Regression improves precision",
    "text": "20.5 Regression improves precision\nLet’s compare the two approaches to prediction that we have presented:\n\nRound fathers’ heights to closest inch, stratify, and then take the average.\nCompute the regression line and use it to predict.\n\nWe use a Monte Carlo simulation sampling \\(N=50\\) families:\n\nB &lt;- 1000\nN &lt;- 50\n\nset.seed(1983)\nconditional_avg &lt;- replicate(B, {\n  dat &lt;- sample_n(galton_heights, N)\n  dat |&gt; filter(round(father) == 72) |&gt; \n    summarize(avg = mean(son)) |&gt; \n    pull(avg)\n  })\n\nregression_prediction &lt;- replicate(B, {\n  dat &lt;- sample_n(galton_heights, N)\n  mu_x &lt;- mean(dat$father)\n  mu_y &lt;- mean(dat$son)\n  s_x &lt;- sd(dat$father)\n  s_y &lt;- sd(dat$son)\n  r &lt;- cor(dat$father, dat$son)\n  mu_y + r*(72 - mu_x)/s_x*s_y\n})\n\nAlthough the expected value of these two random variables is about the same:\n\nmean(conditional_avg, na.rm = TRUE)\n\n[1] 70.49368\n\nmean(regression_prediction)\n\n[1] 70.50941\n\n\nThe standard error for the regression prediction is substantially smaller:\n\nsd(conditional_avg, na.rm = TRUE)\n\n[1] 0.9635814\n\nsd(regression_prediction)\n\n[1] 0.4520833\n\n\nThe regression line is therefore much more stable than the conditional mean. There is an intuitive reason for this. The conditional average is computed on a relatively small subset: the fathers that are about 72 inches tall. In fact, in some of the permutations we have no data, which is why we use na.rm=TRUE. The regression always uses all the data."
  },
  {
    "objectID": "20-regression.html#bivariate-normal-distribution",
    "href": "20-regression.html#bivariate-normal-distribution",
    "title": "20  Regression",
    "section": "20.6 Bivariate normal distribution",
    "text": "20.6 Bivariate normal distribution\nCorrelation and the regression slope are a widely used summary statistic, but they are often misused or misinterpreted. Anscombe’s examples provide over-simplified cases of dataset in which summarizing with correlation would be a mistake. But there are many more real-life examples.\nThe main way we motivate the use of correlation involves what is called the bivariate normal distribution.\nWhen a pair of random variables is approximated by the bivariate normal distribution, scatterplots look like ovals. As we saw in Section Section 20.2), they can be thin (high correlation) or circle-shaped (no correlation.\nA more technical way to define the bivariate normal distribution is the following: if \\(X\\) is a normally distributed random variable, \\(Y\\) is also a normally distributed random variable, and the conditional distribution of \\(Y\\) for any \\(X=x\\) is approximately normal, then the pair is approximately bivariate normal. When three or more variables have the property that each pair is bivariate normal, we say the variables follow a multivariate normal distribution or that they are jointly normal.\nIf we think the height data is well approximated by the bivariate normal distribution, then we should see the normal approximation hold for each strata. Here we stratify the son heights by the standardized father heights and see that the assumption appears to hold:\n\ngalton_heights |&gt;\n  mutate(z_father = round((father - mean(father)) / sd(father))) |&gt;\n  filter(z_father %in% -2:2) |&gt;\n  ggplot() +  \n  stat_qq(aes(sample = son)) +\n  facet_wrap( ~ z_father) \n\n\n\n\nNow we come back to defining correlation. Galton used mathematical statistics to demonstrate that, when two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations. We don’t show the derivation here, but we can show that under this assumption, for any given value of \\(x\\), the expected value of the \\(Y\\) in pairs for which \\(X=x\\) is:\n\\[\n\\mbox{E}(Y | X=x) = \\mu_Y +  \\rho \\frac{x-\\mu_X}{\\sigma_X}\\sigma_Y\n\\]\nThis is the regression line, with slope \\[\\rho \\frac{\\sigma_Y}{\\sigma_X}\\] and intercept \\(\\mu_y - m\\mu_X\\). It is equivalent to the regression equation we showed earlier which can be written like this:\n\\[\n\\frac{\\mbox{E}(Y \\mid X=x)  - \\mu_Y}{\\sigma_Y} = \\rho \\frac{x-\\mu_X}{\\sigma_X}\n\\]\nThis implies that, if our data is approximately bivariate, the regression line gives the conditional probability. Therefore, we can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to predict.\nIn summary, if our data is approximately bivariate, then the conditional expectation, the best prediction of \\(Y\\) given we know the value of \\(X\\), is given by the regression line."
  },
  {
    "objectID": "20-regression.html#variance-explained",
    "href": "20-regression.html#variance-explained",
    "title": "20  Regression",
    "section": "20.7 Variance explained",
    "text": "20.7 Variance explained\nThe bivariate normal theory also tells us that the standard deviation of the conditional distribution described above is:\n\\[\n\\mbox{SD}(Y \\mid X=x ) = \\sigma_Y \\sqrt{1-\\rho^2}\n\\]\nTo see why this is intuitive, notice that without conditioning, \\(\\mbox{SD}(Y) = \\sigma_Y\\), we are looking at the variability of all the sons. But once we condition, we are only looking at the variability of the sons with a tall, 72-inch, father. This group will all tend to be somewhat tall so the standard deviation is reduced.\nSpecifically, it is reduced to \\(\\sqrt{1-\\rho^2} = \\sqrt{1 - 0.25}\\) = 0.87 of what it was originally. We could say that father heights “explain” 13% of the variability observed in son heights.\nThe statement “\\(X\\) explains such and such percent of the variability” is commonly used in academic papers. In this case, this percent actually refers to the variance (the SD squared). So if the data is bivariate normal, the variance is reduced by \\(1-\\rho^2\\), so we say that \\(X\\) explains \\(1- (1-\\rho^2)=\\rho^2\\) (the correlation squared) of the variance.\nBut it is important to remember that the “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution."
  },
  {
    "objectID": "20-regression.html#there-are-two-regression-lines",
    "href": "20-regression.html#there-are-two-regression-lines",
    "title": "20  Regression",
    "section": "20.8 There are two regression lines",
    "text": "20.8 There are two regression lines\nWe computed a regression line to predict the son’s height from father’s height. We used these calculations:\n\nmu_x &lt;- mean(galton_heights$father)\nmu_y &lt;- mean(galton_heights$son)\ns_x &lt;- sd(galton_heights$father)\ns_y &lt;- sd(galton_heights$son)\nr &lt;- cor(galton_heights$father, galton_heights$son)\nm_1 &lt;-  r * s_y / s_x\nb_1 &lt;- mu_y - m_1*mu_x\n\nwhich gives us the function \\(\\mbox{E}(Y\\mid X=x) =\\) 37.3 + 0.46 \\(x\\).\nWhat if we want to predict the father’s height based on the son’s? It is important to know that this is not determined by computing the inverse function: \\(x = \\{ \\mbox{E}(Y\\mid X=x) -\\) 37.3 \\(\\} /\\) 0.5.\nWe need to compute \\(\\mbox{E}(X \\mid Y=y)\\). Since the data is approximately bivariate normal, the theory described above tells us that this conditional expectation will follow a line with slope and intercept:\n\nm_2 &lt;-  r * s_x / s_y\nb_2 &lt;- mu_x - m_2 * mu_y\n\nSo we get \\(\\mbox{E}(X \\mid Y=y) =\\) 40.9 + 0.41y. Again we see regression to the average: the prediction for the father is closer to the father average than the son heights \\(y\\) is to the son average.\nHere is a plot showing the two regression lines, with blue for the predicting son heights with father heights and red for predicting father heights with son heights:\n\ngalton_heights |&gt; \n  ggplot(aes(father, son)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(intercept = b_1, slope = m_1, col = \"blue\") +\n  geom_abline(intercept = -b_2/m_2, slope = 1/m_2, col = \"red\")"
  },
  {
    "objectID": "20-regression.html#linear-models",
    "href": "20-regression.html#linear-models",
    "title": "20  Regression",
    "section": "20.9 Linear models",
    "text": "20.9 Linear models\n\nLet’s learn about the connection between regression and linear models.\nIf data is bivariate normal then the conditional expectations follow the regression line. This resul is derived, not assumed\nIn practice it is common to explicitly write down a model that describes the relationship between two or more variables using a linear model.\nlinear here does not refer to lines exclusively, but rather to the fact that the conditional expectation is a linear combination of known quantities. For example,\n\n\\[2 + 3x - 4y + 5z\\]\nis a linear combination of \\(x\\), \\(y\\), and \\(z\\).\n\nIf we write\n\n\\[\nY = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nthen if we assume \\(\\varepsilon\\) follows a normal distribution with expected value 0 and fixed standard deviation, then \\(Y\\) has the same properties as the regression setup gave us.\n\n\n\n\n\n\nNote\n\n\n\nIn statistical textbooks, the \\(\\varepsilon\\)s are referred to as “errors,” which originally represented measurement errors in the initial applications of these models. These errors were associated with inaccuracies in measuring height, weight, or distance. However, the term “error” is now used more broadly, even when the \\(\\varepsilon\\)s do not necessarily signify an actual error. For instance, in the case of height, if someone is 2 inches taller than expected based on their parents’ height, those 2 inches should not be considered an error. Despite its lack of descriptive accuracy, the term “error” is employed to elucidate the unexplained variability in the model, unrelated to other included terms.\n\n\n\nLinear model for Galton’s data, we would denote the \\(N\\) observed father heights with \\(x_1, \\dots, x_n\\), then we model the \\(N\\) son heights we are trying to predict with:\n\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\, i=1,\\dots,N.\n\\]\n*We can further assume that \\(\\varepsilon_i\\) are independent from each other and all have the same standard deviation.\n\nTo have a useful model for prediction, we need \\(\\beta_0\\) and \\(\\beta_1\\). We estimate these from the data.\nIn practice, linear models are just assumed without necessarily assuming normality: the distribution of the \\(\\varepsilon\\)s is not necessarily specified. Nevertheless, if your data is bivariate normal, the above linear model holds. If your data is not bivariate normal, then you will need to have other ways of justifying the model.\nOne reason linear models are popular is that they are interpretable.\nIn the case of Galton’s data, we can interpret the data like this: due to inherited genes, the son’s height prediction grows by \\(\\beta_1\\) for each inch we increase the father’s height \\(x\\).\nBecause not all sons with fathers of height \\(x\\) are of equal height, we need the term \\(\\varepsilon\\), which explains the remaining variability.\nThis remaining variability includes the mother’s genetic effect, environmental factors, and other biological randomness.\nGiven how we wrote the model above, the intercept \\(\\beta_0\\) is not very interpretable as it is the predicted height of a son with a father with no height. Due to regression to the mean, the prediction will usually be a bit larger than 0.\nTo make the slope parameter more interpretable, we can rewrite the model slightly as:\n\n\\[\nY_i = \\beta_0 + \\beta_1 (x_i - \\bar{x}) + \\varepsilon_i, \\, i=1,\\dots,N\n\\]\n\nIn this case \\(\\beta_0\\) represents the height when \\(x_i = \\bar{x}\\), which is the height of the son of an average father."
  },
  {
    "objectID": "20-regression.html#sec-lse",
    "href": "20-regression.html#sec-lse",
    "title": "20  Regression",
    "section": "20.10 Least Squares Estimates",
    "text": "20.10 Least Squares Estimates\nThe standard approach to estimate the \\(\\beta\\)s is to find the values that minimize the distance of the fitted model to the data:\n\\[\nRSS = \\sum_{i=1}^n \\left\\{  y_i - \\left(\\beta_0 + \\beta_1 x_i \\right)\\right\\}^2\n\\]\n\nThis quantity is called the residual sum of squares (RSS).\nOnce we find the values that minimize the RSS, we will call the values the least squares estimates (LSE) and denote them with \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\nLet’s demonstrate this with the previously defined dataset:\n\n\nlibrary(HistData)\nset.seed(1983)\ngalton_heights &lt;- GaltonFamilies |&gt;\n  filter(gender == \"male\") |&gt;\n  group_by(family) |&gt;\n  sample_n(1) |&gt;\n  ungroup() |&gt;\n  select(father, childHeight) |&gt;\n  rename(son = childHeight)\n\nLet’s write a function that computes the RSS for any pair of values \\(\\beta_0\\) and \\(\\beta_1\\).\n\nrss &lt;- function(beta0, beta1, data){\n  resid &lt;- galton_heights$son - (beta0 + beta1*galton_heights$father)\n  return(sum(resid^2))\n}\n\nSo for any pair of values, we get an RSS. Here is a plot of the RSS as a function of \\(\\beta_1\\) when we keep the \\(\\beta_0\\) fixed at 25.\n\nbeta1 = seq(0, 1, length = nrow(galton_heights))\nresults &lt;- data.frame(beta1 = beta1,\n                      rss = sapply(beta1, rss, beta0 = 25))\nresults |&gt; ggplot(aes(beta1, rss)) + geom_line() + \n  geom_line(aes(beta1, rss))\n\n\n\n\nWe can see a clear minimum for \\(\\beta_1\\) at around 0.65. However, this minimum for \\(\\beta_1\\) is for when \\(\\beta_0 = 25\\), a value we arbitrarily picked. We don’t know if (25, 0.65) is the pair that minimizes the equation across all possible pairs. We don’t need trial and error because we can use calculus."
  },
  {
    "objectID": "20-regression.html#the-lm-function",
    "href": "20-regression.html#the-lm-function",
    "title": "20  Regression",
    "section": "20.11 The lm function",
    "text": "20.11 The lm function\nIn R, we can obtain the least squares estimates using the lm function. To fit the model:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n\\]\nwith \\(Y_i\\) the son’s height and \\(x_i\\) the father’s height, we can use this code to obtain the least squares estimates.\n\nfit &lt;- lm(son ~ father, data = galton_heights)\nfit$coef\n\n(Intercept)      father \n  37.287605    0.461392 \n\n\n\nThe most common way we use lm is by using the character ~ to let lm know which is the variable we are predicting (left of ~) and which we are using to predict (right of ~).\nThe intercept is added automatically to the model that will be fit. To fit a model with no intercept you need to use -1 in the rightside of the formula.\nThe object fit includes more information about the fit. We can use the function summary to extract more of this information (not shown):\n\n\nsummary(fit)\n\n\nCall:\nlm(formula = son ~ father, data = galton_heights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3543 -1.5657 -0.0078  1.7263  9.4150 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.28761    4.98618   7.478 3.37e-12 ***\nfather       0.46139    0.07211   6.398 1.36e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.45 on 177 degrees of freedom\nMultiple R-squared:  0.1878,    Adjusted R-squared:  0.1833 \nF-statistic: 40.94 on 1 and 177 DF,  p-value: 1.36e-09\n\n\n\nTo understand some of the information included in this summary we need to remember that the LSE are random variables. Mathematical statistics gives us some ideas of the distribution of these random variables."
  },
  {
    "objectID": "20-regression.html#lse-are-random-variables",
    "href": "20-regression.html#lse-are-random-variables",
    "title": "20  Regression",
    "section": "20.12 LSE are random variables",
    "text": "20.12 LSE are random variables\nThe LSE is derived from the data \\(y_1,\\dots,y_N\\), which are a realization of random variables \\(Y_1, \\dots, Y_N\\). This implies that our estimates are random variables. Here is a Monte Carlo simulation demonstrating it:\n\nB &lt;- 1000\nN &lt;- 50\nlse &lt;- replicate(B, {\n  sample_n(galton_heights, N, replace = TRUE) |&gt; \n    lm(son ~ father, data = _) |&gt; \n    coef()\n})\nlse &lt;- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) \n\nWe can see the variability of the estimates by plotting their distributions:\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\n\n\n\n\nThe reason these look normal is because the central limit theorem applies here as well.\nThe standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them and they are included in the summary provided by the lm function.\nHere it is for one of our simulated data sets:\n\n\nsample_n(galton_heights, N, replace = TRUE) |&gt; \n  lm(son ~ father, data = _) |&gt; \n  summary() |&gt; \n  coef()\n\n              Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 19.2791952 11.6564590 1.653950 0.1046637693\nfather       0.7198756  0.1693834 4.249977 0.0000979167\n\n\n/8 You can see that the standard errors estimates reported by the summary are close to the standard errors from the simulation:\n\nlse |&gt; summarize(se_0 = sd(beta_0), se_1 = sd(beta_1))\n\n     se_0      se_1\n1 8.83591 0.1278812\n\n\nThe summary function also reports t-statistics (t value) and p-values (Pr(&gt;|t|)). The t-statistic is not actually based on the central limit theorem but rather on the assumption that the \\(\\varepsilon\\)s follow a normal distribution. Under this assumption, mathematical theory tells us that the LSE divided by their standard error, \\(\\hat{\\beta}_0 / \\hat{\\mbox{SE}}(\\hat{\\beta}_0 )\\) and \\(\\hat{\\beta}_1 / \\hat{\\mbox{SE}}(\\hat{\\beta}_1 )\\), follow a t-distribution with \\(N-p\\) degrees of freedom, with \\(p\\) the number of parameters in our model. In the case of height \\(p=2\\), the two p-values are testing the null hypothesis that \\(\\beta_0 = 0\\) and \\(\\beta_1=0\\), respectively.\n\n\n\n\n\n\nWarning\n\n\n\nAlthough we do not show examples in this lecture, hypothesis testing with regression models is commonly used in epidemiology and economics to make statements such as “the effect of A on B was statistically significant after adjusting for X, Y, and Z”. However, several assumptions have to hold for these statements to be true."
  },
  {
    "objectID": "20-regression.html#predicted-values-are-random-variables",
    "href": "20-regression.html#predicted-values-are-random-variables",
    "title": "20  Regression",
    "section": "20.13 Predicted values are random variables",
    "text": "20.13 Predicted values are random variables\nOnce we fit our model, we can obtain prediction of \\(Y\\) by plugging in the estimates into the regression model. For example, if the father’s height is \\(x\\), then our prediction \\(\\hat{Y}\\) for the son’s height will be:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\]\nWhen we plot \\(\\hat{Y}\\) versus \\(x\\), we see the regression line.\nThe ggplot2 layer geom_smooth(method = \"lm\") that we previously used plots \\(\\hat{Y}\\) and surrounds it by confidence intervals:\n\ngalton_heights |&gt; ggplot(aes(son, father)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nThe R function predict takes an lm object as input and returns the prediction. If requested, the standard errors and other information from which we can construct confidence intervals is provided:\n\n\nfit &lt;- galton_heights |&gt; lm(son ~ father, data = _) \n\ny_hat &lt;- predict(fit, se.fit = TRUE)\n\nnames(y_hat)\n\n[1] \"fit\"            \"se.fit\"         \"df\"             \"residual.scale\""
  },
  {
    "objectID": "20-regression.html#diagnostic-plots",
    "href": "20-regression.html#diagnostic-plots",
    "title": "20  Regression",
    "section": "20.14 Diagnostic plots",
    "text": "20.14 Diagnostic plots\nWhen the linear model is assumed rather than derived, all interpretations depend on the usefulness of the model. The lm function will fit the model and return summaries even when the model is wrong and unuseful.\n\nVisually inspecting residuals, defined as the difference between observed values and predicted values\n\n\\[\nr = Y - \\hat{Y} = Y - \\left(\\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\right),\n\\]\nand summaries of the residuals, is a powerful way to diagnose if the model is useful.\n\nNote that the residuals can be thought of estimates of the errors since\n\n\\[\n\\varepsilon = Y - \\left(\\beta_0 + \\beta_1 x_i \\right).\n\\]\nIn fact, residuals are often denoted as \\(\\hat{\\varepsilon}\\). This motivates several diagnostic plots. Becasue we obervere, \\(r\\) but don’t observe \\(\\varepsilon\\), we based the plots on the residuals.\n\nBecause the errors are assumed not to depend on the expected value of \\(Y\\), a plot of \\(r\\) versus the fitted values \\(\\hat{Y}\\) should show no relationship.\nIn cases in which we assume the errors follow a normal distribtuion a qqplot of standardized \\(r\\) should fall on a line when plotted against theoretical quantiles.\nBecause we assume the standard deviation of the errors is constant, if we plot the absolute value of the residuals, it should appear constant.\n\nWe prefer plots rather than summaries based on, for example, correlation because, as noted in Section @ascombe, correlation is not always the best summary of association. The function plot applied to an lm object automatically plots these.\n\nplot(fit, which = 1:3)\n\nThis function can produce six different plots, and the argument which let’s you specify which you want to see. You can learn more by reading the plot.lm help file."
  },
  {
    "objectID": "20-regression.html#the-regression-fallacy",
    "href": "20-regression.html#the-regression-fallacy",
    "title": "20  Regression",
    "section": "20.15 The regression fallacy",
    "text": "20.15 The regression fallacy\nWikipedia defines the sophomore slump as:\n\nA sophomore slump or sophomore jinx or sophomore jitters refers to an instance in which a second, or sophomore, effort fails to live up to the standards of the first effort. It is commonly used to refer to the apathy of students (second year of high school, college or university), the performance of athletes (second season of play), singers/bands (second album), television shows (second seasons) and movies (sequels/prequels).\n\nIn Major League Baseball, the rookie of the year (ROY) award is given to the first-year player who is judged to have performed the best. The sophmore slump phrase is used to describe the observation that ROY award winners don’t do as well during their second year. For example, this Fox Sports article asks “Will MLB’s tremendous rookie class of 2015 suffer a sophomore slump?”.\nDoes the data confirm the existence of a sophomore slump? Let’s take a look. Examining the data for widely used measure of success, the batting average, we see that this observation holds true for the top performing ROYs:\n\n\n\n\n\nnameFirst\nnameLast\nrookie_year\nrookie\nsophomore\n\n\n\n\nWillie\nMcCovey\n1959\n0.3541667\n0.2384615\n\n\nIchiro\nSuzuki\n2001\n0.3497110\n0.3214838\n\n\nAl\nBumbry\n1973\n0.3370787\n0.2333333\n\n\nFred\nLynn\n1975\n0.3314394\n0.3136095\n\n\nAlbert\nPujols\n2001\n0.3288136\n0.3135593\n\n\n\n\n\n\n\nIn fact, the proportion of players that have a lower batting average their sophomore year is 0.6981132.\nSo is it “jitters” or “jinx”? To answer this question, let’s turn our attention to all players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year).\n\n\n`summarise()` has grouped output by 'playerID'. You can override using the\n`.groups` argument.\n\n\nThe same pattern arises when we look at the top performers: batting averages go down for most of the top performers.\n\n\n\n\n\nnameFirst\nnameLast\n2013\n2014\n\n\n\n\nMiguel\nCabrera\n0.3477477\n0.3126023\n\n\nHanley\nRamirez\n0.3453947\n0.2828508\n\n\nMichael\nCuddyer\n0.3312883\n0.3315789\n\n\nScooter\nGennett\n0.3239437\n0.2886364\n\n\nJoe\nMauer\n0.3235955\n0.2769231\n\n\n\n\n\n\n\nBut these are not rookies! Also, look at what happens to the worst performers of 2013:\n\n\n\n\n\nnameFirst\nnameLast\n2013\n2014\n\n\n\n\nDanny\nEspinosa\n0.1582278\n0.2192192\n\n\nDan\nUggla\n0.1785714\n0.1489362\n\n\nJeff\nMathis\n0.1810345\n0.2000000\n\n\nB. J.\nUpton\n0.1841432\n0.2080925\n\n\nAdam\nRosales\n0.1904762\n0.2621951\n\n\n\n\n\n\n\nTheir batting averages mostly go up! Is this some sort of reverse sophomore slump? It is not. There is no such thing as the sophomore slump. This is all explained with a simple statistical fact: the correlation for performance in two separate years is high, but not perfect:\n\n\n\n\n\nThe correlation is 0.460254 and the data look very much like a bivariate normal distribution, which means we predict a 2014 batting average \\(Y\\) for any given player that had a 2013 batting average \\(X\\) with:\n\\[ \\frac{Y - .255}{.032} = 0.46 \\left( \\frac{X - .261}{.023}\\right) \\]\nBecause the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014. It’s not a jinx; it’s just due to chance. The ROY are selected from the top values of \\(X\\) so it is expected that \\(Y\\) will regress to the mean."
  },
  {
    "objectID": "20-regression.html#exercises",
    "href": "20-regression.html#exercises",
    "title": "20  Regression",
    "section": "20.16 Exercises",
    "text": "20.16 Exercises\n1. Load the GaltonFamilies data from the HistData. The children in each family are listed by gender and then by height. Create a dataset called galton_heights by picking a male and female at random.\n2. Make a scatterplot for heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.\n3. Compute the correlation in heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons."
  },
  {
    "objectID": "21-multivariate-regression.html#case-study-moneyball",
    "href": "21-multivariate-regression.html#case-study-moneyball",
    "title": "21  Multivariate Regression",
    "section": "21.1 Case study: Moneyball",
    "text": "21.1 Case study: Moneyball\n\nMoneyball: The Art of Winning an Unfair Game is a book by Michael Lewis about the Oakland Athletics (A’s) baseball team and its general manager, the person tasked with building the team, Billy Beane.\nTraditionally, baseball teams use scouts to help them decide what players to hire. Scouts tend to favor athletic players with observable physical abilities. For this reason, scouts tend to agree on who the best players are and, as a result, these players tend to be in high demand.\nIn 1995 the A’s team ownercut the budget drastically, leaving then general manager, Sandy Alderson (Billy Bean’s mentor), with one of the lowest payrolls in baseball. Alderson began using a statistical approach to find inefficiencies in the market. Billy Beane, became GM in 1998 and fully embraced a data approach.\nAs motivation for today’s lecture, we will pretend it is 2002 and try to build a baseball team with a limited budget, just like the A’s had to do. To appreciate what you are up against, note that in 2002 the Yankees’ payroll of $125,928,583 more than tripled the Oakland A’s $39,679,746:"
  },
  {
    "objectID": "21-multivariate-regression.html#baseball-basics",
    "href": "21-multivariate-regression.html#baseball-basics",
    "title": "21  Multivariate Regression",
    "section": "21.2 Baseball basics",
    "text": "21.2 Baseball basics\nTo see how regression will help us find undervalued players, we actually don’t need to understand all the details about the game of baseball, which has over 100 rules. Here, we distill the sport to the basic knowledge one needs to know how to effectively attack the data science problem.\n\nThe goal of a baseball game is to score more runs (points) than the other team.\nEach team has 9 batters that have an opportunity to hit a ball with a bat in a predetermined order. After the 9th batter has had their turn, the first batter bats again, then the second, and so on.\nEach time a batter has an opportunity to bat, we call it a plate appearance (PA). At each PA, the other team’s pitcher throws the ball and the batter tries to hit it.\nThe PA ends with an binary outcome: the batter either makes an out (failure) and returns to the bench or the batter doesn’t (success) and can run around the bases, and potentially score a run (reach all 4 bases).\nEach team gets nine tries, referred to as innings, to score runs and each inning ends after three outs (three failures).\nThere is a lot of chance involved in a plate appearance.\n\nHere is a video showing a success.\nAnd here is one showing a failure.\n\nThere are several ways to succeed. Understanding this distinction will be important for our analysis. When the batter hits the ball, the batter wants to pass as many bases as possible. There are four bases with the fourth one called home plate. Home plate is where batters start by trying to hit, so the bases form a cycle.\n\n\n\nA batter who goes around the bases and arrives home, scores a run.\nWe are simplifying a bit, but there are five ways a batter can succeed, that is, not make an out:\n\nSingle - Batter hits the ball and gets to first base.\nDouble (2B) - Batter hits the ball and gets to second base.\nTriple (3B) - Batter hits the ball and gets to third base.\nHome Run (HR) - Batter hits the ball and goes all the way home and scores a run.\nBases on balls (BB) - the pitcher fails to throw the ball through a predefined area considered to be hittable (the strikezone), so the batter is permitted to go to first base.\n\n\nHere is an example of a HR.\n\nWhile the batter is on base, the batter can also try to steal a base (SB). Here is an example of a stolen base.\nAll these events are kept track of during the season and are available to us through the Lahman package:\n\n\nlibrary(Lahman)"
  },
  {
    "objectID": "21-multivariate-regression.html#no-awards-for-bb",
    "href": "21-multivariate-regression.html#no-awards-for-bb",
    "title": "21  Multivariate Regression",
    "section": "21.3 No awards for BB",
    "text": "21.3 No awards for BB\n\n\nOne of the first data-driven insights, made by Bill James, is that the batting average ignores BB, but a BB is a success.\nJames proposed we use the on base percentage (OBP) instead of batting average. He defined OBP as (H+BB)/PA which is simply the proportion of plate appearances that don’t result in an out, a very intuitive measure.\nA player that gets many more BB than the average player might not be recognized if the batter does not excel in batting average. But is this player not helping produce runs?\nNo award is given to the player with the most BB. However, bad habits are hard to break and baseball did not immediately adopt OBP as an important statistic.\nIn contrast, total stolen bases were considered important and an awards given to the player with the most. But players with high totals of SB also made more outs as they did not always succeed.\nDoes a player with high SB total help produce runs? Can we use data science to determine if it’s better to pay for players with high BB or SB?"
  },
  {
    "objectID": "21-multivariate-regression.html#base-on-balls-or-stolen-bases",
    "href": "21-multivariate-regression.html#base-on-balls-or-stolen-bases",
    "title": "21  Multivariate Regression",
    "section": "21.4 Base on balls or stolen bases?",
    "text": "21.4 Base on balls or stolen bases?\nLet’s explore if stolen bases or\n\nlibrary(tidyverse)\ndat &lt;- Teams |&gt; filter(yearID %in% 1962:2002) |&gt;\n  mutate(team = teamID, year = yearID, r = R/G, \n         singles = (H - X2B - X3B - HR)/G, doubles = X2B/G, triples = X3B/G, \n         hr = HR/G,\n         sb = SB/G, bb = BB/G) |&gt;\n  select(team, year, r, singles, doubles, triples, hr, sb, bb)\n\nNow let’s start with a obvious question: does teams that hit more home runs score more runs? The visualization of choice when exploring the relationship between two variables is a scatter plot.\n\np &lt;- dat |&gt; ggplot(aes(hr, r)) + geom_point(alpha = 0.5)\np \n\n\n\n\nWe defined p because we will add to this plot latter. The plot shows a strong association: teams with more HRs tend to score more runs. Now let’s examine the relationship between stolen bases and runs:\n\ndat |&gt; ggplot(aes(sb, r)) + geom_point(alpha = 0.5)\n\n\n\n\nHere the relationship is not as clear. Finally, let’s examine the relationship between BB and runs:\n\ndat |&gt; ggplot(aes(bb, r)) + geom_point(alpha = 0.5)\n\n\n\n\nHere again we see a clear association. But does this mean that increasing a team’s BBs causes an increase in runs? One of the most important lessons you learn in this course is that association is not causation. In fact, it looks like BBs and HRs are also associated:\n\ndat |&gt; ggplot(aes(hr, bb)) + geom_point(alpha = 0.5)\n\n\n\n\n\nWe know that HRs cause runs because when a player hits a HR they are guaranteed at least one run. Could it be that HRs also cause BB and this makes it appear as if BB cause runs? When this happens we say there is confounding, an important concept we will learn more about throughout this chapter.\n\n8 Linear regression will help us parse all this out and quantify the associations."
  },
  {
    "objectID": "21-multivariate-regression.html#regression-applied-to-baseball-statistics",
    "href": "21-multivariate-regression.html#regression-applied-to-baseball-statistics",
    "title": "21  Multivariate Regression",
    "section": "21.5 Regression applied to baseball statistics",
    "text": "21.5 Regression applied to baseball statistics\n\nCan we use regression with these data?\nDoes the bivariate normal model work?\n\n\ndat |&gt; mutate(z_hr = round(scale(hr))) |&gt;\n  filter(z_hr %in% -2:3) |&gt;\n  ggplot() +  \n  stat_qq(aes(sample = r)) +\n  facet_wrap(~z_hr) \n\n\n\n\n\nLet’s use linear regression to predict the number of runs a team will score if we know how many home runs the team hits using regression:\n\n\nhr_fit  &lt;- lm(r ~ hr, data = dat)$coef\np + geom_abline(intercept = hr_fit[[1]], slope = hr_fit[[2]])\n\n\n\n\n\nFaster with geom_smooth:\n\n\np + geom_smooth(method = \"lm\")\n\n\n\n\nThe slope is\n\nlm(r ~ hr, data = dat)$coef[[2]]\n\n[1] 1.851745\n\n\n\nSo this tells us that teams that hit 1 more HR per game than the average team, score this coefficient amount of runs per game than the average team.\nNot surprisingly, HR hitters are very expensive we will need to find some other way to increase wins. We will use linear regression."
  },
  {
    "objectID": "21-multivariate-regression.html#the-broom-package",
    "href": "21-multivariate-regression.html#the-broom-package",
    "title": "21  Multivariate Regression",
    "section": "21.6 The broom package",
    "text": "21.6 The broom package\nThe broom package facilitates the use of R function such as lm within the tidyverse.\n\nlibrary(broom)\nfit &lt;- lm(r ~ bb, data = dat)\ntidy(fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.93     0.116       16.7 1.91e-55\n2 bb             0.739    0.0348      21.2 1.90e-83\n\n\nWe can add other important summaries, such as confidence intervals:\n\ntidy(fit, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.93     0.116       16.7 1.91e-55    1.70      2.15 \n2 bb             0.739    0.0348      21.2 1.90e-83    0.671     0.807\n\n\nAnother useful function in broom:\n\nglance(fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.304         0.303 0.493      451. 1.90e-83     1  -737. 1480. 1495.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nAs an alternative to\n\nsummary(fit)\n\n\nCall:\nlm(formula = r ~ bb, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.34598 -0.34959 -0.00931  0.35370  1.60233 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.92615    0.11556   16.67   &lt;2e-16 ***\nbb           0.73887    0.03477   21.25   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4926 on 1036 degrees of freedom\nMultiple R-squared:  0.3035,    Adjusted R-squared:  0.3029 \nF-statistic: 451.5 on 1 and 1036 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "21-multivariate-regression.html#confounding",
    "href": "21-multivariate-regression.html#confounding",
    "title": "21  Multivariate Regression",
    "section": "21.7 Confounding",
    "text": "21.7 Confounding\n\nPreviously, we noted a strong relationship between Runs and BB. If we find the regression line for predicting runs from bases on balls, we a get slope of:\n\n\nbb_slope &lt;- lm(r ~ bb, data = dat)$coef[2]\nbb_slope \n\n       bb \n0.7388725 \n\n\n\nSo does this mean that if we go and hire low salary players with many BB, and who therefore increase the number of walks per game by 2, our team will score these many more runs per game? Association is not causation.\nNote that if we compute the regression line slope for singles we get:\n\n\nlm(r ~ singles, data = dat)$coef[2]\n\n  singles \n0.4324101 \n\n\n\nThe reason this happen is because of confounding. Here we show the correlation between HR, BB, and singles:\n\n\ndat |&gt; summarize(cor(bb, hr), cor(singles, hr), cor(bb, singles))\n\n  cor(bb, hr) cor(singles, hr) cor(bb, singles)\n1   0.4064585       -0.1862848      -0.05126617\n\n\n\nExplanation from experts: pitchers, afraid of HRs, will sometimes avoid throwing strikes to HR hitters."
  },
  {
    "objectID": "21-multivariate-regression.html#understanding-confounding-through-stratification",
    "href": "21-multivariate-regression.html#understanding-confounding-through-stratification",
    "title": "21  Multivariate Regression",
    "section": "21.8 Understanding confounding through stratification",
    "text": "21.8 Understanding confounding through stratification\nDoes the relationship between BB and R hold if we keep HR fixed?\n\ndat |&gt; mutate(hr_strata = round(hr, 1)) |&gt; \n  filter(hr_strata &gt;= 0.4 & hr_strata &lt;= 1.2) |&gt;\n  ggplot(aes(bb, r)) +  \n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(~hr_strata) \n\n\n\n\n\nRemember that the regression slope for predicting runs with BB was\n\n\nround(bb_slope, 1)\n\n bb \n0.7 \n\n\n\nOnce we stratify by HR, these slopes are substantially reduced:\n\n\ndat |&gt; mutate(hr_strata = round(hr, 1)) |&gt; \n  filter(hr_strata &gt;= 0.5 & hr_strata &lt;= 1.2) |&gt;  \n  group_by(hr_strata) |&gt;\n  reframe(tidy(lm(r ~ bb))) |&gt;\n  filter(term == \"bb\")\n\n# A tibble: 8 × 6\n  hr_strata term  estimate std.error statistic  p.value\n      &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1       0.5 bb       0.566    0.110       5.14 3.02e- 6\n2       0.6 bb       0.405    0.0984      4.12 7.46e- 5\n3       0.7 bb       0.284    0.0717      3.96 1.13e- 4\n4       0.8 bb       0.378    0.0638      5.92 1.75e- 8\n5       0.9 bb       0.254    0.0762      3.33 1.08e- 3\n6       1   bb       0.506    0.0720      7.02 9.46e-11\n7       1.1 bb       0.444    0.0878      5.06 2.77e- 6\n8       1.2 bb       0.469    0.0804      5.84 2.91e- 7\n\n\n\nThe slopes are reduced, but they are not 0, which indicates that BBs are helpful for producing runs, just not as much as previously thought.\nIn fact, the values above are closer to the slope we obtained from singles,\n\n\nround(lm(r~singles, data = dat)$coef[2],1)\n\nsingles \n    0.4 \n\n\n\nThis is more consistent with our intuition since both singles and BB get us to first base, they should have about the same predictive power."
  },
  {
    "objectID": "21-multivariate-regression.html#sec-regression-in-r",
    "href": "21-multivariate-regression.html#sec-regression-in-r",
    "title": "21  Multivariate Regression",
    "section": "21.9 Multivariable regression",
    "text": "21.9 Multivariable regression\n\nIt is somewhat complex to be computing regression lines for each strata. We are essentially fitting models like this:\n\n\\[\n\\mbox{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1(x_2) x_1 + \\beta_2(x_1) x_2\n\\]\n\nIf we take random variability into account, the slopes in the strata don’t appear to change much. If these slopes are in fact the same, this implies that \\(\\beta_1(x_2)\\) and \\(\\beta_2(x_1)\\) are constants.\nThis in turn implies that the expectation of runs conditioned on HR and BB can be written like this:\n\n\\[\n\\mbox{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\n\nIn this analysis, referred to as multivariable regression, you will often hear people say that the BB slope \\(\\beta_1\\) is adjusted for the HR effect.\n\n8 Because the data is approximately normal and conditional distributions were also normal we are justified in using a linear model:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\varepsilon_i\n\\]\nwith \\(Y_i\\) runs per game for team \\(i\\), \\(x_{i,1}\\) walks per game, and \\(x_{i,2}\\).\n\nTo use lm here, we need to let the function know we have two predictor variables. So we use the + symbol as follows:\n\n\ntidy(lm(r ~ bb + hr, data = dat), conf.int = TRUE) \n\n# A tibble: 3 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.74     0.0820      21.2 3.38e- 83    1.58      1.90 \n2 bb             0.387    0.0269      14.4 8.41e- 43    0.334     0.440\n3 hr             1.57     0.0488      32.1 1.39e-157    1.47      1.66 \n\n\nWhen we fit the model with only one variable, the estimated slopes were\n\nlm(r ~ bb, data = dat)$coef[2]\n\n       bb \n0.7388725 \n\nlm(r ~ hr, data = dat)$coef[2]\n\n      hr \n1.851745 \n\n\nNote that when fitting the multivariable model both go down, with the BB effect decreasing much more.\n\n\n\n\n\n\nNote\n\n\n\nYou are ready to do exercises 1-12 if you want to practice before continuing."
  },
  {
    "objectID": "21-multivariate-regression.html#building-a-baseball-team",
    "href": "21-multivariate-regression.html#building-a-baseball-team",
    "title": "21  Multivariate Regression",
    "section": "21.10 Building a baseball team",
    "text": "21.10 Building a baseball team\n\nWe take somewhat of a “leap of faith” and assume that these five variables are jointly normal.\n\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\beta_3 x_{i,3}+ \\beta_4 x_{i,4} + \\beta_5 x_{i,5} + \\varepsilon_i\n\\]\nwith \\(x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\\) representing BB, singles, doubles, triples, and HR respectively.\n\nUsing lm, we can quickly find the LSE for the parameters using:\n\n\nfit &lt;- dat |&gt;  filter(year &lt;= 2001) |&gt; \n  lm(r ~ bb + singles + doubles + triples + hr, data = _)\n\n\nNote we fit the model to data up until 2001, the year before we will construct our team.\nHere are the resulting estimates:\n\n\ntidy(fit, conf.int = TRUE) |&gt; filter(term != \"(Intercept)\")\n\n# A tibble: 5 × 7\n  term    estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 bb         0.370    0.0119      31.2 1.00e-149    0.347     0.393\n2 singles    0.517    0.0128      40.5 5.29e-213    0.492     0.543\n3 doubles    0.775    0.0229      33.8 7.09e-168    0.730     0.820\n4 triples    1.24     0.0778      15.9 4.62e- 51    1.09      1.39 \n5 hr         1.44     0.0248      58.1 1.98e-323    1.39      1.49 \n\n\nTo see how well our metric actually predicts runs, we can predict the number of runs for each team in 2002 using the function predict, then make a plot:\n\ndat |&gt; mutate(r_hat = predict(fit, newdata = dat)) |&gt;\n  filter(year == 2002) %&gt;%\n  ggplot(aes(r_hat, r, label = team)) + \n  geom_point() +\n  geom_text(nudge_x = 0.1, cex = 2) + \n  geom_abline()\n\n\n\n\n\nThis motivates the definition of a new statistic that is more related to run production:\n\n\nb &lt;- round(coef(fit), 2)\ncat(b[1], \"+\", \n    b[2], \"x BB +\", \n    b[3], \"x singles +\", \n    b[4], \"x doubles +\",\n    b[5], \"x triples +\",\n    b[6], \"x HR\")\n\n-2.76 + 0.37 x BB + 0.52 x singles + 0.78 x doubles + 1.24 x triples + 1.44 x HR\n\n\n\nWe apply to statistic to each player but using a per plate appearance:\n\n\npa_per_game &lt;- Batting |&gt; filter(yearID == 2002) |&gt; \n  group_by(teamID) |&gt;\n  summarize(pa_per_game = sum(AB + BB)/162) |&gt; \n  pull(pa_per_game) |&gt; \n  mean()\n\nplayers &lt;- Batting |&gt; \n  filter(yearID %in% 1997:2001) |&gt; \n  group_by(playerID) |&gt;\n  mutate(pa = BB + AB) |&gt;\n  summarize(g = sum(pa)/pa_per_game,\n    bb = sum(BB)/g,\n    singles = sum(H - X2B - X3B - HR)/g,\n    doubles = sum(X2B)/g, \n    triples = sum(X3B)/g, \n    hr = sum(HR)/g,\n    avg = sum(H)/sum(AB),\n    pa = sum(pa)) |&gt;\n  filter(pa &gt;= 1000) |&gt;\n  select(-g)\n\nplayers$r_hat = predict(fit, newdata = players)\n\n\nThe distribution shows that there is wide variability across players:\n\n\nhist(players$r_hat, main = \"Predicted runs per game\")\n\n\n\n\n\nTo actually build the team, we will need to know their salaries as well as their defensive position.\n\n\nplayers &lt;- Salaries |&gt; \n  filter(yearID == 2002) |&gt;\n  select(playerID, salary) |&gt;\n  right_join(players, by = \"playerID\")\n\n\nNext add their defensive position\n\n\nposition_names &lt;- \n  paste0(\"G_\", c(\"p\",\"c\",\"1b\",\"2b\",\"3b\",\"ss\",\"lf\",\"cf\",\"rf\", \"dh\"))\n\ntmp &lt;- Appearances |&gt; \n  filter(yearID == 2002) |&gt; \n  group_by(playerID) |&gt;\n  summarize_at(position_names, sum) |&gt;\n  ungroup()\n  \npos &lt;- tmp |&gt;\n  select(all_of(position_names)) |&gt;\n  apply(X = _, 1, which.max) \n\nplayers &lt;- tibble(playerID = tmp$playerID, POS = position_names[pos]) |&gt;\n  mutate(POS = str_to_upper(str_remove(POS, \"G_\"))) |&gt;\n  filter(POS != \"P\") |&gt;\n  right_join(players, by = \"playerID\") |&gt;\n  filter(!is.na(POS)  & !is.na(salary))\n\n\nFinally, we add their first and last name:\n\n\nplayers &lt;- People |&gt;\n  select(playerID, nameFirst, nameLast, debut) |&gt;\n  mutate(debut = as.Date(debut)) |&gt;\n  right_join(players, by = \"playerID\")\n\nIf you are a baseball fan, you will recognize the top 10 players according to our new metric:\n\nplayers |&gt; select(nameFirst, nameLast, POS, salary, r_hat) |&gt; \n  arrange(desc(r_hat)) |&gt; head(10) \n\n   nameFirst nameLast POS   salary    r_hat\n1      Barry    Bonds  LF 15000000 8.052460\n2      Larry   Walker  RF 12666667 7.960583\n3       Todd   Helton  1B  5000000 7.403074\n4      Manny  Ramirez  LF 15462727 7.352475\n5      Sammy     Sosa  RF 15000000 7.201670\n6       Jeff  Bagwell  1B 11000000 7.053805\n7       Mike   Piazza   C 10571429 6.993616\n8      Jason   Giambi  1B 10428571 6.916405\n9      Edgar Martinez  DH  7086668 6.912145\n10       Jim    Thome  1B  8000000 6.885270\n\n\n\nOn average, players with a higher metric have higher salaries:\n\n\nplayers |&gt; ggplot(aes(salary, r_hat, color = POS)) + \n  geom_point() +\n  scale_x_log10()\n\n\n\n\n\nWe can search for good deals by looking at players who produce many more runs than others with similar salaries.\n\n\n\n  nameFirst  nameLast POS   salary    r_hat\n1      Todd    Helton  1B  5000000 7.403074\n2      Mike    Piazza   C 10571429 6.993616\n3     Edgar  Martinez  DH  7086668 6.912145\n4       Jim   Edmonds  CF  7333333 6.231373\n5      Jeff      Kent  2B  6000000 6.079064\n6      Phil     Nevin  3B  2600000 5.857409\n7      Matt    Stairs  RF   500000 5.758631\n8     Henry Rodriguez  LF   300000 5.640563\n9      John  Valentin  SS   550000 5.000417\n\n\n\nWe see that all these players have above average BB and most have above average HR rates, while the same is not true for singles and batting average. Here is a table with statistics standardized across players so that, for example, above average HR hitters have values above 0.\n\n\n\n   nameLast        bb      singles     doubles    triples          hr\n1    Helton 0.9088340 -0.214782777  2.64899973 -0.3105275  1.52212542\n2    Piazza 0.3281058  0.423121711  0.20371605 -1.4181571  1.82536533\n3  Martinez 2.1352215 -0.005170206  1.26490438 -1.2242578  0.80798171\n4   Edmonds 1.0706548 -0.557910413  0.79123812 -1.1517126  0.97300516\n5      Kent 0.2316321 -0.732290164  2.01139875  0.4483097  0.76586929\n6     Nevin 0.3066863 -0.905122480  0.47876338 -1.1908955  1.19270552\n7    Stairs 1.0996635 -1.512756207 -0.04608759 -1.1285395  1.12090808\n8 Rodriguez 0.2011513 -1.596359532  0.33245570 -0.7823620  1.32027344\n9  Valentin 0.1802855 -0.928706889  1.79403790 -0.4348410 -0.04524622\n         avg       r_hat\n1  2.6704562  2.54160226\n2  2.1990055  2.09347964\n3  2.2032836  2.00431446\n4  0.8543566  1.25925673\n5  0.7871932  1.09256504\n6  0.1048721  0.84997981\n7 -0.5608456  0.74187395\n8 -0.6723416  0.61265636\n9 -0.4717038 -0.08793886"
  },
  {
    "objectID": "21-multivariate-regression.html#exercises",
    "href": "21-multivariate-regression.html#exercises",
    "title": "21  Multivariate Regression",
    "section": "21.11 Exercises",
    "text": "21.11 Exercises\nWe have shown how BB and singles have similar predictive power for scoring runs. Another way to compare the usefulness of these baseball metrics is by assessing how stable they are across the years. Since we have to pick players based on their previous performances, we will prefer metrics that are more stable. In these exercises, we will compare the stability of singles and BBs.\n\nBefore we get started, we want to generate two tables. One for 2002 and another for the average of 1999-2001 seasons. We want to define per plate appearance statistics. Here is how we create the 2017 table. Keeping only players with more than 100 plate appearances.\n\n\nlibrary(Lahman)\ndat &lt;- Batting |&gt; filter(yearID == 2002) |&gt;\n  mutate(pa = AB + BB, \n         singles = (H - X2B - X3B - HR) / pa, bb = BB / pa) |&gt;\n  filter(pa &gt;= 100) |&gt;\n  select(playerID, singles, bb)\n\nNow compute a similar table but with rates computed over 1999-2001.\n\nYou can use the inner_join function to combine the 2001 data and averages in the same table:\n\n\ndat &lt;- inner_join(dat, avg, by = \"playerID\")\n\nCompute the correlation between 2002 and the previous seasons for singles and BB.\n\nNote that the correlation is higher for BB. To quickly get an idea of the uncertainty associated with this correlation estimate, we will fit a linear model and compute confidence intervals for the slope coefficient. However, first make scatterplots to confirm that fitting a linear model is appropriate.\nNow fit a linear model for each metric and use the confint function to compare the estimates.\nWe cam compute the correlation between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons (we randomly pick one offspring) using the following:\n\n\nlibrary(HistData)\nset.seed(1)\ngalton_heights &lt;- GaltonFamilies |&gt;\n  group_by(family, gender) |&gt;\n  sample_n(1) |&gt;\n  ungroup()\n\ncors &lt;- galton_heights |&gt; \n  pivot_longer(father:mother, names_to = \"parent\", values_to = \"parentHeight\") |&gt;\n  mutate(child = ifelse(gender == \"female\", \"daughter\", \"son\")) |&gt;\n  unite(pair, c(\"parent\", \"child\")) |&gt; \n  group_by(pair) |&gt;\n  summarize(cor = cor(parentHeight, childHeight))\n\nAre these differences statistically significant? To answer this, we will compute the slopes of the regression line along with their standard errors. Start by using lm and the broom package to compute the slopes LSE and the standard errors.\n\nRepeat the exercise above, but compute a confidence interval as well.\nPlot the confidence intervals and notice that they overlap, which implies that the data is consistent with the inheritance of height being independent of sex.\nBecause we are selecting children at random, we can actually do something like a permutation test here. Repeat the computation of correlations 100 times taking a different sample each time. Hint: use similar code to what we used with simulations.\nUse the Teams dataset to fit a linear regression model to obtain the effects of BB and HR on Runs (at the team level) in 1971. Use the tidy function in the broom package to obtain the results in a data frame.\nNow let’s repeat the above for each year since 1962 and make a plot. Use summarize and the broom package to fit this model for every year since 1962.\nUse the results of the previous exercise to plot the estimated effects of BB on runs.\nWrite a function that takes R, HR, and BB as arguments and fits two linear models: R ~ BB and R~BB+HR. Then use the summary function to obtain the BB for both models for each year since 1962. Then plot these against each other as a function of time."
  },
  {
    "objectID": "21-multivariate-regression.html#optional-exercises-will-not-appear-in-midterm",
    "href": "21-multivariate-regression.html#optional-exercises-will-not-appear-in-midterm",
    "title": "21  Multivariate Regression",
    "section": "21.12 Optional Exercises (will not appear in midterm)",
    "text": "21.12 Optional Exercises (will not appear in midterm)\n\nSince the 1980s, sabermetricians have used a summary statistic different from batting average to evaluate players. They realized walks were important and that doubles, triples, and HRs, should be weighed more than singles. As a result, they proposed the following metric:\n\n\\[\n\\frac{\\mbox{BB}}{\\mbox{PA}} + \\frac{\\mbox{Singles} + 2 \\mbox{Doubles} + 3 \\mbox{Triples} + 4\\mbox{HR}}{\\mbox{AB}}\n\\]\nThey called this on-base-percentage plus slugging percentage (OPS). Although the sabermetricians probably did not use regression, here we show how this metric is close to what one gets with regression.\nCompute the OPS for each team in the 2001 season. Then plot Runs per game versus OPS.\n\nFor every year since 1962, compute the correlation between runs per game and OPS; then plot these correlations as a function of year.\nNote that we can rewrite OPS as a weighted average of BBs, singles, doubles, triples, and HRs. We know that the weights for doubles, triples, and HRs are 2, 3, and 4 times that of singles. But what about BB? What is the weight for BB relative to singles? Hint: the weight for BB relative to singles will be a function of AB and PA.\nNote that the weight for BB, \\(\\frac{\\mbox{AB}}{\\mbox{PA}}\\), will change from team to team. To see how variable it is, compute and plot this quantity for each team for each year since 1962. Then plot it again, but instead of computing it for every team, compute and plot the ratio for the entire year. Then, once you are convinced that there is not much of a time or team trend, report the overall average.\nSo now we know that the formula for OPS is proportional to \\(0.91 \\times \\mbox{BB} + \\mbox{singles} + 2 \\times \\mbox{doubles} + 3 \\times \\mbox{triples} + 4 \\times \\mbox{HR}\\). Let’s see how these coefficients compare to those obtained with regression. Fit a regression model to the data after 1962, as done earlier: using per game statistics for each year for each team. After fitting this model, report the coefficients as weights relative to the coefficient for singles.\nWe see that our linear regression model coefficients follow the same general trend as those used by OPS, but with slightly less weight for metrics other than singles. For each team in years after 1962, compute the OPS, the predicted runs with the regression model and compute the correlation between the two as well as the correlation with runs per game.\nWe see that using the regression approach predicts runs slightly better than OPS, but not that much. However, note that we have been computing OPS and predicting runs for teams when these measures are used to evaluate players. Let’s show that OPS is quite similar to what one obtains with regression at the player level. For the 1962 season and after, compute the OPS and the predicted runs from our model for each player and plot them. Use the PA per game correction we used in the previous chapter:\nWhat players have show the largest difference between their rank by predicted runs and OPS?"
  },
  {
    "objectID": "22-linear-models.html#measurement-error-models",
    "href": "22-linear-models.html#measurement-error-models",
    "title": "22  Linear models",
    "section": "22.1 Measurement error models",
    "text": "22.1 Measurement error models\n\nAnother major application of linear models comes from measurement errors models.\nIn these applications, it is common to have a non-random covariate, such as time, and randomness is introduced from measurement error rather than sampling or natural variability.\n\n\n22.1.1 Example: falling object\n\nUse dslabs function rfalling_object generates measurements of position taken from a falling object:\nHere is the data:\n\n\nfalling_object |&gt; \n  ggplot(aes(time, observed_distance)) + \n  geom_point() +\n  ylab(\"Distance in meters\") + \n  xlab(\"Time in seconds\")\n\n\n\n\n\nThe data seems to imply the trajectory follows a parabola, which we can write like this:\n\n\\[\nf(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2\n\\]\n\nBut data does not fall exactly on a parabola. This most likely due to measurement error.\n\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\varepsilon_i, i=1,\\dots,n\n\\]\n\n\\(Y_i\\) representing distance in meters\n\\(x_i\\) representing time in seconds\n\\(\\varepsilon\\) accounting for measurement error.\nThe measurement error is assumed to be random, independent from each other, and having the same distribution for each \\(i\\). We also assume that there is no bias, which means the expected value \\(\\mbox{E}[\\varepsilon] = 0\\).\n\nIs this a liner model?\n\nNote that LSE calculations do not require the errors to be approximately normal so we can use lm to find the \\(\\beta\\)s:\n\n\nlibrary(broom)\nfit &lt;- falling_object |&gt; \n  mutate(time_sq = time^2) |&gt; \n  lm(observed_distance~time+time_sq, data = _)\ntidy(fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   55.2       0.543   102.    1.06e-17\n2 time           0.387     0.777     0.498 6.28e- 1\n3 time_sq       -4.96      0.230   -21.6   2.40e-10\n\n\nLet’s check if the estimated parabola fits the data. The broom function augment lets us do this easily:\n\naugment(fit) |&gt; \n  ggplot() +\n  geom_point(aes(time, observed_distance)) + \n  geom_line(aes(time, .fitted), col = \"blue\")\n\n\n\n\n\nWe actually know that the equation for the trajectory of a falling object is:\n\n\\[\nd(t) = h_0 + v_0 t -  0.5 \\times 9.8 \\, t^2\n\\]\nwith \\(h_0\\) and \\(v_0\\) the starting height and velocity, respectively.\n\nThese are consistent with the parameter estimates:\n\n\ntidy(fit, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   55.2       0.543   102.    1.06e-17    54.0      56.4 \n2 time           0.387     0.777     0.498 6.28e- 1    -1.32      2.10\n3 time_sq       -4.96      0.230   -21.6   2.40e-10    -5.47     -4.46"
  },
  {
    "objectID": "22-linear-models.html#sec-treatment-effect-models",
    "href": "22-linear-models.html#sec-treatment-effect-models",
    "title": "22  Linear models",
    "section": "22.2 Treatment effect models",
    "text": "22.2 Treatment effect models\n\nLinear models can also be used to quantify treatment effects in randomized and controlled experiments.\nOne of the first applications was in agriculture,In fact the use of \\(Y\\) for the outcome in Statistics, is due to the mathematical theory being developed for crop yield as the outcome.\nSince, the same ideas have been applied in other areas, such as randomized trials and A/B testing\nFurthermore, the use of these models has been extended to observational studies to adjust for factors such as age, sex, and smoking status."
  },
  {
    "objectID": "22-linear-models.html#mice-weights",
    "href": "22-linear-models.html#mice-weights",
    "title": "22  Linear models",
    "section": "22.3 Mice weights",
    "text": "22.3 Mice weights\nWe are going to examine data from an experiment to see if a high fat diet increases weight.\n\nlibrary(dslabs)\ntable(mice_weights$diet)\n\n\nchow   hf \n 394  386 \n\n\n\nA boxplot shows that the high fat diet mice are, on average, heavier.\n\n\nwith(mice_weights, boxplot(body_weight ~ diet))\n\n\n\n\n\nBut, given that we divided the mice at random, is it possible the observed difference is simply due to chance?"
  },
  {
    "objectID": "22-linear-models.html#comparing-group-means",
    "href": "22-linear-models.html#comparing-group-means",
    "title": "22  Linear models",
    "section": "22.4 Comparing group means",
    "text": "22.4 Comparing group means\nThe sample averages for the two groups, high-fat and chow diets, are different:\n\nlibrary(tidyverse)\nmice_weights |&gt; group_by(diet) |&gt; summarize(average = mean(body_weight))\n\n# A tibble: 2 × 2\n  diet  average\n  &lt;fct&gt;   &lt;dbl&gt;\n1 chow     31.5\n2 hf       36.7\n\n\nIs this difference due to chance?"
  },
  {
    "objectID": "22-linear-models.html#hypothesis-testing-review",
    "href": "22-linear-models.html#hypothesis-testing-review",
    "title": "22  Linear models",
    "section": "22.5 Hypothesis testing review",
    "text": "22.5 Hypothesis testing review\n\nDenote with \\(\\mu_1\\) and \\(\\sigma_1\\) the high-fat diet population average and standard deviation for weight.\nDefine \\(\\mu_0\\) and \\(\\sigma_0\\) similarly for the chow diet.\nDefine \\(N_1\\) and \\(N_0\\) as the sample sizes, let’s call them \\(\\bar{X}_1\\) and \\(\\bar{X}_0\\) as the sample averages, and \\(s_1\\) and \\(s_0\\) the sample standard deviations for the for the high-fat and chow diets, respectively.\nBecause this is a random sample the central limit theorem tells us that the difference in averages\n\n\\[\nbar{X}_1 - \\bar{X}_0\n\\]\nfollows a normal distribution with expected value \\(\\mu_1-\\mu_0\\) and standard error \\(\\sqrt{\\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_0^2}{N_0}}\\).\n\nIf we define the null hypothesis as the high-fat diet having no effect, or \\(\\mu_1 - \\mu_0 = 0\\), the the following summary statistic\n\n\\[\nt = \\frac{\\bar{X}_1 - \\bar{X}_0}{\\sqrt{\\frac{s_1^2}{N_1} + \\frac{s_0^2}{N_0}}}\n\\]\nfollows a standard normal distribution when the null hypothesis is true, which implies we can easily compute the probability of observing a value as large as the one we did:\n\nstats &lt;- mice_weights |&gt; group_by(diet) |&gt; summarize(xbar = mean(body_weight), s = sd(body_weight), n = n()) \nt_stat &lt;- with(stats, (xbar[2] - xbar[1])/sqrt(s[2]^2/n[2] + s[1]^2/n[1]))\nt_stat\n\n[1] 9.339096\n\n\n\nHere \\(t\\) is well over 3, so we don’t really need to compute the p-value 1-pnorm(t_stat) as we know it will be very small.\nNote that when \\(N\\) is not large, then the CLT does not apply. However, if the outcome data, in this case weight, follows a normal distribution, then \\(t\\) follows a t-distribution with \\(N_1+N_2-2\\) degrees of freedom.\nSo the calculation of the p-value is the same except we use 1-pt(t_stat, with(stats, n[2]+n[1]-2) to compute the p-value.\nBecause using differences in mean are so common in scientific studies, this t-statistic is one of the most widely reported summaries. When use it in a hypothesis testing setting, it is referred to a performing a t test.\n\n\n\n\n\n\n\nWarning\n\n\n\nIn the computation above we computed the probability of t being as large as what we observed. However, when we are equally interested in both directions, for example, either an increase or decrease in weight, then we need to compute the probability of t being as extreme as what we observe. The formula simply changes to using the absolute value: 1 - pnorm(abs(t-test)) or 1-pt(t_stat, with(stats, n[2]+n[1]-2)."
  },
  {
    "objectID": "22-linear-models.html#one-factor-design",
    "href": "22-linear-models.html#one-factor-design",
    "title": "22  Linear models",
    "section": "22.6 One factor design",
    "text": "22.6 One factor design\n\nAlthough the t-test is useful for cases in which we only account for two treatments, it is common to have other variables affect our outcomes.\nLinear models permit hypothesis testing in more general situations.\nWe start the description of the use linear models for estimating treatment effects by demonstrating how they can be used to perform t-tests.\nIf we assume that the weight distributions for both chow and high-fat diets are normally distributed, we can write the following linear model to represent the data:\n\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n\\]\n\nwith \\(X_i\\) 1 if the \\(i\\)-th mice was fed the high-fat diet and 0 otherwise and the errors \\(\\varepsilon_i\\) independent and normally distributed with expected value 0 and standard deviation \\(\\sigma\\).\nNotice that now \\(\\beta_0\\) represents the population average height of the mice on the chow diet and \\(\\beta_0 + \\beta_1\\) represents the population average for the weight of the mice on the high-fat diet.\nA nice feature of this model is that \\(\\beta_1\\) represents the treatment effect of receiving the high-fat diet.\nIf the null hypothesis that the high-fat diet has no effect can be quantified as \\(\\beta_1 = 0\\).\nWe can then estimate \\(\\beta_1\\) and answer the question of weather or not the observed difference is real by computing the estimates being as large as it was under the null.\nA powerful characteristics of linear models is that we can can estimate the parameters \\(\\beta\\)s and their standard errors with the same LSE machinery:\n\n\nfit &lt;- lm(body_weight ~ diet, data = mice_weights)\n\n\nBecause diet is a factor with two entries, the lm function knows to fit the model above with a \\(x_i\\) a indicator variable. The summary function shows us the resulting estimate, standard error, and p-value:\n\n\ncoefficients(summary(fit))\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 31.537005  0.3858192 81.740369 0.000000e+00\ndiethf       5.136078  0.5484506  9.364705 8.021959e-20\n\n\nor using broom we can write:\n\ntidy(fit, conf.int = TRUE) |&gt; filter(term == \"diethf\")\n\n# A tibble: 1 × 7\n  term   estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 diethf     5.14     0.548      9.36 8.02e-20     4.06      6.21\n\n\n\nThe statistic computed here is the estimate divided by its standard error:\n\n\\[\n\\hat{\\beta}_1 / \\hat{\\mbox{SE}}(\\hat{\\beta}_1)\n\\]\n\nIn the case for the simple one-factor model, we can show that this statistic is almost equivalent to the t-test.\n\n\nc(coefficients(summary(fit))[2,3], t_stat)\n\n[1] 9.364705 9.339096\n\n\n\nOne minor difference is that the linear model does not assume a different standard deviation for each population.\n\n\n\n\n\n\n\nNote\n\n\n\nIn the linear model description provided here we assumed \\(\\varepsilon\\) follows a normal distribution. This assumption permits us to show that the statistics formed by dividing estimates by their estimated standard errors follow t-distribution, which in turn permits us to estimate p-values or confidence intervals. However, note that we do not need this assumption to compute the expected value and standard error of the least squared estimates. Furthermore, if the number of observations in large enough, then the central limit theorem applies and we can obtain p-values and confidence intervals even without the normal distribution assumption."
  },
  {
    "objectID": "22-linear-models.html#two-factor-designs",
    "href": "22-linear-models.html#two-factor-designs",
    "title": "22  Linear models",
    "section": "22.7 Two factor designs",
    "text": "22.7 Two factor designs\n\nNote that this experiment included male and female mice, and male mice are known to be heavier. ’\nNote the residuals depend on the sex variable:\n\n\nboxplot(fit$residuals ~ mice_weights$sex)\n\n\n\n\n\nThis misspecification can have real implications since if more male mice received the high-fat diet, then this could explain the increase.\nOr if less received it, then we might underestimate the diet effect. Sex might be a confounder. Our model can certainly be improved.\nFrom examining the data:\n\n\nmice_weights |&gt; ggplot(aes(diet, log2(body_weight), fill = sex)) + geom_boxplot()\n\n\n\n\nwe see that there diet effect is observed for both sexes and that males are heavier than females.\n\nAlthough not nearly as obvious, it also appears the diet effect is stronger in males. A linear models that permits a different expected value four groups, 1) female on chow diet, 2) females on high-fat diet, 3) male on chow diet, and 4)males on high-fat diet,\n\n\\[\nY_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2}  + \\beta_3 x_{i,3}  + \\beta_4 x_{i,4}  + \\varepsilon_i\n\\]\nwith the \\(x_i\\)s indicator variables for each of the four groups.\n\nHowever, with this representation, none of the \\(\\beta\\)s represent the effect of interest: the diet effect.\nFurthermore, we now are accounting for the possibility that the diet effect is different for males and females have a different, and can test that hypothesis as well.\nA powerful feature of linear models is that we can rewrite the model so that we still have a different expected value for each group, but the parameters represent effects we are interested. So, for example, in the representation\n\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1}  + \\beta_2 x_{i,2}  + \\beta_3 x_{i,1} x_{i,2}  + \\varepsilon_i\n\\]\nwith \\(x_{i,1}\\) and indicator that is one if you have the treatment and \\(x_{i,2}\\) an indicator that is one if you are male, the \\(\\beta_1\\) can be interpreted as the treatment effect for females, \\(\\beta_2\\) as the difference between males and females, and \\(\\beta_3\\) the added treatment effect for males.\n\nThis last effect is referred to as an interaction effect.\nThe \\(\\beta_0\\) is consider the baseline value which is the average weight of females on the chow diet.\nStatistical textbooks describes several other ways in which the model can be rewritten to obtain other types of interpretations. For example, we might want \\(\\beta_2\\) to represent the average treatment effect between females and males, rather that the female treatment effects. This is achieved by defining what contrasts we are interested.\nIn R we can specific this model using the following\n\n\nfit &lt;- lm(body_weight ~ diet*sex, data = mice_weights)\n\n\nThe * implies that the term that multiplies \\(x_{i,1}\\) and \\(x_{i,2}\\) should be included, along with the \\(x_{i,1}\\) and \\(x_{i,2}\\) terms.\n\n\ntidy(fit, conf.int = TRUE) |&gt; filter(!str_detect(term, \"Intercept\"))\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 diethf          3.88     0.624      6.22 8.02e-10    2.66       5.10\n2 sexM            7.53     0.627     12.0  1.27e-30    6.30       8.76\n3 diethf:sexM     2.66     0.891      2.99 2.91e- 3    0.912      4.41\n\n\n\nNote that the male effect is larger that the diet effect, and the diet effect is statistically significant for both sexes, with the males having a higher effect by between 1 and 4.5 grams.\nA common approach applied when more than one factor is thought to affect the measurement is to simply include an additive effect for each factor like this:\n\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1}  + \\beta_2 x_{i,2}   + \\varepsilon_i\n\\]\n\nIn this model, the \\(\\beta_1\\) is a general diet effect that applies regardless of sex. In R we use the following code using a + instead of *:\n\n\nfit &lt;- lm(body_weight ~ diet + sex, data = mice_weights)\n\n\nBecause their a strong interaction effect, a diagnostic plots shows that the residuals are biased: the average negative for females on the diet and positive for the males on the diet, rather than 0.\n\n\nplot(fit, which = 1)\n\n\n\n\n\nScientific studies, particularly within epidemiology and social sciences, frequently omit interaction terms from models due to the high number of variables.\nAdding interactions necessitates numerous parameters, which, in extreme cases, may prevent the model from fitting. However, this approach assumes that the interaction terms are zero, which, if incorrect, can skew the results’ interpretation.\nConversely, when this assumption is valid, models excluding interactions are simpler to interpret as parameters are typically viewed as the extent to which the outcome increases with the assigned treatment.\n\n\n\n\n\n\n\nTip\n\n\n\nLinear models are very flexible and can be applied in many contexts. For example, we can include many more factors than 2. We have just scratched the surface of how linear models can be used to estimate treatment effects. We highly recommend learning more about this through linear model textbooks and R manuals on using the lm, contrasts, and model.matrix functions."
  },
  {
    "objectID": "22-linear-models.html#analysis-of-variance",
    "href": "22-linear-models.html#analysis-of-variance",
    "title": "22  Linear models",
    "section": "22.8 Analysis of variance",
    "text": "22.8 Analysis of variance\n\nOften we have variables of interest that have more than one level.\nFor example, we might have tested a third diet on the mice.\nIn statistics textbooks these variables are referred to as factor.\nIn these cases it is common to want to know rather than the effect of each levels of the factor, a more general quantification regarding the variability across the levels.\nAnalysis of variances or ANOVA does just this.\nThe summary used to quantify the variability of a factor is the mean squared error of the estimated effects of each level.\nAs an example, consider that the mice in our dataset are actually from several generations:\n\n\ntable(mice_weights$gen)\n\n\n  4   7   8   9  11 \n 97 195 193  97 198 \n\n\n\nWe can fit a linear model that fits an effect for each of these generations along with diet and sex model previously fit:\n\n\nfit &lt;- lm(body_weight ~ diet * sex + gen,  data = mice_weights)\n\nWe can then perform an analysis of variance with the R aov function:\n\nsummary(aov(fit))\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \ndiet          1   5143    5143 133.581 &lt;2e-16 ***\nsex           1  15260   15260 396.329 &lt;2e-16 ***\ngen           4    295      74   1.914 0.1061    \ndiet:sex      1    349     349   9.061 0.0027 ** \nResiduals   772  29725      39                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis analysis shows that the largest variation is explained by sex and then diet. The generation factor explains very little variation in comparison and is not found to be statistically significant.\n\n\n\n\n\n\nNote\n\n\n\nIn this book, we do not provide the details for how we compute this p-value. There are several books on analysis of variance and textbooks on linear models often include chapters on this topic. Those interested in learning more about these topics can consult these textbooks."
  },
  {
    "objectID": "22-linear-models.html#exercises",
    "href": "22-linear-models.html#exercises",
    "title": "22  Linear models",
    "section": "22.9 Exercises",
    "text": "22.9 Exercises\n\nPlot of co2 evels for the first 12 months of the co2 dataset and notice it seems to follow a sin wave with frequency 1 cycle per month. This means that a measurement error model that might work is\n\n\\[\ny_i = \\mu + A \\sin(2\\pi t_i / 12 + \\phi) + \\varepsilon_i\n\\] with \\(t_i\\) the month number of observation \\(i\\). Is this a linear model for the parameters \\(mu\\), \\(A\\) and \\(\\phi\\)?\n\nUsing trigonometry we can show that we can rewrite this model as\n\n\\[\ny_i = \\beta_0 + \\beta_1 \\sin(2\\pi t_i/12) + \\beta_2 \\cos(2\\pi t_i/12) + \\varepsilon_i\n\\]\n\nFind least square estimates for the \\(\\beta\\)s using lm. Show a plot of \\(y_i\\) versus \\(t_i\\) with a curve on the same plot showing \\(\\hat{Y}_i\\) versus \\(t_i\\).\n\n\ny &lt;- as.vector(co2)[1:12]\ntt &lt;- seq_along(y)\nplot(tt,y)\nx1 &lt;- sin(2*pi*tt/12)\nx2 &lt;- cos(2*pi*tt/12)\nfit &lt;- lm(y~x1+x2)\nlines(tt, fit$fitted)\n\n\n\n\n\nNow fit a measurement error model to the entire co2 dataset that includes a trend term that is a parabola as well as the sine wave model.\n\n\ny &lt;- as.vector(co2)\ntt &lt;- seq_along(y)\nx1 &lt;- sin(2*pi*tt/12)\nx2 &lt;- cos(2*pi*tt/12)\nfit &lt;- lm(y~x1+x2+poly(tt,2))\nplot(tt, y)\nlines(tt, fit$fitted, col = 2)\n\n\n\nsummary(fit)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + poly(tt, 2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.87720 -0.66136  0.00051  0.62583  2.36405 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  337.05353    0.04190 8043.63   &lt;2e-16 ***\nx1             2.20617    0.05927   37.22   &lt;2e-16 ***\nx2            -1.72426    0.05926  -29.10   &lt;2e-16 ***\npoly(tt, 2)1 319.24379    0.90669  352.10   &lt;2e-16 ***\npoly(tt, 2)2  31.30944    0.90650   34.54   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9065 on 463 degrees of freedom\nMultiple R-squared:  0.9964,    Adjusted R-squared:  0.9963 \nF-statistic: 3.171e+04 on 4 and 463 DF,  p-value: &lt; 2.2e-16\n\n\n\nRun diagnostic plots for the fitted model and describe the results.\nGenerate a sample of size \\(N=50\\) from an urn model with 50% blue beads:\n\n\nN &lt;- 50 \np &lt;- 0.5\nx &lt;- rbinom(N, 1, 0.5)\n\nThen compute a p-value testing if \\(p=0.5\\). Repeat this 10,000 times and report how often do we incorrectly is the p-value lower than 0.05? How often is it lower than 0.01?\n\nMake a histogram of the p-values you generated in exercise 1. Which of the following seems to be true:\n\n\nThe p-values are all 0.05\nThe p-values are normally distributed, CLT seems to hold.\nThe p-values are uniformly distributed\nThe p-values\n\n\nGenerate a sample of size \\(N=50\\) from an urn model with 52% blue beads:\n\n\nN &lt;- 50 \np &lt;- 0.52\nx &lt;- rbinom(N, 1, 0.5)\n\nThen compute a p-value testing if \\(p=0.5\\). Repeat this 10,000 times and report how often do we incorrectly is the p-value larger than 0.05? Note that you are computing 1 - power.\n\nRepeat exercise for but for the following values:\n\n\nvalues &lt;- expand.grid(N = c(25, 50, 100, 500, 1000), p = seq(0.51 ,0.75, 0.01))\n\nPlot power as a function of \\(N\\) with a different color curve for each value of p.\n\nOnce you fit a model, the estimate of the standard error \\(\\sigma\\) can be obtained like this:\n\n\nfit &lt;- lm(body_weight ~ diet, data = mice_weights)\nsummary(fit)$sigma\n\nCompute the estimate of \\(\\sigma\\) using the model that includes just diet and a model that accounts for sex. Are the estimates the same? If not, why not?\n\nOne of the assumption of the linear model fit by lm is that the standard deviation of the errors \\(\\varepsilon_i\\) is equal for all \\(i\\). This implies it does not depend on the expected value. Group the mice by their weight like this:\n\n\nbreaks &lt;- with(mice_weights, seq(min(body_weight), max(body_weight), 1))\ndat &lt;- mutate(mice_weights, group = cut(body_weight, breaks, include_lowest = TRUE))\n\nCompute the average and standard deviation for groups having more than 10 observations and use data exploration to see if this assumption holds?\n\nThe dataset also includes a variable indicating which litter the mice came from. Create a boxplot showing weights by litter. Use faceting to make separate plots for each diet and sex combination.\nUse a linear model to test for a litter effect. Account for sex and diet. Use ANOVA to compare the variability explained by litter to other factors.\nThe mouse_weights data includes two other outcomes: bone density and percent fat. Make a boxplot showing bone density by sex and diet. Compare what the visualizations shows for the diet effect by sex.\nFit a linear model and test for the diet effect on bone density separately for each sex. Note that the diet effect is statistically significant for females but not for males. Then fit the model to the entire dataset that includes diet, sex and their interaction. Note that the diet effect is significant, yet the interaction effect is not. Explain how this can happen? Hint: To fit a model to the entire dataset that fit a separate effect for males and females you can use the formula ~ sex + diet:sex\nWe previously talked about pollster bias. We used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election.\n\n\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(pollster %in% c(\"Rasmussen Reports/Pulse Opinion Research\",\n                         \"The Times-Picayune/Lucid\") &\n           enddate &gt;= \"2016-10-15\" &\n           state == \"U.S.\") |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) \n\nWe want to answer the question: is there pollsetr bias? Make a plot showing the spreads for each pollster.\n\nThe data does seem to suggest there is a difference. However, these data are subject to variability. Perhaps the differences we observe are due to chance.\n\nThe urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call \\(\\mu\\). Use a linear model to answer “is there an pollster effect?”"
  },
  {
    "objectID": "23-association-tests.html#case-study-funding-success-rates",
    "href": "23-association-tests.html#case-study-funding-success-rates",
    "title": "23  Association tests",
    "section": "23.1 Case study: funding success rates",
    "text": "23.1 Case study: funding success rates\nA 2014 PNAS paper1 analyzed success rates from funding agencies in the Netherlands and concluded that their:\n\nresults reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language use in instructional and evaluation materials.\n\nThe main evidence for this conclusion comes down to a comparison of the percentages. Table S1 in the paper includes the information we need. Here are the three columns showing the overall outcomes:\n\n\n          discipline applications_total success_rates_total\n1  Chemical sciences                122                26.2\n2  Physical sciences                174                20.1\n3            Physics                 76                26.3\n4         Humanities                396                16.4\n5 Technical sciences                251                17.1\n6  Interdisciplinary                183                15.8\n\n\nWe have these values for each gender:\n\nnames(research_funding_rates)\n\n [1] \"discipline\"          \"applications_total\"  \"applications_men\"   \n [4] \"applications_women\"  \"awards_total\"        \"awards_men\"         \n [7] \"awards_women\"        \"success_rates_total\" \"success_rates_men\"  \n[10] \"success_rates_women\"\n\n\nWe can compute the totals that were successful and the totals that were not as follows:\n\ntotals &lt;- research_funding_rates |&gt; \n  select(-discipline) |&gt; \n  summarize_all(sum) |&gt;\n  summarize(yes_men = awards_men, \n            no_men = applications_men - awards_men, \n            yes_women = awards_women, \n            no_women = applications_women - awards_women) \n\nSo we see that a larger percent of men than women received awards:\n\ntotals |&gt; summarize(percent_men = yes_men/(yes_men+no_men),\n                    percent_women = yes_women/(yes_women+no_women))\n\n  percent_men percent_women\n1     0.17737     0.1489899\n\n\nBut could this be due just to random variability? Here we learn how to perform inference for this type of data."
  },
  {
    "objectID": "23-association-tests.html#chi-square-test",
    "href": "23-association-tests.html#chi-square-test",
    "title": "23  Association tests",
    "section": "23.2 Chi-square Test",
    "text": "23.2 Chi-square Test\nImagine we have 290, 1,345, 177, 1,011 applicants, some are men and some are women and some get funded, whereas others don’t. We saw that the success rates for men and woman were:\n\ntotals |&gt; summarize(percent_men = yes_men/(yes_men+no_men),\n                    percent_women = yes_women/(yes_women+no_women))\n\n  percent_men percent_women\n1     0.17737     0.1489899\n\n\nrespectively. Would we see this again if we randomly assign funding at the overall rate:\n\nrate &lt;- with(totals, (yes_men + yes_women))/sum(totals)\nrate\n\n[1] 0.1654269\n\n\nThe Chi-square test answers this question. The first step is to create the two-by-two data table:\n\ntwo_by_two &lt;- with(totals, data.frame(awarded = c(\"no\", \"yes\"), \n                                      men = c(no_men, yes_men),\n                                      women = c(no_women, yes_women)))\ntwo_by_two\n\n  awarded  men women\n1      no 1345  1011\n2     yes  290   177\n\n\nThe general idea of the Chi-square test is to compare this two-by-two table to what you expect to see, which would be:\n\nwith(totals, data.frame(awarded = c(\"no\", \"yes\"), \n                        men = (no_men + yes_men) * c(1 - rate, rate),\n                        women = (no_women + yes_women) * c(1 - rate, rate)))\n\n  awarded       men    women\n1      no 1364.5271 991.4729\n2     yes  270.4729 196.5271\n\n\nWe can see that more men than expected and fewer women than expected received funding. However, under the null hypothesis these observations are random variables. The Chi-square test tells us how likely it is to see a deviation this large or larger. This test uses an asymptotic result, similar to the CLT, related to the sums of independent binary outcomes. The R function chisq.test takes a two-by-two table and returns the results from the test:\n\nchisq_test &lt;- chisq.test(two_by_two[, -1])\n\nWe see that the p-value is 0.0509:\n\nchisq_test$p.value\n\n[1] 0.05091372"
  },
  {
    "objectID": "23-association-tests.html#sec-glm",
    "href": "23-association-tests.html#sec-glm",
    "title": "23  Association tests",
    "section": "23.3 Generalized linear models",
    "text": "23.3 Generalized linear models\n\nWe presented a way to perform hypothesis testing for determining if there is association between two binary outcome, but we have not yet described how to quantify effects.\nCan we estimate the effect of being a woman in funding success in the Netherlands? Note that if our outcomes are binary, then the linear models presented in the treatment effect chapter are not appropriate because the \\(\\beta\\)s and \\(\\varepsilon\\) are continuous.\n\n*An adaptation of these methods, that is widely used in, for example, medical studies, gives us a way to estimate effects along with their standard errors.\n\nThe idea is to model a transformation of the expected value of the outcomes with a linear model.\n\n\\[\ng\\{\\mbox{E}(Y_i)\\} = \\beta_0 + \\beta_1 x_i\n\\]\nTo finish describing the model we impose a distribution on \\(Y\\) such as binomial or Poisson. These are referred to as generalized linear models.\n\nWe define \\(Y_i\\) to be 1 if person \\(i\\) received funding and 0 otherwise and \\(x_i\\) to be 1 for person \\(i\\) is a women and 0 for men.\nFor this data the expected value of \\(Y_i\\) is the probability of funding for person \\(i\\) \\(\\mbox{Pr}(Y_i=1)\\).\nWe assume the outcomes \\(Y_i\\) are binomial with \\(N=1\\) and probability \\(p_i\\).\nFor binomial data, the most widely used transformation is the logit function \\(g(p) = \\log \\{p / (1-p)\\}\\) which takes numbers between 0 and 1 to any continuous number. The model looks like this:\n\n\\[\n\\log \\frac{\\mbox{Pr}(Y_i=1)}{1-\\mbox{Pr}(Y_i=1)} = \\beta_0 +  \\beta_1 x_i\n\\]\n\n23.3.1 The odds ratio\n\nTo understand how \\(\\beta_1\\) can be used to quantify the effect of being a woman on success rates, first note that\n\n\\[\n\\mbox{Pr}(Y_i=1)/\\{1-\\mbox{Pr}(Y_i=1)\\} = \\mbox{Pr}(Y_i=1)/\\mbox{Pr}(Y_i=0)\n\\]\nis the odds of person \\(i\\) getting funding.\n\nThis implies that \\(e^{\\beta_0}\\) is the odds for men and \\(e^{\\beta_0}e^{\\beta_1}\\) is the odds for women, which implies \\(e^{\\beta_1}\\) is the odds for women divided by the odds for men. This quantity is called the odds ratio.\nTo see this not that if use \\(p_1\\) and \\(p_0\\) to denote the probability of success for women and men, respectively, then \\(e^\\{beta_1\\) can be rewritten as\n\n\\[\ne^{\\beta_1} = \\frac{p_1}{1-p_1} \\, / \\, \\frac{p_0}{1-p_0}\n\\]\n\n\\(\\beta_1\\) therefore quantifies the log odds ratio.\nLeast squares is no longer an optimal way of estimating the parameters and instead we use an approach called maximum likelihood estimation (MLE).\nA version of the central limit theorem applies and the estimates obtained this way are approximately normal when th number of observations is large.\nThe theory also provides a way to calculate standard errors for the estimates of the \\(\\beta\\)s.\n\n\n\n23.3.2 Fitting the model\n\nTo obtain the maximum likelihood estimates using R we can use the glm function with the family argument set to binomial. This defaults to using the logit transformation.\nWe do not have the individual level data, but because we our model assumes the probability of success is the same for all women and all men, then the number of success can be modeled as binomial with \\(N_1\\) trials and probability \\(p_1\\) for women and binomial with \\(N_0\\) trials and probability \\(p_0\\) for men, with \\(N_1\\) and \\(N_0\\) the total number of women and men.\nIn this case the glm function is used like this:\n\n\nsuccess &lt;- with(totals, c(yes_men, yes_women))\nfailure &lt;- with(totals, c(no_men, no_women))\ngender &lt;- factor(c(\"men\", \"women\"))\nfit &lt;- glm(cbind(success, failure) ~ gender, family = \"binomial\") \ncoefficients(summary(fit))\n\n              Estimate Std. Error    z value      Pr(&gt;|z|)\n(Intercept) -1.5342684 0.06474387 -23.697508 3.825454e-124\ngenderwomen -0.2082771 0.10407015  -2.001315  4.535850e-02\n\n\n\nThe estimate of the odds ratio is 0.811982 which is interpreted as the odds being lowered by 20% for women as compared to men.\nBut is this due to chance?\nWe already noted that the p-value is about 0.05, but the GLM approach also permits us to compute confidence intervals using the confint function:\n\n\nexp(confint(fit, 2))\n\n    2.5 %    97.5 % \n0.6613153 0.9946601 \n\n\n\n\n\n\n\n\nNote\n\n\n\nWe have used a simple version of GLMs in which the only variable is binary. However, the method can be expanded to use multiple variables including continuous ones. However, in these contexts the log odds ratio interpretation becomes more complex. Also note that we have shown just one version of GLM appropriate for binomial data using a logit transformation. This version is referred to often referred to as logistic regression. However, GLM can be used with other transformation and distributions. You can learn more by consulting a GLM text book.\n\n\n\n\n23.3.3 Simple standard error approximation for two-by-two table odds ratio\n\nUsing glm we can obtain estimates, standard errors, and confidence intervals for a wide range of models. To do this we use a rather complex algorithms. In the case of two-by-two tables we can obtain a standard error for the log odds ratio using a simple approximation.\nIf our two-by-two tables has the following entries:\n\n\n\n\n\n\n\nMen\nWomen\n\n\n\n\nAwarded\na\nb\n\n\nNot Awarded\nc\nd\n\n\n\n\n\n\n\n\nIn this case, the odds ratio is simply\n\n\\[\n\\frac{a/c}{b/d} = \\frac{ad}{bc}\n\\].\n\nWe can confirm we obtain the same estimate as when using glm:\n\n\nor &lt;- with(two_by_two, women[2]/sum(women) / (women[1]/sum(women)) / ((men[2]/sum(men)) / (men[1]/sum(men))))\nc(log(or), fit$coef[2])\n\n            genderwomen \n -0.2082771  -0.2082771 \n\n\n\nStatistical theory tells us that when all four entries of the two-by-two table are large enough, then the log odds ratio is approximately normal with standard error\n\n\\[\n\\sqrt{1/a + 1/b + 1/c + 1/d}\n\\]\n\nThis implies that a 95% confidence interval for the log odds ratio can be formed by:\n\n\\[\n\\log\\left(\\frac{ad}{bc}\\right) \\pm 1.96 \\sqrt{1/a + 1/b + 1/c + 1/d}\n\\]\n\nBy exponentiating these two numbers we can construct a confidence interval of the odds ratio.\nUsing R we can compute this confidence interval as follows:\n\n\nse &lt;- two_by_two |&gt; select(-awarded) |&gt;\n  summarize(se = sqrt(sum(1/men) + sum(1/women))) |&gt;\n  pull(se)\nexp(log(or) + c(-1,1) * qnorm(0.975) * se)\n\n[1] 0.6621581 0.9957060\n\n\n\nNote that 1 is not included in the confidence interval which must mean that the p-value is smaller than 0.05. We can confirm this using:\n\n\n2*(1 - pnorm(abs(log(or)), 0, se))\n\n[1] 0.0453586\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that the p-values obtained with chisq.test, glm and this simple approximation are all slightly different. This is because these are both based on different approximation approaches."
  },
  {
    "objectID": "23-association-tests.html#large-samples-small-p-values",
    "href": "23-association-tests.html#large-samples-small-p-values",
    "title": "23  Association tests",
    "section": "23.4 Large samples, small p-values",
    "text": "23.4 Large samples, small p-values\n\nAs mentioned earlier, reporting only p-values is not an appropriate way to report the results of data analysis.\nIn scientific journals, for example, some studies seem to overemphasize p-values.\nSome of these studies have large sample sizes and report impressively small p-values.\nYet when one looks closely at the results, we realize odds ratios are quite modest: barely bigger than 1.\nIn this case the difference may not be practically significant or scientifically significant.\nThe relationship between odds ratio and p-value is not one-to-one.\nIt depends on the sample size. So a very small p-value does not necessarily mean a very large odds ratio.\nNotice what happens to the p-value if we multiply our two-by-two table by 10, which does not change the odds ratio:\n\n\ntwo_by_two_x_10 &lt;- two_by_two |&gt; \n  select(-awarded) |&gt;\n  mutate(men = men*10, women = women*10) \nchisq.test(two_by_two_x_10)$p.value\n\n[1] 2.625423e-10\n\n\n:::{.callout-note title = “Small count correction”} Note that the log odds ratio is not defined if any of the cells of the two-by-two table is 0. This is because if \\(a\\), \\(b\\), \\(c\\), or \\(d\\) is 0, the \\(\\log(\\frac{ad}{bc})\\) is either the log of 0 or has a 0 in the denominator. For this situation, it is common practice to avoid 0s by adding 0.5 to each cell. This is referred to as the Haldane–Anscombe correction and has been shown, both in practice and theory, to work well. :::"
  },
  {
    "objectID": "23-association-tests.html#exercises",
    "href": "23-association-tests.html#exercises",
    "title": "23  Association tests",
    "section": "23.5 Exercises",
    "text": "23.5 Exercises\n\nA famous athlete has an impressive career, winning 70% of her 500 career matches. However, this athlete gets criticized because in important events, such as the Olympics, she has a losing record of 8 wins and 9 losses. Perform a Chi-square test to determine if this losing record can be simply due to chance as opposed to not performing well under pressure.\n\n\nmat &lt;- matrix(c(500*.3, 500*.7, 9,8), 2, 2, byrow = TRUE)\nchisq.test(mat, correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  mat\nX-squared = 4.0631, df = 1, p-value = 0.04383\n\n\n\nWhy did we use the Chi-square test instead of Fisher’s exact test in the previous exercise?\n\n\nIt actually does not matter, since they give the exact same p-value.\nFisher’s exact and the Chi-square are different names for the same test.\nBecause the sum of the rows and columns of the two-by-two table are not fixed so the hypergeometric distribution is not an appropriate assumption for the null hypothesis. For this reason, Fisher’s exact test is rarely applicable with observational data.\nBecause the Chi-square test runs faster.\n\nAnswer: c\n\nCompute the odds ratio of “losing under pressure” along with a confidence interval.\n\n\nlor &lt;- log((mat[1,1]*mat[2,2])/(mat[1,2]*mat[2,1]))\nse &lt;- sqrt(sum(1/mat))\nexp(lor + c(-1,1)*1.96*se)\n\n[1] 0.1442096 1.0063459\n\n2*pnorm(-abs(lor/se))\n\n[1] 0.05150641\n\n\n\nNotice that p-values obtained with the four approaches that produce three different p-values:\n\n\nchisq.test(mat)$p.value\n\n[1] 0.08037602\n\nchisq.test(mat, correct = FALSE)$p.value\n\n[1] 0.0438292\n\n2*pnorm(-abs(lor/se))\n\n[1] 0.05150641\n\nsummary(glm(mat ~ c(1,0), family = \"binomial\"))$coef[2,4]\n\n[1] 0.0515064\n\n\nWhy is this?\n\nWe made a mistake in our code.\nThese are based on t-statistics so the connection between p-value and confidence intervals does not apply.\nDifferent approximations are used for the p-value and the confidence interval calculation. If we had a larger sample size the match would be better.\nWe should use the Fisher exact test to get confidence intervals.\n\nAnswer: c\n\nMultiply the two-by-two table by 2 and see if the p-value and confidence retrieval are a better match."
  },
  {
    "objectID": "23-association-tests.html#optional-exercises-wont-appear-in-the-midterm",
    "href": "23-association-tests.html#optional-exercises-wont-appear-in-the-midterm",
    "title": "23  Association tests",
    "section": "23.6 Optional exercises (won’t appear in the midterm)",
    "text": "23.6 Optional exercises (won’t appear in the midterm)\n\nWhen analyzing the trump tweets we computed sentiment_counts like this:\n\n\nlibrary(tidytext)\nlinks &lt;- \"https://t.co/[A-Za-z\\\\d]+|&amp;\"\nnrc &lt;- get_sentiments(\"nrc\") |&gt; select(word, sentiment)\nandroid_iphone &lt;- trump_tweets |&gt; \n  extract(source, \"source\", \"Twitter for (.*)\") |&gt;\n  filter(source %in% c(\"Android\", \"iPhone\") &\n           created_at &gt;= ymd(\"2015-06-17\") & \n           created_at &lt; ymd(\"2016-11-08\")) |&gt;\n  filter(!is_retweet) |&gt;\n  arrange(created_at) |&gt; \n  mutate(text = str_replace_all(text, links, \"\"))  |&gt;\n  unnest_tokens(word, text) |&gt;\n  filter(!word %in% stop_words$word &\n           !str_detect(word, \"^\\\\d+$\")) |&gt;\n  mutate(word = str_replace(word, \"^'\", \"\")) |&gt;\n  filter(!word %in% stop_words$word)\n\nsentiment_counts &lt;- android_iphone |&gt;\n  left_join(nrc, by = \"word\", relationship = \"many-to-many\") |&gt;\n  count(source, sentiment) |&gt;\n  pivot_wider(names_from = \"source\", values_from = \"n\") |&gt;\n  mutate(sentiment = replace_na(sentiment, replace = \"none\"))\n\nCompute an odds ratio comparing Android to iPhone for each sentiment and add it to the table.\n\nCompute a 95% confidence interval for each odds ratio.\nGenerate a plot showing the estimated odds ratios along with their confidence intervals.\nTest the null hypothesis that there is no difference between tweets from Android and iPhone and report the sentiments with p-values less than 0.05 and more likely to come from Android.\nFor each sentiment, find the words assigned to that sentiment, keep words that appear at least 25 times, compute the odd ratio for each, and show a barplot for those with odds ratio larger than 2 or smaller than 1/2."
  },
  {
    "objectID": "23-association-tests.html#footnotes",
    "href": "23-association-tests.html#footnotes",
    "title": "23  Association tests",
    "section": "",
    "text": "http://www.pnas.org/content/112/40/12349.abstract↩︎"
  },
  {
    "objectID": "24-corrleation-not-causation.html#spurious-correlation",
    "href": "24-corrleation-not-causation.html#spurious-correlation",
    "title": "24  Association is not causation",
    "section": "24.1 Spurious correlation",
    "text": "24.1 Spurious correlation\nThe following comical example underscores that correlation is not causation. It shows a very strong correlation between divorce rates and margarine consumption.\n\n\n\n\n\nDoes this mean that margarine causes divorces? Or do divorces cause people to eat more margarine? Of course the answer to both these questions is no. This is just an example of what we call a spurious correlation.\nYou can see many more absurd examples on the Spurious Correlations website1.\nThe cases presented in the spurious correlation site are all instances of what is generally called data dredging, data fishing, or data snooping. It’s basically a form of what in the US they call cherry picking. An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend.\nA Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables. We will save the results of our simulation into a tibble:\n\nlibrary(tidyverse)\nN &lt;- 25\ng &lt;- 1000000\nsim_data &lt;- tibble(group = rep(1:g, each = N), \n                   x = rnorm(N*g), \n                   y = rnorm(N*g))\n\nThe first column denotes group. We created groups and for each one we generated a pair of independent vectors, \\(X\\) and \\(Y\\), with 25 observations each, stored in the second and third columns. Because we constructed the simulation, we know that \\(X\\) and \\(Y\\) are not correlated.\nNext, we compute the correlation between X and Y for each group and look at the max:\n\nres &lt;- sim_data |&gt; \n  group_by(group) |&gt; \n  summarize(r = cor(x, y)) |&gt; \n  arrange(desc(r))\nres\n\n# A tibble: 1,000,000 × 2\n    group     r\n    &lt;int&gt; &lt;dbl&gt;\n 1 250839 0.812\n 2 117437 0.783\n 3 711908 0.776\n 4 667042 0.775\n 5 953055 0.773\n 6 146868 0.760\n 7 529114 0.758\n 8 477932 0.751\n 9 135819 0.748\n10 189486 0.742\n# ℹ 999,990 more rows\n\n\nWe see a maximum correlation of 0.8118926 and if you just plot the data from the group achieving this correlation, it shows a convincing plot that \\(X\\) and \\(Y\\) are in fact correlated:\n\nsim_data |&gt; filter(group == res$group[which.max(res$r)]) |&gt;\n  ggplot(aes(x, y)) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nRemember that the correlation summary is a random variable. Here is the distribution generated by the Monte Carlo simulation:\n\nres |&gt; ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = \"black\")\n\n\n\n\nIt’s just a mathematical fact that if we observe random correlations that are expected to be 0, but have a standard error of 0.2042353, the largest one will be close to 1.\nIf we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:\n\nlibrary(broom)\nsim_data |&gt; \n  filter(group == res$group[which.max(res$r)]) |&gt;\n  summarize(tidy(lm(y ~ x))) |&gt; \n  filter(term == \"x\")\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic     p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 x        0.991     0.149      6.67 0.000000836\n\n\nThis particular form of data dredging is referred to as p-hacking. P-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results. In epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and several exposures and report only the one exposure that resulted in a small p-value. Furthermore, they might try fitting several different models to account for confounding and pick the one that yields the smallest p-value. In experimental disciplines, an experiment might be repeated more than once, yet only the results of the one experiment with a small p-value reported. This does not necessarily happen due to unethical behavior, but rather as a result of statistical ignorance or wishful thinking. In advanced statistics courses, you can learn methods to adjust for these multiple comparisons."
  },
  {
    "objectID": "24-corrleation-not-causation.html#outliers",
    "href": "24-corrleation-not-causation.html#outliers",
    "title": "24  Association is not causation",
    "section": "24.2 Outliers",
    "text": "24.2 Outliers\nSuppose we take measurements from two independent outcomes, \\(X\\) and \\(Y\\), and we standardize the measurements. However, imagine we make a mistake and forget to standardize entry 23. We can simulate such data using:\n\nset.seed(1985)\nx &lt;- rnorm(100,100,1)\ny &lt;- rnorm(100,84,1)\nx[-23] &lt;- scale(x[-23])\ny[-23] &lt;- scale(y[-23])\n\nThe data look like this:\n\nqplot(x, y)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nNot surprisingly, the correlation is very high:\n\ncor(x,y)\n\n[1] 0.9878382\n\n\nBut this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, which is what it should be:\n\ncor(x[-23], y[-23])\n\n[1] -0.04419032\n\n\nThere is an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called Spearman correlation. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other:\n\nqplot(rank(x), rank(y))\n\n\n\n\nThe outlier is no longer associated with a very large value and the correlation comes way down:\n\ncor(rank(x), rank(y))\n\n[1] 0.002508251\n\n\nSpearman correlation can also be calculated like this:\n\ncor(x, y, method = \"spearman\")\n\n[1] 0.002508251\n\n\nThere are also methods for robust fitting of linear models which you can learn about in, for instance, this book: Robust Statistics: Edition 2 by Peter J. Huber & Elvezio M. Ronchetti."
  },
  {
    "objectID": "24-corrleation-not-causation.html#reversing-cause-and-effect",
    "href": "24-corrleation-not-causation.html#reversing-cause-and-effect",
    "title": "24  Association is not causation",
    "section": "24.3 Reversing cause and effect",
    "text": "24.3 Reversing cause and effect\nAnother way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around.\nA form of this claim actually made it into an op-ed in the New York Times titled Parental Involvement Is Overrated2. Consider this quote from the article:\n\n\nWhen we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades… Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse.\n\n\nA very likely possibility is that the children needing regular parental help, receive this help because they don’t perform well in school.\nWe can easily construct an example of cause and effect reversal using the father and son height data. If we fit the model:\n\\[X_i = \\beta_0 + \\beta_1 y_i + \\varepsilon_i, i=1, \\dots, N\\]\nto the father and son height data, with \\(X_i\\) the father height and \\(y_i\\) the son height, we do get a statistically significant result. We use the galton_heights dataset defined in Chapter Chapter 20:\n\ngalton_heights |&gt; summarize(tidy(lm(father ~ son)))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   40.9      4.40        9.29 5.47e-17\n2 son            0.407    0.0636      6.40 1.36e- 9\n\n\nThe model fits the data very well. If we look at the mathematical formulation of the model above, it could easily be incorrectly interpreted so as to suggest that the son being tall caused the father to be tall. But given what we know about genetics and biology, we know it’s the other way around. The model is technically correct. The estimates and p-values were obtained correctly as well. What is wrong here is the interpretation."
  },
  {
    "objectID": "24-corrleation-not-causation.html#confounders",
    "href": "24-corrleation-not-causation.html#confounders",
    "title": "24  Association is not causation",
    "section": "24.4 Confounders",
    "text": "24.4 Confounders\nConfounders are perhaps the most common reason that leads to associations begin misinterpreted.\nIf \\(X\\) and \\(Y\\) are correlated, we call \\(Z\\) a confounder if changes in \\(Z\\) causes changes in both \\(X\\) and \\(Y\\). Earlier, when studying baseball data, we saw how Home Runs was a confounder that resulted in a higher correlation than expected when studying the relationship between Bases on Balls and Runs. In some cases, we can use linear models to account for confounders. However, this is not always the case.\nIncorrect interpretation due to confounders is ubiquitous in the lay press and they are often hard to detect. Here, we present a widely used example related to college admissions.\n\n24.4.1 Example: UC Berkeley admissions\nAdmission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O’Connell. Science (1975). We can load the data and  a statistical test, which clearly rejects the hypothesis that gender and admission are independent:\n\ntwo_by_two &lt;- admissions |&gt; group_by(gender) |&gt; \n  summarize(total_admitted = round(sum(admitted / 100 * applicants)), \n            not_admitted = sum(applicants) - sum(total_admitted)) |&gt;\n  select(-gender) \n  \nchisq.test(two_by_two)$p.value\n\n[1] 1.055797e-21\n\n\nBut closer inspection shows a paradoxical result. Here are the percent admissions by major:\n\nadmissions |&gt; select(major, gender, admitted) |&gt;\n  pivot_wider(names_from = \"gender\", values_from = \"admitted\") |&gt;\n  mutate(women_minus_men = women - men)\n\n# A tibble: 6 × 4\n  major   men women women_minus_men\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n1 A        62    82              20\n2 B        63    68               5\n3 C        37    34              -3\n4 D        33    35               2\n5 E        28    24              -4\n6 F         6     7               1\n\n\nFour out of the six majors favor women. More importantly, all the differences are much smaller than the 14.2 difference that we see when examining the totals.\nThe paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear. What’s going on? This actually can happen if an uncounted confounder is driving most of the variability.\nSo let’s define three variables: \\(X\\) is 1 for men and 0 for women, \\(Y\\) is 1 for those admitted and 0 otherwise, and \\(Z\\) quantifies the selectivity of the major. A gender bias claim would be based on the fact that \\(\\mbox{Pr}(Y=1 | X = x)\\) is higher for \\(x=1\\) than \\(x=0\\). However, \\(Z\\) is an important confounder to consider. Clearly \\(Z\\) is associated with \\(Y\\), as the more selective a major, the lower \\(\\mbox{Pr}(Y=1 | Z = z)\\). But is major selectivity \\(Z\\) associated with gender \\(X\\)?\nOne way to see this is to plot the total percent admitted to a major versus the percent of women that made up the applicants:\n\nadmissions |&gt; \n  group_by(major) |&gt; \n  summarize(major_selectivity = sum(admitted * applicants)/sum(applicants),\n            percent_women_applicants = sum(applicants * (gender==\"women\")) /\n                                             sum(applicants) * 100) |&gt;\n  ggplot(aes(major_selectivity, percent_women_applicants, label = major)) +\n  geom_text()\n\n\n\n\nThere seems to be association. The plot suggests that women were much more likely to apply to the two “hard” majors: gender and major’s selectivity are confounded. Compare, for example, major B and major E. Major E is much harder to enter than major B and over 60% of applicants to major E were women, while less than 30% of the applicants of major B were women.\n\n\n24.4.2 Confounding explained graphically\nThe following plot shows the number of applicants that were admitted and those that were not by:\n\n\n\n\n\nIt also breaks down the acceptances by major. This breakdown allows us to see that the majority of accepted men came from two majors: A and B. It also lets us see that few women applied to these majors.\n\n\n24.4.3 Average after stratifying\nIn this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away:\n\nadmissions |&gt; \n  ggplot(aes(major, admitted, col = gender, size = applicants)) +\n  geom_point()\n\n\n\n\nNow we see that major by major, there is not much difference. The size of the dot represents the number of applicants, and explains the paradox: we see large red dots and small blue dots for the easiest majors, A and B.\nIf we average the difference by major, we find that the percent is actually 3.5% higher for women.\n\nadmissions |&gt;  group_by(gender) |&gt; summarize(average = mean(admitted))\n\n# A tibble: 2 × 2\n  gender average\n  &lt;chr&gt;    &lt;dbl&gt;\n1 men       38.2\n2 women     41.7"
  },
  {
    "objectID": "24-corrleation-not-causation.html#simpsons-paradox",
    "href": "24-corrleation-not-causation.html#simpsons-paradox",
    "title": "24  Association is not causation",
    "section": "24.5 Simpson’s paradox",
    "text": "24.5 Simpson’s paradox\nThe case we have just covered is an example of Simpson’s paradox. It is called a paradox because we see the sign of the correlation flip when comparing the entire publication and specific strata. As an illustrative example, suppose you have three random variables \\(X\\), \\(Y\\), and \\(Z\\) and that we observe realizations of these. Here is a plot of simulated observations for \\(X\\) and \\(Y\\) along with the sample correlation:\n\n\n\n\n\nYou can see that \\(X\\) and \\(Y\\) are negatively correlated. However, once we stratify by \\(Z\\) (shown in different colors below) another pattern emerges:\n\n\n\n\n\nIt is really \\(Z\\) that is negatively correlated with \\(X\\). If we stratify by \\(Z\\), the \\(X\\) and \\(Y\\) are actually positively correlated as seen in the plot above."
  },
  {
    "objectID": "24-corrleation-not-causation.html#exercises",
    "href": "24-corrleation-not-causation.html#exercises",
    "title": "24  Association is not causation",
    "section": "24.6 Exercises",
    "text": "24.6 Exercises\nFor the next set of exercises, we examine the data from a 2014 PNAS paper3 that analyzed success rates from funding agencies in the Netherlands and concluded:\n\nOur results reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language used in instructional and evaluation materials.\n\nA response4 was published a few months later titled No evidence that gender contributes to personal research funding success in The Netherlands: A reaction to Van der Lee and Ellemers which concluded:\n\nHowever, the overall gender effect borders on statistical significance, despite the large sample. Moreover, their conclusion could be a prime example of Simpson’s paradox; if a higher percentage of women apply for grants in more competitive scientific disciplines (i.e., with low application success rates for both men and women), then an analysis across all disciplines could incorrectly show “evidence” of gender inequality.\n\nWho is right here? The original paper or the response? Here, you will examine the data and come to your own conclusion.\n\nThe main evidence for the conclusion of the original paper comes down to a comparison of the percentages. Table S1 in the paper includes the information we need:\n\n\nlibrary(dslabs)\nresearch_funding_rates\n\nConstruct the two-by-two table used for the conclusion about differences in awards by gender.\n\nCompute the difference in percentage from the two-by-two table.\nIn the previous exercise, we noticed that the success rate is lower for women. But is it significant? Compute a p-value using a Chi-square test.\nWe see that the p-value is about 0.05. So there appears to be some evidence of an association. But can we infer causation here? Is gender bias causing this observed difference? The response to the original paper claims that what we see here is similar to the UC Berkeley admissions example. Specifically they state that this “could be a prime example of Simpson’s paradox; if a higher percentage of women apply for grants in more competitive scientific disciplines, then an analysis across all disciplines could incorrectly show ‘evidence’ of gender inequality.” To settle this dispute, create a dataset with number of applications, awards, and success rate for each gender. Re-order the disciplines by their overall success rate. Hint: use the reorder function to re-order the disciplines in a first step, then use pivot_longer, separate, and pivot_wider to create the desired table.\nTo check if this is a case of Simpson’s paradox, plot the success rates versus disciplines, which have been ordered by overall success, with colors to denote the genders and size to denote the number of applications.\nWe definitely do not see the same level of confounding as in the UC Berkeley example. It is hard to say there is a confounder here. However, we do see that, based on the observed rates, some fields favor men and others favor women and we do see that the two fields with the largest difference favoring men are also the fields with the most applications. But, unlike the UC Berkeley example, women are not more likely to apply for the more challenging fields. So perhaps some of the selection committees are biased and others are not.\n\nBut, before we conclude this, we must check if these differences are any different than what we get by chance. Are any of the differences seen above statistically significant? Keep in mind that even when there is no bias, we will see differences due to random variability in the review process as well as random variability across candidates. Perform a Chi-square test for each discipline. Hint: define a function that receives the total of a two-by-two table and returns a data frame with the p-value. Use the 0.5 correction. Then use the summarize function.\n\nFor the medical sciences, there appears to be a statistically significant difference. But is this a spurious correlation? We performed 9 tests. Reporting only the one case with a p-value less than 0.05 might be considered an example of cherry picking. Repeat the exercise above, but instead of a p-value, compute a log odds ratio divided by their standard error. Then use qq-plot to see how much these log odds ratios deviate from the normal distribution we would expect: a standard normal distribution."
  },
  {
    "objectID": "24-corrleation-not-causation.html#footnotes",
    "href": "24-corrleation-not-causation.html#footnotes",
    "title": "24  Association is not causation",
    "section": "",
    "text": "http://tylervigen.com/spurious-correlations↩︎\nhttps://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated↩︎\nhttp://www.pnas.org/content/112/40/12349.abstract↩︎\nhttp://www.pnas.org/content/112/51/E7036.extract↩︎"
  },
  {
    "objectID": "25-matrices-in-R.html#sec-mnist",
    "href": "25-matrices-in-R.html#sec-mnist",
    "title": "25  Matrices in R",
    "section": "25.1 Case study: MNIST",
    "text": "25.1 Case study: MNIST\nAn example comes from handwritten digits. The first step in handling mail received in the post office is sorting letters by zip code:\n\nThese are the digitized images:\n\n\n\n\n\n\nThe images are converted into \\(28 \\times 28 = 784\\) pixels\nFor each pixel, we obtain a grey scale intensity between 0 (white) and 255 (black).\n\n\n\n\n\n\nFor each digitized image, indexed by \\(i\\), we are provided 784 variables and a categorical outcome, or label, representing which digit among \\(0, 1, 2, 3, 4, 5, 6, 7 , 8,\\) and \\(9\\) the image is representing. Let’s load the data using the dslabs package:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nif (!exists(\"mnist\")) mnist &lt;- read_mnist()\n\nIn these cases, the pixel intensities are saved in a matrix:\n\nclass(mnist$train$images)\n\n[1] \"matrix\" \"array\" \n\n\n\nThis matrix represents 60,000 observations, each a digit.\nFor example, let’s take a smaller subset:\n\n\nx &lt;- mnist$train$images[1:300,] \ny &lt;- mnist$train$labels[1:300]"
  },
  {
    "objectID": "25-matrices-in-R.html#sec-matrix-notation",
    "href": "25-matrices-in-R.html#sec-matrix-notation",
    "title": "25  Matrices in R",
    "section": "25.2 Mathematical notation",
    "text": "25.2 Mathematical notation\n\nWhen working with linear algebra in R we have three types of objects:\n\n\nscalars,\nvectors, and\nmatrices.\n\n\nA scalar is just one number, for example \\(a = 1\\).\nVectors are like the numeric vectors we define in R:\n\n\nlength(x[20,])\n\n[1] 784\n\n\n\nEach feature is represented by a columns of x. For example, the first column contains the values for the first pixel of all 1,000 images:\n\n\nlength(x[,1])\n\n[1] 300\n\n\nIn matrix algebra, we use lower case bold letters to represent a vector of features/predictors/covariates:\n\\[\n\\mathbf{x} =\n\\begin{pmatrix}\nx_1\\\\\\\nx_2\\\\\\\n\\vdots\\\\\\\nx_n\n\\end{pmatrix}\n\\]\nSimilarly, we can use math notation to represent different features by adding an index:\n\\[\n\\mathbf{x}_1 = \\begin{pmatrix}\nx_{1,1}\\\\\n\\vdots\\\\\nx_{n,1}\n\\end{pmatrix} \\mbox{ and }\n\\mathbf{x}_2 = \\begin{pmatrix}\nx_{1,2}\\\\\n\\vdots\\\\\nx_{n,2}\n\\end{pmatrix}\n\\]\n\nA matrix can be defined as a series of vectors of the same size joined together as columns:\n\n\nx_1 &lt;- 1:5\nx_2 &lt;- 6:10\ncbind(x_1, x_2)\n\n     x_1 x_2\n[1,]   1   6\n[2,]   2   7\n[3,]   3   8\n[4,]   4   9\n[5,]   5  10\n\n\n\nMathematically, we represent them with bold upper case letters:\n\n\\[\n\\mathbf{X} = ( \\mathbf{x}_1 \\, \\mathbf{x}_2 ) = \\begin{pmatrix}\nx_{1,1}&x_{1,2}\\\\\n\\vdots&\\vdots\\\\\nx_{n,1}&x_{n,2}\n\\end{pmatrix}\n\\]\n\nWe can use this notation to denote an arbitrary number of predictors with the following \\(n\\times p\\) matrix, for example, with \\(n = 300\\), and \\(p=784\\):\n\n\\[\n\\mathbf{X} =\n\\begin{pmatrix}\n  x_{1,1}&x_{1,2}&\\dots & x_{1,p} \\\\\n  x_{2,1}&x_{2,2}&\\dots & x_{2,p} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\\\\n  x_{n,1}&x_{n,2}&\\dots & x_{n,p}\n  \\end{pmatrix}\n\\]\n\nThe dimension of a matrix is often an important characteristic needed to assure that certain operations can be performed.\n\n\ndim(x)\n\n[1] 300 784\n\n\n\n\n\n\n\n\nNotation for rows versus columns\n\n\n\nBold lower case letter are also commonly used to represent rows, rather than columns, of a matrix. This can be confusing because \\(\\mathbf{x}_1\\) can represent either the first row or the first column. One way to distinguish then is using notation similar to computer code by using the colon \\(:\\) to represent all. So \\(\\mathbf{X}_{1,:}\\) is a row, the first row and all the columns, and \\(\\mathbf{X}_{:,1}\\) is a column, the first column and all the rows. Another approach is to distinguish by the index, with \\(i\\) used for rows and \\(j\\) used for columns. So \\(\\mathbf{x}_i\\) is the \\(i\\)th row and \\(\\mathbf{x}_j\\) is the \\(j\\)th column. With this approach it is important to clarify which dimension, row or column, is being represented. We use this last one in the next chapter."
  },
  {
    "objectID": "25-matrices-in-R.html#converting-vectors-to-a-matrices",
    "href": "25-matrices-in-R.html#converting-vectors-to-a-matrices",
    "title": "25  Matrices in R",
    "section": "25.3 Converting vectors to a matrices",
    "text": "25.3 Converting vectors to a matrices\n\nVectors can be thought of as \\(n\\times 1\\) matrices. However, in R, a vector does not have dimensions:\n\n\ndim(x_1)\n\nNULL\n\n\n\nVectors are not matrices in R. However, we can easily convert then to a matrix:\n\n\ndim(matrix(x_1))\n\n[1] 5 1\n\n\n\nIt is also possible to change the dimensions of the resulting matrix.\nTo see an example of how can this be useful, consider wanting to visualize the the rows pixel intensities in their original \\(28\\times28\\) grid.\n\n\nmy_vector &lt;- 1:15\nmat &lt;- matrix(my_vector, 5, 3)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    6   11\n[2,]    2    7   12\n[3,]    3    8   13\n[4,]    4    9   14\n[5,]    5   10   15\n\n\n\nWe can fill by row by using the byrow argument:\n\n\nmat_t &lt;- matrix(my_vector, 3, 5, byrow = TRUE)\nmat_t\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n[3,]   11   12   13   14   15"
  },
  {
    "objectID": "25-matrices-in-R.html#motivating-questions",
    "href": "25-matrices-in-R.html#motivating-questions",
    "title": "25  Matrices in R",
    "section": "25.4 Motivating questions",
    "text": "25.4 Motivating questions\nTo motivate the use of matrices in R, we will pose five questions/challenges related to the handwritten digits data:\n1. Do some digits require more ink to write than others? We will study the distribution of the total pixel darkness and how it varies by digits.\n2. Are some pixels uninformative? We will study the variation of each pixel across digits and remove predictors (columns) associated with pixels that don’t change much and thus can’t provide much information for classification.\n3. Can we remove smudges? We will first, look at the distribution of all pixel values. Then we will use this to pick a cutoff to define unwritten space. Then, set anything below that cutoff to 0.\n4. Binarize the data. First, we will look at the distribution of all pixel values. We will then use this to pick a cutoff to distinguish between writing and no writing. Then, we will convert all entries into either 1 or 0.\n5. Standardize the digits. We will scale each of the predictors in each entry to have the same average and standard deviation.\n\n\n\n\n\n\nWarning\n\n\n\nThe matrix function recycles values in the vector without warning if the product of columns and rows does not match the length of the vector:\n\nmatrix(1:3, 2, 5)\n\nWarning in matrix(1:3, 2, 5): data length [3] is not a sub-multiple or multiple\nof the number of rows [2]\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    2    1    3\n[2,]    2    1    3    2    1\n\n\n\n\n\nTo put the pixel intensities of our, say, 3rd entry, which is a 4 into grid, we can use:\n\n\ngrid &lt;- matrix(x[3,], 28, 28)\n\n\nConfirm with plots\n\n\nimage(1:28, 1:28, grid)\nimage(1:28, 1:28, grid[, 28:1])"
  },
  {
    "objectID": "25-matrices-in-R.html#row-and-column-summaries",
    "href": "25-matrices-in-R.html#row-and-column-summaries",
    "title": "25  Matrices in R",
    "section": "25.5 Row and column summaries",
    "text": "25.5 Row and column summaries\n\nThe function rowSums takes a matrix as input and computes the desired values:\n\n\nsums &lt;- rowSums(x)\n\n\nWe can also compute the averages with rowMeans\n\n\navg &lt;- rowMeans(x)\n\n\nFor the first task, look at boxplots:\n\n\nboxplot(avg ~ y)\n\n\n\n\n\nIs this expected?\nWe can compute the column sums and averages using the function colSums and colMeans, respectively.\n\nThe matrixStats package adds functions that performs operations on each row or column very efficiently, including the functions rowSds and colSds."
  },
  {
    "objectID": "25-matrices-in-R.html#apply",
    "href": "25-matrices-in-R.html#apply",
    "title": "25  Matrices in R",
    "section": "25.6 apply",
    "text": "25.6 apply\n\nThe apply function lets you apply any function, not just sum or mean, to the rows or columns of a matrix.\nThe first argument is the matrix, the second is the dimension, 1 for rows, 2 for columns, and the third is the function.\nSo, for example, rowMeans can be written as:\n\n\navgs &lt;- apply(x, 1, mean)\n\n\nfor the sds:\n\n\nsds &lt;- apply(x, 2, sd)\n\n\nThe trade off for this flexibility is that these operations are not as fast as dedicated functions such as rowMeans."
  },
  {
    "objectID": "25-matrices-in-R.html#filtering-columns-based-on-summaries",
    "href": "25-matrices-in-R.html#filtering-columns-based-on-summaries",
    "title": "25  Matrices in R",
    "section": "25.7 Filtering columns based on summaries",
    "text": "25.7 Filtering columns based on summaries\n\nWe now turn to task 2: studying the variation of each pixel and removing columns associated with pixels that don’t change much and thus do not inform the classification.\n\n\nlibrary(matrixStats)\n\n\nAttaching package: 'matrixStats'\n\n\nThe following object is masked from 'package:dplyr':\n\n    count\n\nsds &lt;- colSds(x)\n\n\nA quick look at the distribution of these values shows that some pixels have very low entry to entry variability:\n\n\nhist(sds, breaks = 30, main = \"SDs\")\n\n\n\n\n\nTo see where the low variance pixels are:\n\n\nimage(1:28, 1:28, matrix(sds, 28, 28)[, 28:1])\n\n\n\n\n\nWe see that there is little variation in the corners.\nWe could remove features that have no variation since these can’t help us predict. We can extract columns from matrices using the following code:\n\n\nx[ ,c(351,352)]\n\nand rows like this:\n\nx[c(2,3),]\n\n\nWe can also use logical indexes to determine which columns or rows to keep.\n\n\nnew_x &lt;- x[,colSds(x) &gt; 60]\ndim(new_x)\n\n[1] 300 316\n\n\n\nOnly the columns for which the standard deviation is above 60 are kept, which removes over half the predictors.\nHere we add an important warning related to subsetting matrices: if you select one column or one row, the result is no longer a matrix but a vector.\n\n\nclass(x[, 1])\n\n[1] \"integer\"\n\ndim(x[,1 ])\n\nNULL\n\n\n\nHowever, we can preserve the matrix class by using the argument drop=FALSE:\n\n\nclass(x[, 1, drop = FALSE])\n\n[1] \"matrix\" \"array\" \n\ndim(x[, 1, drop = FALSE])\n\n[1] 300   1"
  },
  {
    "objectID": "25-matrices-in-R.html#indexing-with-matrices",
    "href": "25-matrices-in-R.html#indexing-with-matrices",
    "title": "25  Matrices in R",
    "section": "25.8 Indexing with matrices",
    "text": "25.8 Indexing with matrices\n\nWe can turn matrices into vectors.\n\n\nmat &lt;- matrix(1:15, 5, 3)\nas.vector(mat)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\n\nTo see a histogram of all our predictor data, we can use:\n\nhist(as.vector(x), breaks = 30, main = \"Pixel intensities\")\n\n\n\n\n\nWe notice a clear dichotomy which is explained as parts of the image with ink and parts without. If we think that values below, say, 50 are smudges, we can quickly make them zero using:\n\n\nnew_x &lt;- x\nnew_x[new_x &lt; 50] &lt;- 0\n\n\nTo see what this does, we look at a smaller matrix:\n\n\nmat &lt;- matrix(1:15, 5, 3)\nmat[mat &lt; 9] &lt;- 0\nmat\n\n     [,1] [,2] [,3]\n[1,]    0    0   11\n[2,]    0    0   12\n[3,]    0    0   13\n[4,]    0    9   14\n[5,]    0   10   15\n\n\n\nWe can also use logical operations with matrix logical:\n\n\nmat &lt;- matrix(1:15, 5, 3)\nmat[mat &gt; 6 & mat &lt; 12] &lt;- 0\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    6    0\n[2,]    2    0   12\n[3,]    3    0   13\n[4,]    4    0   14\n[5,]    5    0   15"
  },
  {
    "objectID": "25-matrices-in-R.html#binarizing-the-data",
    "href": "25-matrices-in-R.html#binarizing-the-data",
    "title": "25  Matrices in R",
    "section": "25.9 Binarizing the data",
    "text": "25.9 Binarizing the data\nThe histogram above seems to suggest that this data is mostly binary. A pixel either has ink or does not. Using what we have learned, we can binarize the data using just matrix operations:\n\nbin_x &lt;- x\nbin_x[bin_x &lt; 255/2] &lt;- 0 \nbin_x[bin_x &gt; 255/2] &lt;- 1\n\n\nWe can also convert to a matrix of logicals and then coerce to numbers like this:\n\n\nbin_X &lt;- (x &gt; 255/2)*1\nimage(matrix(x[3,], 28, 28)[,28:1])\n\n\n\nimage(matrix(bin_X[3,], 28, 28)[,28:1])"
  },
  {
    "objectID": "25-matrices-in-R.html#vectorization-for-matrices",
    "href": "25-matrices-in-R.html#vectorization-for-matrices",
    "title": "25  Matrices in R",
    "section": "25.10 Vectorization for matrices",
    "text": "25.10 Vectorization for matrices\nIn R, if we subtract a vector from a matrix, the first element of the vector is subtracted from the first row, the second element from the second row, and so on. Using mathematical notation, we would write it as follows:\n\\[\n\\begin{pmatrix}\n  X_{1,1}&\\dots & X_{1,p} \\\\\n  X_{2,1}&\\dots & X_{2,p} \\\\\n   & \\vdots & \\\\\n  X_{n,1}&\\dots & X_{n,p}\n  \\end{pmatrix}\n-\n\\begin{pmatrix}\na_1\\\\\\\na_2\\\\\\\n\\vdots\\\\\\\na_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  X_{1,1}-a_1&\\dots & X_{1,p} -a_1\\\\\n  X_{2,1}-a_2&\\dots & X_{2,p} -a_2\\\\\n   & \\vdots & \\\\\n  X_{n,1}-a_n&\\dots & X_{n,p} -a_n\n  \\end{pmatrix}\n\\]\n\nThe same holds true for other arithmetic operations. This implies that we can scale each row of a matrix like this:\n\n\n(x - rowMeans(x)) / rowSds(x)\n\n\nIf you want to scale each column, be careful since this approach does not work for columns. To perform a similar operation, we convert the columns to rows using the transpose t, proceed as above, and then transpose back:\n\n\nt(t(X) - colMeans(X))\n\n\nWe can also use a function called sweep that works similarly to apply. It takes each entry of a vector and subtracts it from the corresponding row or column.\n\n\nX_mean_0 &lt;- sweep(x, 2, colMeans(x))\n\n\nThe function sweep actually has another argument that lets you define the arithmetic operation. So to divide by the standard deviation, we do the following:\n\n\nx_mean_0 &lt;- sweep(x, 2, colMeans(x))\nx_standardized &lt;- sweep(x_mean_0, 2, colSds(x), FUN = \"/\")\n\n\nIn R, if you add, subtract, nultiple or divide two matrices, the operation is done elementwise. For example, if two matrices are stored in x and y, then\n\n\nx*y\n\ndoes not result in matrix multiplication. Instead, the entry in row \\(i\\) and column \\(j\\) of this product is the product of the entryin row \\(i\\) and column \\(j\\) of x and y, respectively."
  },
  {
    "objectID": "25-matrices-in-R.html#exercises",
    "href": "25-matrices-in-R.html#exercises",
    "title": "25  Matrices in R",
    "section": "25.11 Exercises",
    "text": "25.11 Exercises\n\nCreate a 100 by 10 matrix of randomly generated normal numbers. Put the result in x.\n\n\nx &lt;- matrix(rnorm(100*10), nrow = 100, ncol = 10)\n\n\nApply the three R functions that give you the dimension of x, the number of rows of x, and the number of columns of x, respectively.\n\n\ndim(x)\n\n[1] 100  10\n\nnrow(x)\n\n[1] 100\n\nncol(x)\n\n[1] 10\n\n\n\nAdd the scalar 1 to row 1, the scalar 2 to row 2, and so on, to the matrix x.\n\n\n#x &lt;- sweep(x, 1, 1:nrow(x), FUN = \"+\")\nx &lt;- x + 1:nrow(x)\n\n\nAdd the scalar 1 to column 1, the scalar 2 to column 2, and so on, to the matrix x. Hint: use sweep with FUN = \"+\".\n\n\nx &lt;- sweep(x, 2, 1:ncol(x), FUN = \"+\")\n\n\nCompute the average of each row of x.\n\n\nrowMeans(x)\n\n  [1]   6.477288   7.367261   8.181667   9.660570  11.028052  11.544575\n  [7]  12.441120  13.039651  14.019369  16.046838  16.242200  17.047185\n [13]  18.309173  19.905317  20.500210  21.835051  22.260542  23.135782\n [19]  24.460199  25.879866  26.754392  27.425336  28.415541  29.511094\n [25]  30.541871  31.116494  32.499914  33.209959  35.490908  35.815862\n [31]  36.802065  37.400740  38.001311  39.800111  40.443514  41.449242\n [37]  42.537396  43.532097  45.056531  45.166026  46.242811  47.645917\n [43]  48.993353  49.992419  50.854520  51.925613  52.211193  53.560072\n [49]  54.211283  55.803137  56.178404  58.024011  59.075901  59.633904\n [55]  60.205026  61.060169  62.424013  63.335907  64.611784  65.543338\n [61]  66.695806  67.349716  68.624622  69.045593  70.186725  71.419169\n [67]  71.869221  73.213363  74.176837  75.266510  76.443212  77.719628\n [73]  78.784417  79.850226  80.681503  81.482931  82.950850  83.593832\n [79]  85.062871  85.921158  86.279011  87.316271  88.245355  88.840018\n [85]  90.640814  92.010270  92.204402  93.022158  94.872883  95.490450\n [91]  96.877535  97.841359  98.208877  99.693700 100.201428 101.793013\n [97] 102.555899 103.488157 104.236679 105.676375\n\n\n\nCompute the average of each column of x.\n\n\ncolMeans(x)\n\n [1] 51.41577 52.54656 53.46062 54.62697 55.54851 56.62446 57.60971 58.37201\n [9] 59.45493 60.51226\n\n\n\nFor each digit in the MNIST training data, compute the proportion of pixels that are in a grey area, defined as values between 50 and 205. Make boxplot by digit class. Hint: use logical operators and rowMeans.\n\n\nga &lt;- rowMeans(mnist$train$images &gt; 50 & mnist$train$images &lt; 205)\nboxplot(ga ~ mnist$train$labels)"
  },
  {
    "objectID": "26-linear-algebra.html#transpose-of-a-matrix",
    "href": "26-linear-algebra.html#transpose-of-a-matrix",
    "title": "26  Applied Linear Algebra",
    "section": "26.1 Transpose of a matrix",
    "text": "26.1 Transpose of a matrix\nIf\n\\[\n\\mathbf{X} =\n\\begin{pmatrix}\n  x_{1,1}&x_{1,2}&\\dots & x_{1,p} \\\\\n  x_{2,1}&x_{2,2}&\\dots & x_{2,p} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\\\\n  x_{n,1}&x_{n,2}&\\dots & x_{n,p}\n  \\end{pmatrix}\n\\]\nThe transponse\n\\[\n\\mathbf{X}^\\top =\n\\begin{pmatrix}\n  x_{1,1}&x_{2,1}&\\dots & x_{n,1} \\\\\n  x_{1,2}&x_{2,2}&\\dots & x_{n,2} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\\\\n  x_{1,p}&x_{2,p}&\\dots & x_{n,p}\n  \\end{pmatrix}\n\\]\nIf we are writing out a column, such as \\(\\mathbf{x}_1\\) defined above, in a sentence we often use the notation: \\(\\mathbf{x}_1 = ( x_{1,1}, \\dots x_{n,1})^\\top\\) to avoid wasting vertical space in the text.\nIn R we compute the transpose using the function t\n\ndim(x)\n\n[1] 300 784\n\ndim(t(x))\n\n[1] 784 300"
  },
  {
    "objectID": "26-linear-algebra.html#matrix-multiplication",
    "href": "26-linear-algebra.html#matrix-multiplication",
    "title": "26  Applied Linear Algebra",
    "section": "26.2 Matrix multiplication",
    "text": "26.2 Matrix multiplication\nLinear algebra was born from mathematicians developing systematic ways to solve systems of linear equations, for example\n\\[\n\\begin{align}\nx +  3 y  - 2 z  &= 5\\\\\n3x + 5y + 6z &= 7\\\\\n2x + 4y + 3z &= 8.\n\\end{align}\n\\]\nTo explain matrix multiplication, define two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\na_{11}&a_{12}&\\dots&a_{1n}\\\\\na_{21}&a_{22}&\\dots&a_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{m1}&a_{2}&\\dots&a_{mn}\n\\end{pmatrix}, \\,\n\\mathbf{B} = \\begin{pmatrix}\nb_{11}&b_{12}&\\dots&b_{1p}\\\\\nb_{21}&b_{22}&\\dots&b_{2p}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nb_{n1}&b_{n2}&\\dots&b_{np}\n\\end{pmatrix}\n\\]\nand define the product of matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) as the matrix \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\) that has entries \\(c_{ij}\\) equal to the sum of the component-wise product of the \\(i\\)th row of \\(\\mathbf{A}\\) with the \\(j\\)th column of \\(\\mathbf{B}\\). Using R code we can define \\(\\mathbf{C}= \\mathbf{A}\\mathbf{B}\\) as follows:\n\nm &lt;- nrow(A)\np &lt;- ncol(B)\nC &lt;- matrix(0, m, p)\nfor(i in 1:m){\n  for(j in 1:p){\n    C[i,j] &lt;- sum(A[i,] * B[,j])\n  }\n}\n\nBecause this operation is so common, R includes a mathematical operator %*% for matrix multiplication:\n\nC &lt;- A %*% B\n\nUsing mathematical notation \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\) looks like this:\n\\[\n\\begin{pmatrix}\na_{11}b_{11} + \\dots + a_{1n}b_{n1}&\na_{11}b_{12} + \\dots + a_{1n}b_{n2}&\n\\dots&\na_{11}b_{1p} + \\dots + a_{1n}b_{np}\\\\\na_{21}b_{11} + \\dots + a_{2n}b_{n1}&\na_{21}b_{n2} + \\dots + a_{2n}b_{n2}&\n\\dots&\na_{21}b_{1p} + \\dots + a_{2n}b_{np}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{m1}b_{11} + \\dots +a_{mn}b_{n1}&\na_{m1}b_{n2} + \\dots + a_{mn}b_{n2}&\n\\dots&\na_{m1}b_{1p} + \\dots + a_{mn}b_{np}\\\\\n\\end{pmatrix}\n\\]\nNote this definition implies that the multiplication \\(\\mathbf{A}\\mathbf{B}\\) is only possible when the number of rows of \\(\\mathbf{A}\\) matches the number of columns of \\(\\mathbf{B}\\).\nSo how does this definition of matrix multiplication help solve systems of equations? First, any system of equations with unknowns \\(x_1, \\dots x_n\\)\n\\[\n\\begin{align}\na_{11} x_1 + a_{12} x_2 \\dots + a_{1n}x_n &= b_1\\\\\na_{21} x_1 + a_{22} x_2 \\dots + a_{2n}x_n &= b_2\\\\\n\\vdots\\\\\na_{n1} x_1 + a_{n2} x_2 \\dots + a_{nn}x_n &= b_n\\\\\n\\end{align}\n\\]\ncan now be represented as matrix multiplication by defining the following matrices:\n\\[\n\\mathbf{A} =\\begin{pmatrix}\na_{11}&a_{12}&\\dots&a_{1n}\\\\\na_{21}&a_{22}&\\dots&a_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{m1}&a_{22}&\\dots&a_{nn}\n\\end{pmatrix}\n,\\,\n\\mathbf{b} =\n\\begin{pmatrix}\nb_1\\\\\nb_2\\\\\n\\vdots\\\\\nb_n\n\\end{pmatrix}\n,\\, \\mbox{ and }\n\\mathbf{x} =\n\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\n\\vdots\\\\\nx_n\n\\end{pmatrix}\n\\]\nand rewriting the equation simply as\n\\[\n\\mathbf{A}\\mathbf{x} =  \\mathbf{b}\n\\]\nThe linear algebra algorithms listed above, such as Gaussian elimination, provide a way to compute the inverse matrix \\(A^{-1}\\) that solves the equation for \\(\\mathbf{x}\\):\n\\[\n\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{x} =   \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\n\\]\nTo solve the first equation we wrote out in R, we can use the function solve:\n\nA &lt;- matrix(c(1, 3, -2, 3, 5, 6, 2, 4, 3), 3, 3, byrow = TRUE)\nb &lt;- matrix(c(5, 7, 8))\nsolve(A, b)"
  },
  {
    "objectID": "26-linear-algebra.html#the-identity-matrix",
    "href": "26-linear-algebra.html#the-identity-matrix",
    "title": "26  Applied Linear Algebra",
    "section": "26.3 The identity matrix",
    "text": "26.3 The identity matrix\nThe identity matrix, represented with a bold I, is like the number 1, but for matrices: if you multiply a matrix by the identity matrix, you get back the matrix.\n\\[\n\\mathbf{I}\\mathbf{x} = \\mathbf{x}\n\\]\nIf you do some math with the definition of matrix multiplication you will realize that \\(\\mathbf{I}\\) is a matrix with the same number of rows and columns (refereed to as square matrix) with 0s everywhere except the diagonal:\n\\[\n\\mathbf{1}=\\begin{pmatrix}\n1&0&\\dots&0\\\\\n0&1&\\dots&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n0&0&\\dots&1\n\\end{pmatrix}\n\\]\nIt also implies that due to the definition of an inverse matrix we have\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{1}\n\\]\nBecause the default for the second argument in solve is an identity matrix, if we simply type solve(A), we obtain the inverse \\(\\mathbf{A}^{-1}\\). This means we can also obtain a solution to our system of equations with:\n\nsolve(A) %*% b"
  },
  {
    "objectID": "26-linear-algebra.html#distance",
    "href": "26-linear-algebra.html#distance",
    "title": "26  Applied Linear Algebra",
    "section": "26.4 Distance",
    "text": "26.4 Distance\nTo define distance, we introduce another linear algebra concept: the norm. Recall that a point in two dimensions can represented in polar coordinates as:\n\n\n\n\n\nwith \\(\\theta = \\arctan{\\frac{x2}{x1}}\\) and \\(r = \\sqrt{x_1^2 + x_2^2}\\).\n\nIf we think of the point as two dimensional vector \\(\\mathbf{x} = (x_1, x_2)^\\top\\), \\(r\\) defines the norm of \\(\\mathbf{x}\\).\nThe norm can be thought of as the size of the two-dimensional vector disregarding the direction: if we change the angle, the vector changes but the size does not.\nThe point of defining the norm is that we can extrapolated the concept of size to higher dimensions.\nSpecifically, we write the norm for any vector \\(\\mathbf{x}\\) as:\n\n\\[\n||\\mathbf{x}|| = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2}\n\\]\n\nWe can use the linear algebra concepts we have learned to define the norm like this:\n\n\\[\n||\\mathbf{x}||^2 = \\mathbf{x}^\\top\\mathbf{x}\n\\]\n\nTo define distance, suppose we have two two-dimensional points \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\). We can define how similar they are by simply using euclidean distance.\n\n\n\n\n\n\n\nWe know that the distance is equal to the length of the hypotenuse:\n\n\\[\n\\sqrt{(x_{11} - x_{12})^2 + (x_{21} - x_{22})^2}\n\\]\n\nThe reason we introduced the norm is because this distance is the size of the vector between the two points and this can be extrapolated to any dimension.\nThe distance between two points, regardless of the dimensions, is defined as the norm of the difference:\n\n\\[\n|| \\mathbf{x}_1 - \\mathbf{x}_2||.\n\\]\n\nIf we use the digit data, the distance between the first and second observation will compute distance using all 784 features:\n\n\\[\n|| \\mathbf{x}_1 - \\mathbf{x}_2 || = \\sqrt{ \\sum_{j=1}^{784} (x_{1,j}-x_{2,j })^2 }\n\\]\n\nTo demonstrate, let’s pick the features for three digits:\n\n\nx_1 &lt;- x[6,]\nx_2 &lt;- x[17,]\nx_3 &lt;- x[16,]\n\n\nWe can compute the distances between each pair using the definitions we just learned:\n\n\nc(sum((x_1 - x_2)^2), sum((x_1 - x_3)^2), sum((x_2 - x_3)^2)) |&gt; sqrt()\n\n[1] 2319.867 2331.210 2518.969\n\n\n\nIn R, the function crossprod(x) is convenient for computing norms it multiplies t(x) by x\n\n\nc(crossprod(x_1 - x_2), crossprod(x_1 - x_3), crossprod(x_2 - x_3)) |&gt; sqrt()\n\n[1] 2319.867 2331.210 2518.969\n\n\n\nNote crossprod takes a matrix as the first argument and therefore the vectors used here are being coerced into single column matrices.\nAlso note that crossprod(x,y) multiples t(x) by y.\nWe can see that the distance is smaller between the first two.\nThis is in agreement with the fact that the first two are 2s and the third is a 7.\n\n\ny[c(6, 17, 16)]\n\n[1] 2 2 7\n\n\n\nWe can also compute all the distances at once relatively quickly using the function dist, which computes the distance between each row and produces an object of class dist:\n\n\nd &lt;- dist(x)\nclass(d)\n\n[1] \"dist\"\n\n\n\nTo access the entries using row and column indices, we need to coerce it into a matrix.\nWe can see the distance we calculated above like this:\n\n\nas.matrix(d)[c(6, 17, 16), c(6, 17, 16)]\n\n          6       17       16\n6     0.000 2319.867 2331.210\n17 2319.867    0.000 2518.969\n16 2331.210 2518.969    0.000\n\n\n\nWe can quickly see an image of these distances using this code:\n\n\nimage(as.matrix(d))\n\n\nIf we order this distance by the labels, we can see yellowish squares near the diagonal.\nThis is because observations from the same digits tend to be closer than to different digits:\n\n\nimage(as.matrix(d)[order(y), order(y)])"
  },
  {
    "objectID": "26-linear-algebra.html#sec-predictor-space",
    "href": "26-linear-algebra.html#sec-predictor-space",
    "title": "26  Applied Linear Algebra",
    "section": "26.5 Spaces",
    "text": "26.5 Spaces\n\nPredictor space is a concept that is often used to describe machine learning algorithms.\nThe term space refers to an advanced mathematical definition for which we provide a simplified explanation to help understand the term predictor space when used in the context of machine learning algorithms.\nWe can think of all predictors \\((x_{i,1}, \\dots, x_{i,p})^\\top\\) for all observations \\(i=1,\\dots,n\\) as \\(n\\) \\(p\\)-dimensional points.\nA space can be thought of as the collection of all possible points that should be considered for the data analysis in question.\nThis includes points we could see, but have not been observed yet.\nIn the case of the handwritten digits, we can think of the predictor space as any point \\((x_{1}, \\dots, x_{p})^\\top\\) as long as each entry \\(x_i, i = 1, \\dots, p\\) is between 0 and 255.\nSome Machine Learning algorithms also define subspaces.\nA common approach is to define neighborhoods of points that are close to a center.\nWe can do this by selecting a center \\(\\mathbf{x}_0\\), a minimum distance \\(r\\), and defining the subspace as the collection of points \\(\\mathbf{x}\\) that satisfy\n\n\\[\n|| \\mathbf{x} - \\mathbf{x}_0 || \\leq r.\n\\]\n\nWe can think of this a multidimensional sphere since, as in a circle or sphere, every point is the same distance away from the center.\nOther machine learning algorithms partition the predictor space into non-overlapping regions and then make different predictions for each region using the data in the region."
  },
  {
    "objectID": "26-linear-algebra.html#exercises",
    "href": "26-linear-algebra.html#exercises",
    "title": "26  Applied Linear Algebra",
    "section": "26.6 Exercises",
    "text": "26.6 Exercises\n\nGenerate two matrix, A and B, containing randomly generated and normally distributed numbers. The dimensions of these two matrices should \\(4 \\times 3\\) and \\(3 \\times 6\\), respectively. Confirm that C &lt;- A %*% B produce the same results as:\n\n\nm &lt;- nrow(A)\np &lt;- ncol(B)\nC &lt;- matrix(0, m, p)\nfor(i in 1:m){\n  for(j in 1:p){\n    C[i,j] &lt;- sum(A[i,] * B[,j])\n  }\n}\n\n\nSolve the following system of equations using R:\n\n\\[\n\\begin{align}\nx + y + z + w &= 10\\\\\n2x + 3y - z - w &= 5\\\\\n3x - y + 4z - 2w &= 15\\\\\n2x + 2y - 2z - 2w &= 20\\\\\n\\end{align}\n\\]\n\nDefine x\n\n\nmnist &lt;- read_mnist()\nx &lt;- mnist$train$images[1:300,] \ny &lt;- mnist$train$labels[1:300]\n\nand compute the distance matrix\n\nd &lt;- dist(x)\nclass(d)\n\nGenerate a boxplot showing the distances for the second row of d stratified by digits. Do not include the distance to itself which we know it is 0. Can you predict what digit is represented by the second row of x?\n\nUse the apply function and matrix algebra to compute the distance between the fourth digit mnist$train$images[4,] and all other digits represented in mnist$train$images. Then generate as boxplot as in exercise 2 and predict what digit is the fourth row.\n\n\nmy_dist &lt;- apply(mnist$train$image[-4,], 1, function(x){\n  sqrt(sum((mnist$train$image[4,] - x)^2))\n})\nboxplot(my_dist~mnist$train$labels[-4])\n\n\n\n\n\nCompute the distance between each feature and the feature representing the middle pixel (row 14 column 14). Create an image plot of where the distance is shown with color in the pixel position.\n\n\ni &lt;- 13*28+14\nx_0 &lt;- mnist$train$images[,i]\n\n# my_dist &lt;- apply(mnist$train$image, 2, function(x){\n#   sqrt(sum((x - x_0)^2))\n# })\n\nmy_dist &lt;- sqrt(matrix(x_0, nrow=1) %*% mnist$train$images)\n\n#check if it's 0\nmy_dist[i]\n\n[1] 35683.37\n\nmat &lt;- matrix(my_dist, 28, 28)\nimage(mat[,28:1])"
  },
  {
    "objectID": "27-dimension-reduction.html#motivation-preserving-distance",
    "href": "27-dimension-reduction.html#motivation-preserving-distance",
    "title": "27  Dimension reduction",
    "section": "27.1 Motivation: preserving distance",
    "text": "27.1 Motivation: preserving distance\n\nWe simulated heights of pairs of twins, some adults some children.\nWe use the mvrnorm function from the MASS package to simulate bivariate normal data.\n\n\nset.seed(1983)\nlibrary(MASS)\nn &lt;- 100\nSigma &lt;- matrix(c(9, 9 * 0.9, 9 * 0.9, 9), 2, 2)\nx &lt;- rbind(mvrnorm(n / 2, c(69, 69), Sigma),\n           mvrnorm(n / 2, c(60, 60), Sigma))\n\n\nA scatterplot quickly reveals that the correlation is high and that there are two groups of twins, the adults (upper right points) and the children (lower left points):\n\n\n\n\n\n\n\nWe want to explore the data through a histogram of a one-dimensional variable.\nWe want to reduce the dimensions from two to one, but still be able to understand important characteristics of the data, for example that the observations cluster into two groups: adults and children.\nTo show the ideas presented here are generally useful, we will standardize the data so that observations are in standard units rather than inches:\n\n\nx &lt;- scale(x)\n\nIn the figure above we show the distance between observation 1 and 2 (blue), and observation 1 and 51 (red). Note that the blue line is shorter, which implies 1 and 2 are closer.\nWe can compute these distances using dist:\n\nd &lt;- dist(x)\nas.matrix(d)[1, 2]\n\n[1] 0.5949407\n\nas.matrix(d)[2, 51]\n\n[1] 1.388275\n\n\nThis distance is based on two dimensions and we need a distance approximation based on just one.\nLet’s start with the naive approach of simply removing one of the two dimensions. Let’s compare the actual distances to the distance computed with just the first dimension:\n\nz &lt;- x[,1]\n\nTo make the distances comparable, we divide the sum of squares by the number of dimensions. So for the two dimensional case we have\n\\[\n\\sqrt{ \\frac{1}{2} \\sum_{j=1}^2 (x_{1,j}-x_{2,j})^2 },\n\\]\nso we divide the distance by \\(\\sqrt{2}\\):\n\nplot(dist(x) / sqrt(2), dist(z))\nabline(0, 1, col = \"red\")\n\n\n\n\n\nBased on the plot, this one number summary does ok at preserving distances, but, can we pick a one-dimensional summary that makes this one-number approximation even better?"
  },
  {
    "objectID": "27-dimension-reduction.html#rotations",
    "href": "27-dimension-reduction.html#rotations",
    "title": "27  Dimension reduction",
    "section": "27.2 Rotations",
    "text": "27.2 Rotations\nAny two-dimensional point \\((X_1, X_2)\\) can be written as the base and height of a triangle with a hypotenuse going from \\((0,0)\\) to \\((X_1, X_2)\\):\n\\[\nx_1 = r \\cos\\phi, \\,\\, x_2 = r \\sin\\phi\n\\]\nwith \\(r\\) the length of the hypothenus and \\(\\phi\\) the angel between the hypotenuse and the x-axis.\nWe can rotate the point \\((x_1, x_2)\\) around a circle with center \\((0,0)\\) and radius \\(r\\) by an angle \\(\\theta\\) by changing the angle in the previous equation to \\(\\phi + \\theta\\):\n\\[\nz_1 = r \\cos(\\phi+ \\theta), \\,\\,\nz_2 = r \\sin(\\phi + \\theta)\n\\]\n\n\n\n\n\nWe can use trigonometric identities to rewrite \\((z_1, z_2)\\) in the following way:\n\\[\n\\begin{align}\nz_1 = r \\cos(\\phi + \\theta) = r \\cos \\phi \\cos\\theta -  r \\sin\\phi \\sin\\theta =  x_1 \\cos(\\theta) -  x_2 \\sin(\\theta)\\\\\nz_2 = r \\sin(\\phi + \\theta) =  r \\cos\\phi \\sin\\theta + r \\sin\\phi \\cos\\theta =  x_1 \\sin(\\theta) + x_2 \\cos(\\theta)\n\\end{align}\n\\]\n\nNow we can rotate each point in the dataset by simply applying the formula above to each pair \\((x_{i,1}, x_{i,2})\\).\nHere is what the twin standardized heights look like after rotating each point by \\(-45\\) degrees:\n\n\n\n\n\n\n\nNote that while the variability of \\(x_1\\) and \\(x_2\\) are similar, the variability of \\(z_1\\) is much larger than the variability of \\(z_2\\).\nAlso note that the distances between points appear to be preserved."
  },
  {
    "objectID": "27-dimension-reduction.html#linear-transformations",
    "href": "27-dimension-reduction.html#linear-transformations",
    "title": "27  Dimension reduction",
    "section": "27.3 Linear transformations",
    "text": "27.3 Linear transformations\n\nNote that each row of \\(\\mathbf{X}\\) was transformed using a linear transformation.\nFor any row \\(i\\), the first entry was:\n\n\\[z_{i,1} = a_{11} x_{i,1} + a_{21} x_{i,2}\\]\nwith \\(a_{11} = \\cos\\theta\\) and \\(a_{21} = -\\sin\\theta\\).\n\nThe second entry was also a linear transformation:\n\n\\[z_{i,2} = a_{12} x_{i,1} + a_{22} x_{i,2}\\]\nwith \\(a_{12} = \\sin\\theta\\) and \\(a_{22} = \\cos\\theta\\).\n\nWe can also use linear transformations to get \\(X\\) back from \\(Z\\).\nSolving the system of two linear equations gives us:\n\n\\[x_{i,1} = b_{1,1} z_{i,1} + b_{2,1} z_{i,2}\\]\nwith \\(b_{1,2} = \\cos\\theta\\) and \\(b_{2,1} = \\sin\\theta\\)\nand\n\\[x_{i,2} = b_{2,1} z_{i,1} + b_{2,2} z_{i,2}\\]\nwith \\(b_{2,1} = -\\sin\\theta\\) and \\(b_{1,2} = \\cos\\theta\\).\n\nUsing linear algebra, we can write the operation we just performed like this:\n\n\\[\n\\begin{pmatrix}\nz_1&z_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx_1&x_2\n\\end{pmatrix}\n\\begin{pmatrix}\na_{11}&a_{12}\\\\\na_{21}&a_{22}\n\\end{pmatrix}\n\\]\n\nAn advantage of using linear algebra is that we can write the transformation for the entire dataset by representing it as a \\(N \\times 2\\) matrix \\(X\\), with each row holding the two values for a pair of twins, and the rotation as a linear transformation of \\(X\\):\n\n\\[\n\\mathbf{Z} = \\mathbf{X} \\mathbf{A}\n\\mbox{ with }\n\\mathbf{A} = \\,\n\\begin{pmatrix}\na_{11}&a_{12}\\\\\na_{21}&a_{22}\n\\end{pmatrix}.\n\\]\n\nThis transformation results in an \\(N \\times 2\\) matrix, denoted here with \\(Z\\), with the rotated points in each row.\nAnother advantage of linear algebra is that we can rotate back by simply multiplying \\(Z\\) by the inverse matrix:\n\n\\[\n\\mathbf{Z} \\mathbf{A}^{-1} = \\mathbf{X} \\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{X}\n\\].\n\nThis implies that all the information in \\(\\mathbf{X}\\) is included in the rotation \\(\\mathbf{Z}\\), and it can be retrieved via a linear transformation.\nThese derivations imply that we can use the following code to rotate the data by any angle \\(\\theta\\):\n\n\nrotate &lt;- function(x, theta){\n  theta &lt;- theta/360 * 2 * pi # convert to radians\n  A &lt;- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2, 2)\n  x %*% A\n}\n\n\nWe can use this to confirm that for any rotation the distances are preserved. Here are two examples:\n\n\nmax(dist(rotate(x, -45)) - dist(x))\n\n[1] 8.881784e-16\n\nmax(dist(rotate(x, 30)) - dist(x))\n\n[1] 8.881784e-16"
  },
  {
    "objectID": "27-dimension-reduction.html#orthogonal-transformations",
    "href": "27-dimension-reduction.html#orthogonal-transformations",
    "title": "27  Dimension reduction",
    "section": "27.4 Orthogonal transformations",
    "text": "27.4 Orthogonal transformations\n\nRecall that the distance between two points, say rows \\(h\\) and \\(i\\) of the transformation \\(\\mathbf{Z}\\), can be written like this:\n\n\\[\n||\\mathbf{z}^\\top_h - \\mathbf{z}^\\top_i|| = (\\mathbf{z}_h - \\mathbf{z}_i)(\\mathbf{z}^\\top_h - \\mathbf{z}^\\top_i)\n\\]\nwith \\(\\mathbf{z}_h\\) and \\(\\mathbf{z}_i\\) the \\(h\\)-th and \\(i\\)-th \\(1 \\times p\\) rows, with \\(p=2\\) in our specific example.\n\nUsing linear algebra, we can rewrite this quantity as\n\n\\[\n||\\mathbf{z}^\\top_h - \\mathbf{z}^\\top_i||^2 =  ||\\mathbf{A}^\\top\\mathbf{x}^\\top_h - \\mathbf{A}^\\top\\mathbf{x}^\\top_i||^2 = (\\mathbf{x}_h - \\mathbf{x}_i) \\mathbf{A} \\mathbf{A}^\\top (\\mathbf{x}^\\top_h - \\mathbf{x}^\\top_i)\n\\]\n\nNote that if \\(\\mathbf{A} \\mathbf{A}^\\top = \\mathbf{I}\\) then the distance between the \\(h\\)th and \\(i\\)th rows is the same for the original and transformed data.\nWe to transformation with the property \\(\\mathbf{A} \\mathbf{A}^\\top = \\mathbf{I}\\) as orthogonal transformations and they are guaranteed to preserves the distance between any two points.\nNotice that \\(\\mathbf{A}\\) being orthogonal also guarantees that the total sum of squares of any row in \\(\\mathbf{X}\\) is preserved after the transformation:\n\n\\[\n||\\mathbf{z}^\\top_i||^2 =  ||\\mathbf{x}^\\top_i\\mathbf{A}||^2 = \\mathbf{x}_i \\mathbf{A}\\mathbf{A}^\\top  \\mathbf{x}^\\top_i =  \\mathbf{x}_i   \\mathbf{x}^\\top_i = ||\\mathbf{x}^\\top_i||^2\n\\]\n\nOur transformation has this property:\n\n\ntheta &lt;- 30 # works for any theta\nA &lt;- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2, 2)\nA %*% t(A)\n\n              [,1]          [,2]\n[1,]  1.000000e+00 -1.283172e-17\n[2,] -1.283172e-17  1.000000e+00\n\n\n\nWe can confirm this by computing the total variation for x and z:\n\n\nz &lt;- rotate(x, -45)\nsum(colSums(x^2))\n\n[1] 198\n\nsum(colSums(z^2))\n\n[1] 198\n\n\n\nThis can be interpreted as consequence of the fact that orthogonal transformation guarantee that all the information is preserved."
  },
  {
    "objectID": "27-dimension-reduction.html#sec-pca",
    "href": "27-dimension-reduction.html#sec-pca",
    "title": "27  Dimension reduction",
    "section": "27.5 Principal Component Analysis (PCA)",
    "text": "27.5 Principal Component Analysis (PCA)\n\nNow how does this all relate to our goal of finding a one-dimensional summary that approximates distance between points?\nNote that in our original data the variability of the two dimensions is the same:\n\n\ncolSums(x^2)\n\n[1] 99 99\n\n\n\nBut after a rotations this is no longer true:\n\n\ncolSums(z^2)\n\n[1] 194.986266   3.013734\n\n\n\nWith z, the proportion of the variability included in the first dimension is much higher.\nWe can search for the rotation that maximizes the proportion of the variability included in the first dimension:\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::select() masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\nangles &lt;- seq(0, -90)\nv &lt;- sapply(angles, function(angle) colSums(rotate(x, angle)^2))\nvariance_explained &lt;- v[1,] / (v[1,] + v[2,])\nplot(angles, variance_explained)\n\n\nWe see that the variability included in the first dimension is maximized at about -45 degrees.\nBecuase almost all the variation is explained by this first dimension, with this particular rotation the distance between points in \\(X\\) can be very well approximated by just the first dimension of \\(Z\\), much better than with the first dimension of \\(X\\):\n\n\n\n\n\n\n\nWe also notice that the two groups, adults and children, can be clearly observed with the one number summary:\n\n\nhist(z[,1], nclass = 15)\n\n\n\n\nBelow, we learn that the rotation that maximizes the standard deviation of the first dimention z[,1] gives us the first principal component of the matrix x.\n\n\nIn general, dimension reduction can be described as applying a rotation \\(A\\) to a matrix with many columns \\(X\\) that moves the information contained in \\(\\mathbf{X}\\) to the first few columns of \\(\\mathbf{Z}=\\mathbf{X}\\mathbf{A}\\).\nThen keeping just these few informative columns, reduces the dimension of the vectors contained in the rows.\nIn our simplistic example we reduced the dimensions from 2 to 1, but the ideas extend to higher dimensions.\nIn general, the first principal component (PC) of a matrix \\(\\mathbf{X}\\) is the linear orthogonal transformation that maximizes the variability of the first dimension.\nThe function prcomp finds this transformation:\n\n\npca &lt;- prcomp(x)\npca$rotation\n\n            PC1        PC2\n[1,] -0.7071068  0.7071068\n[2,] -0.7071068 -0.7071068\n\n\n\nNote that the first PC is almost the same as that provided by the \\(\\cos(-45^{\\circ}), \\sin(-45^{\\circ})\\) we used earlier.\nThe function prcomp returns the rotation needed to transform \\(\\mathbf{X}\\) so that the variability of the columns is decreasing from most variable to least (accessed with $rotation) as well as the resulting new matrix (accessed with $x).\nBy default the columns of \\(\\mathbf{X}\\) are first centered (to change this change the center argument to FALSE).\nSo, using the matrix multiplication shown above, we have that the following are the same (demonstrated by a difference between elements of essentially zero):\n\n\nsweep(x, 2, colMeans(x))\npca$x %*% t(pca$rotation)\n\n\nThe rotation is orthogonal which means that the inverse is its transpose.\nSo we also have that these two are identical:\n\n\nsweep(x, 2, colMeans(x)) %*% pca$rotation\npca$x\n\n\nWe can visualize these to see how the first component summarizes the data. In the plot below red represents high values and blue negative values, we learn why we call these weights and patterns:\n\n\n\n\n\n\n\nIt turns out that we can find this linear transformation not just for two dimensions but for matrices of any dimension \\(p\\).\nFor a multidimensional matrix with \\(p\\) columns, we can find a transformation that preserves distance between rows, but with the variance of the columns in decreasing order.\nThe second column is the second principal component, the third column is the third principal component, and so on.\nAs in our example, if after a certain number of columns, say \\(k\\), the variances of the columns of \\(\\mathbf{Z}_j\\), \\(j&gt;k\\) are very small, it means these dimensions have little to contribute to the distance and we can approximate distance between any two points with just \\(k\\) dimensions.\nIf \\(k\\) is much smaller than \\(p\\), then we can achieve a very efficient summary of our data."
  },
  {
    "objectID": "27-dimension-reduction.html#iris-example",
    "href": "27-dimension-reduction.html#iris-example",
    "title": "27  Dimension reduction",
    "section": "27.6 Iris example",
    "text": "27.6 Iris example\n\nThe iris data is a widely used example in data analysis courses. It includes four botanical measurements related to three flower species:\n\n\nnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n\n\nIf you print iris$Species you will see that the data is ordered by the species.\nLet’s compute the distance between each observation.\nYou can clearly see the three species with one species very different from the other two:\n\n\nx &lt;- iris[,1:4] |&gt; as.matrix()\nd &lt;- dist(x)\nimage(as.matrix(d), col = rev(RColorBrewer::brewer.pal(9, \"RdBu\")))\n\n\n\n\n\nOur predictors here have four dimensions, but three are very correlated:\n\n\ncor(x)\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\n\nIf we apply PCA, we should be able to approximate this distance with just two dimensions, compressing the highly correlated dimensions.\nUsing the summary function we can see the variability explained by each PC:\n\n\npca &lt;- prcomp(x)\nsummary(pca)\n\nImportance of components:\n                          PC1     PC2    PC3     PC4\nStandard deviation     2.0563 0.49262 0.2797 0.15439\nProportion of Variance 0.9246 0.05307 0.0171 0.00521\nCumulative Proportion  0.9246 0.97769 0.9948 1.00000\n\n\n\nThe first two dimensions account for 97% of the variability.\nThus we should be able to approximate the distance very well with two dimensions.\nWe can visualize the results of PCA:\n\n\n\n\n\n\n\nAnd see that the first pattern is sepal length, petal length, and petal width (red) in one direction and sepal width (blue) in the other.\nThe second pattern is the sepal length and petal width in one direction (blue) and petal length and petal width in the other (red).\nYou can see from the weights that the first PC1 drives most of the variability and it clearly separates the first third of samples (setosa) from the second two thirds (versicolor and virginica).\nIf you look at the second column of the weights, you notice that it somewhat separates versicolor (red) from virginica (blue).\nWe can see this better by plotting the first two PCs with color representing the species:\n\n\ndata.frame(pca$x[,1:2], Species = iris$Species) |&gt;\n  ggplot(aes(PC1, PC2, fill = Species)) +\n  geom_point(cex = 3, pch = 21) +\n  coord_fixed(ratio = 1)\n\n\n\n\n\nWe see that the first two dimensions preserve the distance:\n\n\nd_approx &lt;- dist(pca$x[, 1:2])\nplot(d, d_approx); abline(0, 1, col = \"red\")\n\n\n\n\n\nThis example is more realistic than the first artificial example we used, since we showed how we can visualize the data using two dimensions when the data was four-dimensional."
  },
  {
    "objectID": "27-dimension-reduction.html#mnist-example",
    "href": "27-dimension-reduction.html#mnist-example",
    "title": "27  Dimension reduction",
    "section": "27.7 MNIST example",
    "text": "27.7 MNIST example\n\nThe written digits example has 784 features. Is there any room for data reduction? Can we create simple machine learning algorithms using fewer features?\nLet’s load the data:\n\n\nlibrary(dslabs)\nif (!exists(\"mnist\")) mnist &lt;- read_mnist()\n\n\nBecause the pixels are so small, we expect pixels close to each other on the grid to be correlated, meaning that dimension reduction should be possible.\nLet’s try PCA and explore the variance of the PCs.\nThis will take a few seconds as it is a rather large matrix.\n\n\ncol_means &lt;- colMeans(mnist$test$images)\npca &lt;- prcomp(mnist$train$images)\n\n\npc &lt;- 1:ncol(mnist$test$images)\nqplot(pc, pca$sdev)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\nWe can see that the first few PCs already explain a large percent of the variability:\n\n\nsummary(pca)$importance[,1:5]\n\n                             PC1       PC2       PC3       PC4       PC5\nStandard deviation     576.82291 493.23822 459.89930 429.85624 408.56680\nProportion of Variance   0.09705   0.07096   0.06169   0.05389   0.04869\nCumulative Proportion    0.09705   0.16801   0.22970   0.28359   0.33228\n\n\n\nAnd just by looking at the first two PCs we see information about the class.\nHere is a random sample of 2,000 digits:\n\n\ndata.frame(PC1 = pca$x[,1], PC2 = pca$x[,2],\n           label = factor(mnist$train$label)) |&gt;\n  sample_n(2000) |&gt;\n  ggplot(aes(PC1, PC2, fill = label)) +\n  geom_point(cex = 3, pch = 21)\n\n\n\n\n\nWe can also see the linear combinations on the grid to get an idea of what is getting weighted:\n\n\n\n\n\n\n\nThe lower variance PCs appear related to unimportant variability in the corners:"
  },
  {
    "objectID": "27-dimension-reduction.html#exercises",
    "href": "27-dimension-reduction.html#exercises",
    "title": "27  Dimension reduction",
    "section": "27.8 Exercises",
    "text": "27.8 Exercises\n\nWe want to explore the tissue_gene_expression predictors by plotting them.\n\n\ndim(tissue_gene_expression$x)\n\nWe want to get an idea of which observations are close to each other, but the predictors are 500-dimensional so plotting is difficult. Plot the first two principal components with color representing tissue type.\n\nThe predictors for each observation are measured on the same measurement device (a gene expression microarray) after an experimental procedure. A different device and procedure is used for each observation. This may introduce biases that affect all predictors for each observation in the same way. To explore the effect of this potential bias, for each observation, compute the average across all predictors and then plot this against the first PC with color representing tissue. Report the correlation.\n\n\navg &lt;- rowMeans(tissue_gene_expression$x)\npca &lt;- prcomp(tissue_gene_expression$x)\ndata.frame(PC1 = pca$x[,1], avg = avg, tissue = tissue_gene_expression$y) |&gt;\n  ggplot(aes(PC1, avg, color = tissue)) + \n  geom_point() +\n  ggtitle(paste(\"The correlation is\", cor(pca$x[,1],avg)))\n\n\n\n\n\nWe see an association with the first PC and the observation averages. Redo the PCA but only after removing the center.\nFor the first 10 PCs, make a boxplot showing the values for each tissue.\nPlot the percent variance explained by PC number. Hint: use the summary function."
  },
  {
    "objectID": "28-gene-expression-case-study.html",
    "href": "28-gene-expression-case-study.html",
    "title": "28  Case Study: Differential expression between ethnicity",
    "section": "",
    "text": "Paper here: https://pubmed.ncbi.nlm.nih.gov/17206142/\n\n\n\nVariation in DNA sequence contributes to individual differences in quantitative traits, but in humans the specific sequence variants are known for very few traits. We characterized variation in gene expression in cells from individuals belonging to three major population groups. This quantitative phenotype differs significantly between European-derived and Asian-derived populations for 1,097 of 4,197 genes tested. For the phenotypes with the strongest evidence of cis determinants, most of the variation is due to allele frequency differences at cis-linked regulators. The results show that specific genetic variation among populations contributes appreciably to differences in gene expression phenotypes. Populations differ in prevalence of many complex genetic diseases, such as diabetes and cardiovascular disease. As some of these are probably influenced by the level of gene expression, our results suggest that allele frequency differences at regulatory polymorphisms also account for some population differences in prevalence of complex diseases.\n\n\n\n\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"Biobase\")\n\nif (!require(\"devtools\", quietly = TRUE))\n    install.packages(\"devtools\")\n\ndevtools::install_github(\"genomicsclass/GSE5859\")\n\n\nlibrary(Biobase)\nlibrary(GSE5859)\ndata(GSE5859)\ndim(exprs(e))\n\n[1] 8793  208\n\ndim(pData(e))\n\n[1] 208   3\n\n\n\nDescribed the distribution of ethnic groups\n\n\ntable(pData(e)$ethnicity)\n\n\nASN CEU HAN \n 82 102  24 \n\npData(e) |&gt; dplyr::count(ethnicity)\n\n  ethnicity   n\n1       ASN  82\n2       CEU 102\n3       HAN  24\n\n\n\nCreate a factorx with the ethnic group information and a matrix y with the gene expression matrix.\n\n\nx &lt;- pData(e)$ethnicity\ny &lt;- exprs(e)\ny &lt;- y[-grep(\"AFFX\", rownames(y)),] ## remove control genes\nd &lt;- lubridate::ymd(pData(e)$date)\n\n\nRemove the HAN group. Make sure you remove from both x and y\n\n\nind &lt;- which(x != \"HAN\")\nx &lt;- x[ind]\nx &lt;- droplevels(x)\ny &lt;- y[,ind]\nd &lt;- d[ind]\n\n\nCompute a t-test for the first gene comparing ASN to CEU.\n\n\nind0 &lt;- x == \"ASN\"\ny1 &lt;- y[1,!ind0]\ny0 &lt;- y[1, ind0]\ntt &lt;- (mean(y1) - mean(y0))/sqrt(sd(y1)^2/length(y1) + sd(y0)^2/length(y0))\n2*(1 - pnorm(abs(tt)))\n\n[1] 4.80249e-09\n\n\n\nNow use rowwise operations to compute t-test for each gene. How many genes have p-values smaller than 0.05 / number of tests?\n\n\nlibrary(matrixStats) #home of rowVars()\n\n\nAttaching package: 'matrixStats'\n\n\nThe following objects are masked from 'package:Biobase':\n\n    anyMissing, rowMedians\n\nind0 &lt;- x == \"ASN\"\ny1 &lt;- y[,!ind0]\ny0 &lt;- y[,ind0]\nm1 &lt;- rowMeans(y1)\nm0 &lt;- rowMeans(y0)\nv1 &lt;- rowVars(y1)\nv0 &lt;- rowVars(y0)\nstat &lt;- (m1 - m0)/sqrt(v1/ncol(y1) + v0/ncol(y0))\np_value &lt;-  2*(1 - pnorm(abs(stat)))\n\n\nIf the null hypothesis is true for all genes, and the genes are independent of each other, what distribution do you expect p-values to have? You can use a Monte Carlo.\n\n\nind0 &lt;- x == \"ASN\"\nnull &lt;- matrix(rnorm(ncol(y)*nrow(y)), nrow(y), ncol(y))\ny1 &lt;- null[,!ind0]\ny0 &lt;- null[,ind0]\nm1 &lt;- rowMeans(y1)\nm0 &lt;- rowMeans(y0)\nv1 &lt;- rowVars(y1)\nv0 &lt;- rowVars(y0)\nnull_stat &lt;- (m1 - m0)/sqrt(v1/ncol(y1) + v0/ncol(y0))\nnull_p_value &lt;-  2*(1 - pnorm(abs(null_stat)))\nhist(null_p_value)\n\n\n\n\n\nUnder the null how many p-values smaller than 0.05 do you expect across all genes.\n\n\n0.05*nrow(y)\n\n[1] 437.3\n\nsum(null_p_value &lt; 0.05)\n\n[1] 473\n\n\n\nMake a histogram of the observed p-values.\n\n\nhist(p_value)\n\n\n\nsum(p_value&lt;0.05)\n\n[1] 6296\n\n\n\nFor the top 5 genes with smallest p-values make a boxplot of gene expression by group.\n\n\nlog_p_value &lt;- pnorm(-abs(stat), log.p = TRUE) + log(2)\ntop_ind &lt;- order(log_p_value)[6:10]\nfor (i in top_ind) {\n  boxplot(y[i,]~x)\n  points(jitter(as.numeric(x)), y[i,])\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompute the first 5 PCs and see how they vary across time.\n\n\nlibrary(ggplot2)\npca &lt;- prcomp(t(y), center = TRUE, rank. = 5)\n## change 1 to other numbers to see other PCs\ndata.frame(date = d, PC = pca$x[,1], eth = x) |&gt;\n  ggplot(aes(date, PC, color = eth)) +\n  geom_point()\n\n\n\n\n\nUse the PCs to identified groups other than ethnic group.\n\n\ng &lt;- factor(lubridate::year(d))\n\n\nFor the top genes, fit a linear model that includes these newly identified groups.\n\n\nfor (i in top_ind) {\n  print(rownames(y)[i])\n  fit &lt;- lm(y[i,]~x)\n  print(summary(fit)$coef[2,])\n  fit &lt;- lm(y[i,]~x+g)\n  print(summary(fit)$coef[2,])\n}\n\n[1] \"202100_at\"\n     Estimate    Std. Error       t value      Pr(&gt;|t|) \n-7.286322e-01  4.848757e-02 -1.502720e+01  1.027943e-33 \n  Estimate Std. Error    t value   Pr(&gt;|t|) \n-0.1151998  0.1721053 -0.6693565  0.5041354 \n[1] \"220371_s_at\"\n    Estimate   Std. Error      t value     Pr(&gt;|t|) \n6.641708e-01 4.796013e-02 1.384839e+01 2.969254e-30 \n  Estimate Std. Error    t value   Pr(&gt;|t|) \n 0.1245036  0.1485252  0.8382657  0.4030057 \n[1] \"204826_at\"\n    Estimate   Std. Error      t value     Pr(&gt;|t|) \n7.532823e-01 5.079853e-02 1.482882e+01 3.914570e-33 \n  Estimate Std. Error    t value   Pr(&gt;|t|) \n 0.1417954  0.1623541  0.8733715  0.3836373 \n[1] \"208104_s_at\"\n    Estimate   Std. Error      t value     Pr(&gt;|t|) \n7.452066e-01 5.367906e-02 1.388263e+01 2.354390e-30 \n  Estimate Std. Error    t value   Pr(&gt;|t|) \n0.01811893 0.16958223 0.10684454 0.91503264 \n[1] \"202073_at\"\n     Estimate    Std. Error       t value      Pr(&gt;|t|) \n-7.009949e-01  4.855401e-02 -1.443743e+01  5.504128e-32 \n  Estimate Std. Error    t value   Pr(&gt;|t|) \n0.02581273 0.16842121 0.15326295 0.87836452 \n\n\nMore here: https://pubmed.ncbi.nlm.nih.gov/17597765/"
  },
  {
    "objectID": "29-regularization.html#sec-recommendation-systems",
    "href": "29-regularization.html#sec-recommendation-systems",
    "title": "29  Regularization",
    "section": "29.1 Case study: recommendation systems",
    "text": "29.1 Case study: recommendation systems\n\nDuring its initial years of operation, Netflix used a 5-star recommendation system.\nOne star suggests it is not a good movie, whereas five stars suggests it is an excellent movie.\nHere, we provide the basics of how these recommendations are made, motivated by some of the approaches taken by the winners of the Netflix challenges.\nIn October 2006, Netflix offered a challenge to the data science community: improve our recommendation algorithm by 10% and win a million dollars.\nIn September 2009, the winners were announced1.\nYou can read a summary of how the winning algorithm was put together here: http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/\nand a more detailed explanation here: https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf.\nWe will now show you some of the data analysis strategies used by the winning team.\n\n\n29.1.1 Movielens data\n\nThe Netflix data is not publicly available, but the GroupLens research lab2 generated their own database with over 20 million ratings for over 27,000 movies by more than 138,000 users.\nWe make a small subset of this data available via the dslabs package:\n\n\n\n# A tibble: 5 × 7\n  movieId title                              year genres userId rating timestamp\n    &lt;int&gt; &lt;chr&gt;                             &lt;int&gt; &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;     &lt;int&gt;\n1      31 Dangerous Minds                    1995 Drama       1    2.5    1.26e9\n2    1029 Dumbo                              1941 Anima…      1    3      1.26e9\n3    1061 Sleepers                           1996 Thril…      1    3      1.26e9\n4    1129 Escape from New York               1981 Actio…      1    2      1.26e9\n5    1172 Cinema Paradiso (Nuovo cinema Pa…  1989 Drama       1    4      1.26e9\n\n\n\nEach row represents a rating given by one user to one movie.\nWe can see the number of unique users that provided ratings and how many unique movies were rated:\n\n\nmovielens |&gt; summarize(n_distinct(userId), n_distinct(movieId))\n\n  n_distinct(userId) n_distinct(movieId)\n1                671                9066\n\n\n\nNot every user rated every movie. So we can think of these data as a very large matrix, with users on the rows and movies on the columns, with many empty cells.\nHere is the matrix for six users and four movies.\n\n\n\n\n\n\nuserId\nPulp Fiction\nShawshank Redemption\nForrest Gump\nSilence of the Lambs\n\n\n\n\n13\n3.5\n4.5\n5.0\nNA\n\n\n15\n5.0\n2.0\n1.0\n5.0\n\n\n16\nNA\n4.0\nNA\nNA\n\n\n17\n5.0\n5.0\n2.5\n4.5\n\n\n19\n5.0\n4.0\n5.0\n3.0\n\n\n20\n0.5\n4.5\n2.0\n0.5\n\n\n\n\n\n\n\n\nYou can think of the task of a recommendation system as filling in the NAs in the table above.\nTo see how sparse the matrix is, here is the matrix for a random sample of 100 movies and 100 users with yellow indicating a user/movie combination for which we have a rating.\n\n\n\n\n\n\n\nLet’s look at some of the general properties of the data to better understand the challenges.\nHere is the distribution of number of ratins for each movie.\n\n\n\n\n\n\n\nWe need to build an algorithm with the collected data that will then be applied outside our control, as users look for movie recommendations.\nSo let’s create a test set to assess the accuracy of the models we implement.\nWe then split the data into a training set and test set by assigning 20% of the ratings made by each user to the test set:\n\n\nset.seed(2006)\nindexes &lt;- split(1:nrow(movielens), movielens$userId)\ntest_ind &lt;- sapply(indexes, function(i) sample(i, ceiling(length(i)*.2))) |&gt; \n  unlist() |&gt;\n  sort()\ntest_set &lt;- movielens[test_ind,]\ntrain_set &lt;- movielens[-test_ind,]\n\n\nTo make sure we don’t include movies that are not in both test and train sets, we remove entries using the semi_join function:\n\n\ntest_set &lt;- test_set |&gt; semi_join(train_set, by = \"movieId\")\ntrain_set &lt;- train_set |&gt; semi_join(test_set, by = \"movieId\")\n\n\nWe use pivot_wider to make a matrix with users represented by rows and movies by the columns\n\n\ny_train &lt;- select(train_set, movieId, userId, rating) |&gt;\n  pivot_wider(names_from = movieId, values_from = rating) |&gt;\n  column_to_rownames(\"userId\") |&gt;\n  as.matrix()\n\ny_test &lt;- select(test_set, movieId, userId, rating) |&gt;\n  pivot_wider(names_from = movieId, values_from = rating) |&gt;\n  column_to_rownames(\"userId\") |&gt;\n  as.matrix() \n\ny_test &lt;- y_test[rownames(y_train), colnames(y_train)]\n\n\nFinally, we create table to map movie ids to titles:\n\n\nmovie_map &lt;- train_set |&gt; select(movieId, title) |&gt; distinct(movieId, .keep_all = TRUE)"
  },
  {
    "objectID": "29-regularization.html#sec-netflix-loss-function",
    "href": "29-regularization.html#sec-netflix-loss-function",
    "title": "29  Regularization",
    "section": "29.2 Loss function",
    "text": "29.2 Loss function\n\nThe Netflix challenge decided on a winner based on the root mean squared error (RMSE) computed on the test set. We define \\(y_{u,i}\\) as the rating for movie \\(i\\) by user \\(u\\) in the test set and denote the prediction, obtained from the training set, with \\(\\hat{y}_{u,i}\\). We then define the residuals as:\n\n\\[\nr_{u,i} = y_{u,i} - \\hat{y}_{u,i}\n\\]\n\nThe metric used by the competion was defined as:\n\n\\[\n\\mbox{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{u,i}^{} r_{u,i}^2 }\n\\]\n\nwith \\(N\\) being the number of user/movie combinations for which we made predictions and the sum occurring over all these combinations.\nWe can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating.\nIf this number is larger than 1, it means our typical error is larger than one star, which is not good.\nWe define a function to compute this quantity for any set of residuals:\n\n\nrmse &lt;- function(r) sqrt(mean(r^2, na.rm = TRUE))\n\n\nIn this chapter and the next we introduce two concepts, regularization and matrix factorization, that were used by the winners of the Netflix challenge to obtain lowest RMSE.\n\n\n\n\n\n\n\nNote\n\n\n\nWe later provide a formal discussion of the mean squared error."
  },
  {
    "objectID": "29-regularization.html#a-first-model",
    "href": "29-regularization.html#a-first-model",
    "title": "29  Regularization",
    "section": "29.3 A first model",
    "text": "29.3 A first model\n\nFirst model\n\n\\[\nY_{u,i} = \\mu + \\varepsilon_{u,i}\n\\]\nwith \\(\\varepsilon_{i,u}\\) independent errors sampled from the same distribution centered at 0 and \\(\\mu\\) the true rating for all movies.\n\nWe know that the estimate that minimizes the RMSE is the least squares estimate of \\(\\mu\\) and, in this case, is the average of all ratings:\n\n\nmu &lt;- mean(y_train, na.rm = TRUE)\nmu\n\n[1] 3.578109\n\n\n\nIf we predict all unknown ratings with \\(\\hat{\\mu}\\) we obtain the following RMSE:\n\n\nrmse(y_test - mu)\n\n[1] 1.050776\n\n\n\nKeep in mind that if you plug in any other number, you get a higher RMSE. For example:\n\n\nrmse(y_test - 4)\n\n[1] 1.13857\n\n\n\nFrom looking at the distribution of ratings, we can visualize that this is the standard deviation of that distribution.\nWe get a RMSE of about 1. To win the grand prize of $1,000,000, a participating team had to get an RMSE of about 0.857.\nSo we can definitely do better!"
  },
  {
    "objectID": "29-regularization.html#modeling-movie-effects",
    "href": "29-regularization.html#modeling-movie-effects",
    "title": "29  Regularization",
    "section": "29.4 Modeling movie effects",
    "text": "29.4 Modeling movie effects\n\nWe can use a linear models with a treatment effect \\(b_i\\) for each movie, which can be interpreted as movie effect or the difference between the average ranking for movie \\(i\\) and the overall average \\(\\mu\\):\n\n\\[\nY_{u,i} = \\mu + b_i + \\varepsilon_{u,i}\n\\]\n\nStatistics textbooks refer to the \\(b\\)s as treatment effects, however in the Netflix challenge papers they refer to them as bias, thus the \\(b\\) notation.\nWe can again use least squares to estimate the \\(b_i\\) in the following way:\n\n\nfit &lt;- lm(rating ~ as.factor(movieId), data = train_set)\n\n\nBecause there are thousands of \\(b_i\\) as each movie gets one, the lm() function will be very slow here.\nWe don’t recommend running the code above.\nInstead, we leverage the fact that in this particular situation, we know that the least squares estimate \\(\\hat{b}_i\\) is just the average of \\(Y_{u,i} - \\hat{\\mu}\\) for each movie \\(i\\).\nSo we can compute them this way:\n\n\nb_i &lt;- colMeans(y_train - mu, na.rm = TRUE)\n\n\nNote that we drop the hat notation in the code to represent estimates going forward.\nWe can see that these estimates vary substantially:\n\n\nhist(b_i)\n\n\n\n\n\nRemember \\(\\hat{\\mu}=3.5\\) so a \\(b_i = 1.5\\) implies a perfect five star rating.\nLet’s see how much our prediction improves once we use \\(\\hat{y}_{u,i} = \\hat{\\mu} + \\hat{b}_i\\):\n\n\nrmse(sweep(y_test - mu, 2, b_i))\n\n[1] 0.9914404\n\n\n\nWe already see an improvement. But can we make it better?"
  },
  {
    "objectID": "29-regularization.html#user-effects",
    "href": "29-regularization.html#user-effects",
    "title": "29  Regularization",
    "section": "29.5 User effects",
    "text": "29.5 User effects\n\nLet’s compute the average rating for user \\(u\\) for those that have rated 100 or more movies:\n\n\nb_u &lt;- rowMeans(y_train, na.rm = TRUE)\nhist(b_u, nclass = 30)\n\n\n\n\n\nNotice that there is substantial variability across users as well: some users are very cranky and others love most movies.\nThis implies that a further improvement to our model may be:\n\n\\[\nY_{u,i} = \\mu + b_i + b_u + \\varepsilon_{u,i}\n\\]\nwhere \\(b_u\\) is a user-specific effect.\n\nNow if a cranky user (negative \\(b_u\\)) rates a great movie (positive \\(b_i\\)), the effects counter each other and we may be able to correctly predict that this user gave this great movie a 3 rather than a 5.\nTo fit this model, we could again use lm like this:\n\n\nlm(rating ~ as.factor(movieId) + as.factor(userId), data = train_set)\n\n\nHowever, for the reasons described earlier, we won’t run this code.\nInstead, we will compute an approximation by computing \\(\\hat{\\mu}\\) and \\(\\hat{b}_i\\) and estimating \\(\\hat{b}_u\\) as the average of \\(y_{u,i} - \\hat{\\mu} - \\hat{b}_i\\):\n\n\nb_u &lt;- rowMeans(sweep(y_train - mu, 2, b_i), na.rm = TRUE)\n\n\nWe can now construct predictors and see how much the RMSE improves:\n\n\nrmse(sweep(y_test - mu, 2, b_i) - b_u)\n\n[1] 0.910013"
  },
  {
    "objectID": "29-regularization.html#penalized-least-squares",
    "href": "29-regularization.html#penalized-least-squares",
    "title": "29  Regularization",
    "section": "29.6 Penalized least squares",
    "text": "29.6 Penalized least squares\n\nIf we look at the top movies, based on our estimates of the movie effect \\(b_i\\) we find that they are all movies rated three or less times:\n\n\ncolSums(!is.na(y_train[, mu + b_i == 5])) |&gt; table()\n\n\n 1  2  3 \n32  3  2 \n\n\n\nThese are the names of those rated more than three times along with the score in the test set:\n\n\nind &lt;- which(b_i + mu == 5 & colSums(!is.na(y_train)) &gt; 1)\ntitles &lt;- filter(movie_map, movieId %in% colnames(y_train)[ind]) |&gt; pull(title)\nsetNames(colMeans(y_test[,ind], na.rm = TRUE), titles)\n\n Face in the Crowd, A     In a Lonely Place       Pawnbroker, The \n                  5.0                   4.5                   4.0 \n         Mother Night Village of the Damned \n                  4.0                   3.5 \n\n\n\nThese all seem like obscure movies. Do we really think these are the top movies in our database?\nNote that the prediction does not hold on the test set.\nThese supposed best movies were rated by very few users and small sample sizes lead to uncertainty.\nTherefore, larger estimates of \\(b_i\\), negative or positive, are more likely. Therefore, these are noisy estimates that we should not trust, especially when it comes to prediction. Large errors can increase our RMSE, so we would rather be conservative when unsure.\nIn previous sections, we computed standard error and constructed confidence intervals to account for different levels of uncertainty.\nHowever, when making predictions, we need one number, one prediction, not an interval. For this, we introduce the concept of regularization.\nRegularization permits us to penalize large estimates that are formed using small sample sizes.\nIt has commonalities with the Bayesian approach that shrunk predictions described in the Bayesian models section.\nThe general idea behind regularization is to constrain the total variability of the effect sizes.\nWhy does this help? Consider a case in which we have movie \\(i=1\\) with 100 user ratings and 4 movies \\(i=2,3,4,5\\) with just one user rating. We intend to fit the model\n\n\\[\nY_{u,i} = \\mu + b_i + \\varepsilon_{u,i}\n\\]\n\nSuppose we know the average rating is, say, \\(\\mu = 3\\).\nIf we use least squares, the estimate for the first movie effect \\(b_1\\) is the average of the 100 user ratings, \\(1/100 \\sum_{i=1}^{100} (Y_{i,1} - \\mu)\\), which we expect to be a quite precise.\nHowever, the estimate for movies 2, 3, 4, and 5 will simply be the observed deviation from the average rating \\(\\hat{b}_i = Y_{u,i} - \\hat{\\mu}\\) which is an estimate based on just one number so it won’t be precise at all.\nNote these estimates make the error \\(Y_{u,i} - \\mu + \\hat{b}_i\\) equal to 0 for \\(i=2,3,4,5\\), but we don’t expect to be this lucky next time, when asked to predict.\nIn fact, ignoring the one user and guessing that movies 2,3,4, and 5 are just average movies (\\(b_i = 0\\)) might provide a better prediction.\nThe general idea of penalized regression is to control the total variability of the movie effects: \\(\\sum_{i=1}^5 b_i^2\\).\nSpecifically, instead of minimizing the least squares equation, we minimize an equation that adds a penalty:\n\n\\[\n\\sum_{u,i} \\left(y_{u,i} - \\mu - b_i\\right)^2 + \\lambda \\sum_{i} b_i^2\n\\]\n\nThe first term is just the sum of squares and the second is a penalty that gets larger when many \\(b_i\\)s are large.\nUsing calculus we can actually show that the values of \\(b_i\\) that minimize this equation are:\n\n\\[\n\\hat{b}_i(\\lambda) = \\frac{1}{\\lambda + n_i} \\sum_{u=1}^{n_i} \\left(Y_{u,i} - \\hat{\\mu}\\right)\n\\]\nwhere \\(n_i\\) is the number of ratings made for movie \\(i\\).\n\nThis approach will have our desired effect: when our sample size \\(n_i\\) is very large, a case which will give us a stable estimate, then the penalty \\(\\lambda\\) is effectively ignored since \\(n_i+\\lambda \\approx n_i\\).\nHowever, when the \\(n_i\\) is small, then the estimate \\(\\hat{b}_i(\\lambda)\\) is shrunken towards 0. The larger \\(\\lambda\\), the more we shrink.\nBut how do we select \\(\\lambda\\)? We will later learn about cross-validation which can be used to do this.\nHere we will simply compute the RMSE we for different values of \\(\\lambda\\) to illustrate the effect:\n\n\nn &lt;- colSums(!is.na(y_train))\nsums &lt;- colSums(y_train - mu, na.rm = TRUE)\nlambdas &lt;- seq(0, 10, 0.1)\nrmses &lt;- sapply(lambdas, function(lambda){\n  b_i &lt;-  sums / (n + lambda)\n  rmse(sweep(y_test - mu, 2, b_i))\n})\n\n\nHere is a plot of the RMSE versus \\(\\lambda\\):\n\n\nplot(lambdas, rmses, type = \"l\")\n\n\n\n\n\nThe minimum is obtained for \\(\\lambda=\\) 3.1\nUsing this\\(\\lambda\\) we can compute the regularized estimates and add to our table of estimates:\n\n\nlambda &lt;- lambdas[which.min(rmses)] \nb_i_reg &lt;- colSums(y_train - mu, na.rm = TRUE) / (n + lambda)\n\n\nTo see how the estimates shrink, let’s make a plot of the regularized estimates versus the least squares estimates.\n\n\n\n\n\n\n\nNow, let’s look at the top 5 best movies based on the penalized estimates \\(\\hat{b}_i(\\lambda)\\):\n\n\n\n                           Wild Wild West \n                                 0.750000 \n          Charlie's Angels: Full Throttle \n                                 2.100000 \n                                 Jaws 3-D \n                                 1.000000 \n                  Speed 2: Cruise Control \n                                 2.166667 \n                        Battlefield Earth \n                                 2.500000 \n       Police Academy 6: City Under Siege \n                                 2.000000 \n                                 Bio-Dome \n                                 2.333333 \n                                10,000 BC \n                                 2.000000 \n  Mighty Morphin Power Rangers: The Movie \n                                 3.000000 \n                        Super Mario Bros. \n                                 2.045455 \n       Police Academy 3: Back in Training \n                                 2.000000 \nPolice Academy 5: Assignment: Miami Beach \n                                 2.071429 \n                           102 Dalmatians \n                                 2.250000 \n                                Max Payne \n                                 1.750000 \n                          Joe's Apartment \n                                 3.000000 \n\n\n\nThese make much more sense! These movies are watched more and have more ratings in the training set:\n\n\nsetNames(colSums(!is.na(y_test[,ind])), titles)\n\n                           Wild Wild West \n                                        2 \n          Charlie's Angels: Full Throttle \n                                        5 \n                                 Jaws 3-D \n                                        2 \n                  Speed 2: Cruise Control \n                                        3 \n                        Battlefield Earth \n                                        2 \n       Police Academy 6: City Under Siege \n                                        1 \n                                 Bio-Dome \n                                        3 \n                                10,000 BC \n                                        4 \n  Mighty Morphin Power Rangers: The Movie \n                                        1 \n                        Super Mario Bros. \n                                       11 \n       Police Academy 3: Back in Training \n                                        1 \nPolice Academy 5: Assignment: Miami Beach \n                                        7 \n                           102 Dalmatians \n                                        4 \n                                Max Payne \n                                        2 \n                          Joe's Apartment \n                                        1 \n\n\n\nWe also see that our RMSE is improved if we use our regularized estimates of the movie effects:\n\n\nb_u &lt;- rowMeans(sweep(y_train - mu, 2, b_i_reg), na.rm = TRUE)\nrmse(sweep(y_test - mu, 2, b_i_reg) - b_u)\n\n[1] 0.8852398\n\n\n\nThe penalized estimates provide an improvement over the least squares estimates:\n\n\n\n\n\n\nmodel\nRMSE\n\n\n\n\nJust the mean\n1.0507764\n\n\nMovie effect\n0.9914404\n\n\nMovie + user effect\n0.9100130\n\n\nRegularized movie + user effect\n0.8852398"
  },
  {
    "objectID": "29-regularization.html#exercises",
    "href": "29-regularization.html#exercises",
    "title": "29  Regularization",
    "section": "29.7 Exercises",
    "text": "29.7 Exercises\n\nFor the movielens data, compute the number of ratings for each movie and then plot it against the year the movie came out. Use the square root transformation on the counts.\nWe see that, on average, movies that came out after 1993 get more ratings. We also see that with newer movies, starting in 1993, the number of ratings decreases with year: the more recent a movie is, the less time users have had to rate it.\n\nAmong movies that came out in 1993 or later, what are the 25 movies with the most ratings per year? Also report their average rating.\n\nFrom the table constructed in the previous example, we see that the most rated movies tend to have above average ratings. This is not surprising: more people watch popular movies. To confirm this, stratify the post 1993 movies by ratings per year and compute their average ratings. Make a plot of average rating versus ratings per year and show an estimate of the trend.\nThe movielens dataset also includes a time stamp. This variable represents the time and data in which the rating was provided. The units are seconds since January 1, 1970. Create a new column date with the date. Hint: use the as_datetime function in the lubridate package.\nCompute the average rating for each week and plot this average against day. Hint: use the round_date function before you group_by.\n\n\nlibrary(lubridate)\nmovielens |&gt; \n  mutate(week = round_date(as_datetime(movielens$timestamp), unit = \"week\")) |&gt;\n  group_by(week) |&gt;\n  summarize(avg = mean(rating)) |&gt;\n  ggplot(aes(week, avg)) +\n  geom_line()\n\n\n\n\n\nThe movielens data also has a genres column. This column includes every genre that applies to the movie. Some movies fall under several genres. Define a category as whatever combination appears in this column. Keep only categories with more than 1,000 ratings. Then compute the average and standard error for each category. Plot these as error bar plots.\nThe plot shows strong evidence of a genre effect. If we define \\(g_{u,i}\\) as the genre for user’s \\(u\\) rating of movie \\(i\\), which of the following models is most appropriate:\n\n\n\\(Y_{u,i} = \\mu + b_i + b_u + d_{u,i} + \\varepsilon_{u,i}\\).\n\\(Y_{u,i} = \\mu + b_i + b_u + d_{u,i}\\beta + \\varepsilon_{u,i}\\).\n\\(Y_{u,i} = \\mu + b_i + b_u + \\sum_{k=1}^K x_{u,i} \\beta_k + \\varepsilon_{u,i}\\), with \\(x^k_{u,i} = 1\\) if \\(g_{u,i}\\) is genre \\(k\\).\n\\(Y_{u,i} = \\mu + b_i + b_u + f(d_{u,i}) + \\varepsilon_{u,i}\\), with \\(f\\) a smooth function of \\(d_{u,i}\\).\n\nAn education expert is advocating for smaller schools. The expert bases this recommendation on the fact that among the best performing schools, many are small schools. Let’s simulate a dataset for 100 schools. First, let’s simulate the number of students in each school.\n\nset.seed(1986)\nn &lt;- round(2^rnorm(1000, 8, 1))\n\nNow let’s assign a true quality for each school completely independent from size. This is the parameter we want to estimate.\n\nmu &lt;- round(80 + 2 * rt(1000, 5))\nrange(mu)\nschools &lt;- data.frame(id = paste(\"PS\",1:100), \n                      size = n, \n                      quality = mu,\n                      rank = rank(-mu))\n\nWe can see that the top 10 schools are:\n\nschools |&gt; top_n(10, quality) |&gt; arrange(desc(quality))\n\nNow let’s have the students in the school take a test. There is random variability in test taking so we will simulate the test scores as normally distributed with the average determined by the school quality and standard deviations of 30 percentage points:\n\nscores &lt;- sapply(1:nrow(schools), function(i){\n  scores &lt;- rnorm(schools$size[i], schools$quality[i], 30)\n  scores\n})\nschools &lt;- schools |&gt; mutate(score = sapply(scores, mean))\n\n\nWhat are the top schools based on the average score? Show just the ID, size, and the average score.\nCompare the median school size to the median school size of the top 10 schools based on the score.\nAccording to this test, it appears small schools are better than large schools. Five out of the top 10 schools have 100 or fewer students. But how can this be? We constructed the simulation so that quality and size are independent. Repeat the exercise for the worst 10 schools.\nThe same is true for the worst schools! They are small as well. Plot the average score versus school size to see what’s going on. Highlight the top 10 schools based on the true quality. Use the log scale transform for the size.\nWe can see that the standard error of the score has larger variability when the school is smaller. This is a basic statistical reality we learned in the probability and inference sections. In fact, note that 4 of the top 10 schools are in the top 10 schools based on the exam score.\n\nLet’s use regularization to pick the best schools. Remember regularization shrinks deviations from the average towards 0. So to apply regularization here, we first need to define the overall average for all schools:\n\noverall &lt;- mean(unlist(scores))\n\nand then define, for each school, how it deviates from that average. Write code that estimates the score above average for each school but dividing by \\(n + \\lambda\\) instead of \\(n\\), with \\(n\\) the school size and \\(\\lambda\\) a regularization parameter. Try \\(\\lambda = 3\\).\n\nlambda &lt;- 3\nschools &lt;- schools |&gt; \n  mutate(sum = sapply(scores, sum)) |&gt;\n  mutate(score_reg = mu + (sum - size*mu)/(size + lambda))\n\n\nNotice that this improves things a bit. The number of small schools that are not highly ranked is now 4. Is there a better \\(\\lambda\\)? Find the \\(\\lambda\\) that minimizes the RMSE = \\(1/100 \\sum_{i=1}^{100} (\\mbox{quality} - \\mbox{estimate})^2\\).\n\n\nlambdas &lt;- seq(0, 50, 0.1)\nrmses &lt;- sapply(lambdas, function(lambda){\n  schools |&gt; \n  mutate(sum = sapply(scores, sum)) |&gt;\n  mutate(score_reg = mu + (sum - size*mu)/(size + lambda)) |&gt;\n  summarize(mean(score_reg - quality)^2)\n})\nplot(lambdas, rmses, type = \"l\")\n\n\nRank the schools based on the average obtained with the best \\(\\lambda\\). Note that few small schools are incorrectly included in the top.\n\n\nlambda &lt;- lambdas[which.min(rmses)]\nschools &lt;- schools |&gt; \n  mutate(sum = sapply(scores, sum)) |&gt;\n  mutate(score_reg = mu + (sum - size*mu)/(size + lambda))\nschools |&gt; arrange(desc(score))\n\n\nA common mistake to make when using regularization is shrinking values towards 0 that are not centered around 0. For example, if we don’t subtract the overall average before shrinking, we actually obtain a very similar result. Confirm this by re-running the code from exercise 6 but without removing the overall mean."
  },
  {
    "objectID": "29-regularization.html#footnotes",
    "href": "29-regularization.html#footnotes",
    "title": "29  Regularization",
    "section": "",
    "text": "http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/↩︎\nhttps://grouplens.org/↩︎"
  },
  {
    "objectID": "30-matrix-factorization.html#sec-factor-analysis",
    "href": "30-matrix-factorization.html#sec-factor-analysis",
    "title": "30  Matrix factorization",
    "section": "30.1 Factor analysis",
    "text": "30.1 Factor analysis\nHere is an illustration, using a simulation, of how we can use some structure to predict the \\(r_{u,i}\\). Suppose our residuals r look like this:\n\nround(r, 1)\n\n   Godfather Godfather2 Goodfellas You've Got Sleepless\n1        2.1        2.5        2.4       -1.6      -1.7\n2        1.9        1.4        2.0       -1.8      -1.3\n3        1.8        2.7        2.3       -2.7      -2.0\n4       -0.5        0.7        0.6       -0.8      -0.5\n5       -0.6       -0.8        0.6        0.4       0.6\n6       -0.1        0.2        0.5       -0.7       0.4\n7       -0.3       -0.1       -0.4       -0.4       0.7\n8        0.3        0.4        0.3        0.0       0.7\n9       -1.4       -2.2       -1.5        2.0       2.8\n10      -2.6       -1.5       -1.3        1.6       1.3\n11      -1.5       -2.0       -2.2        1.7       2.7\n12      -1.5       -1.4       -2.3        2.5       2.0\n\n\nThere seems to be a pattern here. In fact, we can see very strong correlation patterns:\n\ncor(r) \n\n            Godfather Godfather2 Goodfellas You've Got  Sleepless\nGodfather   1.0000000  0.9227480  0.9112020 -0.8976893 -0.8634538\nGodfather2  0.9227480  1.0000000  0.9370237 -0.9498184 -0.9685434\nGoodfellas  0.9112020  0.9370237  1.0000000 -0.9489823 -0.9557691\nYou've Got -0.8976893 -0.9498184 -0.9489823  1.0000000  0.9448660\nSleepless  -0.8634538 -0.9685434 -0.9557691  0.9448660  1.0000000\n\n\nWe can create vectors q and p, that can explain much of the structure we see. The q would look like this:\n\nt(q) \n\n     Godfather Godfather2 Goodfellas You've Got Sleepless\n[1,]         1          1          1         -1        -1\n\n\nand it narrows down movies to two groups: gangster (coded with 1) and romance (coded with -1). We can also reduce the users to three groups:\n\nt(p)\n\n     1 2 3 4 5 6 7 8  9 10 11 12\n[1,] 2 2 2 0 0 0 0 0 -2 -2 -2 -2\n\n\nthose that like gangster movies and dislike romance movies (coded as 2), those that like romance movies and dislike gangster movies (coded as -2), and those that don’t care (coded as 0). The main point here is that we can almost reconstruct \\(r\\), which has 60 values, with a couple of vectors totaling 17 values. Note that p and q are equivalent to the patterns and weights we described in Section Section 27.5.\nIf \\(r\\) contains the residuals for users \\(u=1,\\dots,12\\) for movies \\(i=1,\\dots,5\\) we can write the following mathematical formula for our residuals \\(r_{u,i}\\).\n\\[\nr_{u,i} \\approx p_u q_i\n\\]\nThis implies that we can explain more variability by modifying our previous model for movie recommendations to:\n\\[\nY_{u,i} = \\mu + b_i + b_u + p_u q_i + \\varepsilon_{u,i}\n\\]\nHowever, we motivated the need for the \\(p_u q_i\\) term with a simple simulation. The structure found in data is usually more complex. For example, in this first simulation we assumed there were was just one factor \\(p_u\\) that determined which of the two genres movie \\(u\\) belongs to. But the structure in our movie data seems to be much more complicated than gangster movie versus romance. We may have many other factors. Here we present a slightly more complex simulation. We now add a sixth movie, Scent of Woman.\n\nround(r, 1)\n\n   Godfather Godfather2 Goodfellas You've Got Sleepless Scent\n1        0.0        0.3        2.2        0.2       0.1  -2.3\n2        2.0        1.7        0.0       -1.9      -1.7   0.3\n3        1.9        2.4        0.1       -2.3      -2.0   0.0\n4       -0.3        0.3        0.3       -0.4      -0.3   0.3\n5       -0.3       -0.4        0.3        0.2       0.3  -0.3\n6        0.9        1.1       -0.8       -1.3      -0.8   1.2\n7        0.9        1.0       -1.2       -1.2      -0.7   0.7\n8        1.2        1.2       -0.9       -1.0      -0.6   0.8\n9       -0.7       -1.1       -0.8        1.0       1.4   0.7\n10      -2.3       -1.8        0.3        1.8       1.7  -0.1\n11      -1.7       -2.0       -0.1        1.9       2.3   0.2\n12      -1.8       -1.7       -0.1        2.3       2.0   0.4\n\n\nBy exploring the correlation structure of this new dataset\n\n\n            Godfather  Godfather2  Goodfellas        YGM          SS\nGodfather   1.0000000  0.97596928 -0.17481747 -0.9729297 -0.95881628\nGodfather2  0.9759693  1.00000000 -0.10510523 -0.9863528 -0.99025965\nGoodfellas -0.1748175 -0.10510523  1.00000000  0.1798809  0.08007665\nYGM        -0.9729297 -0.98635285  0.17988093  1.0000000  0.98675100\nSS         -0.9588163 -0.99025965  0.08007665  0.9867510  1.00000000\nSW          0.1298518  0.08758531 -0.94263256 -0.1632361 -0.08174489\n                    SW\nGodfather   0.12985181\nGodfather2  0.08758531\nGoodfellas -0.94263256\nYGM        -0.16323610\nSS         -0.08174489\nSW          1.00000000\n\n\nwe note that perhaps we need a second factor to account for the fact that some users like Al Pacino, while others dislike him or don’t care. Notice that the overall structure of the correlation obtained from the simulated data is not that far off the real correlation:\n\n\n            Godfather Godfather2  Goodfellas        YGM         SS          SW\nGodfather   1.0000000  0.8326000  0.45833896 -0.3445887 -0.3254261  0.15250174\nGodfather2  0.8326000  1.0000000  0.62626754 -0.2971988 -0.3104670  0.21003950\nGoodfellas  0.4583390  0.6262675  1.00000000 -0.2969603 -0.3904577 -0.07988783\nYGM        -0.3445887 -0.2971988 -0.29696030  1.0000000  0.5306141 -0.21887238\nSS         -0.3254261 -0.3104670 -0.39045775  0.5306141  1.0000000 -0.25664758\nSW          0.1525017  0.2100395 -0.07988783 -0.2188724 -0.2566476  1.00000000\n\n\nTo explain this more complicated structure, we need two factors. For example something like this:\n\nt(q) \n\n     Godfather Godfather2 Goodfellas You've Got Sleepless Scent\n[1,]         1          1          1         -1        -1    -1\n[2,]         1          1         -1         -1        -1     1\n\n\nWith the first factor (the first column of q) used to code the gangster versus romance groups and a second factor (the second column of q) to explain the Al Pacino versus no Al Pacino groups. We will also need two sets of coefficients to explain the variability introduced by the \\(3\\times 3\\) types of groups:\n\nt(p)\n\n      1 2 3 4 5 6 7 8  9 10 11 12\n[1,]  1 1 1 0 0 0 0 0 -1 -1 -1 -1\n[2,] -1 1 1 0 0 1 1 1  0 -1 -1 -1\n\n\nThe model with two factors has 36 parameters that can be used to explain much of the variability in the 72 ratings:\n\\[\nY_{u,i} = \\mu + b_i + b_u + p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \\varepsilon_{u,i}\n\\]\nNote that in an actual data application, we need to fit this model to data. To explain the complex correlation we observe in real data, we usually permit the entries of \\(p\\) and \\(q\\) to be continuous values, rather than discrete ones as we used in the simulation. For example, rather than dividing movies into gangster or romance, we define a continuum. Also note that this is not a linear model and to fit it we need to use an algorithm other than the one used by lm to find the parameters that minimize the least squares. The winning algorithms for the Netflix challenge fit a model similar to the above and used regularization to penalize for large values of \\(p\\) and \\(q\\), rather than using least squares. Implementing this approach is beyond the scope of this book."
  },
  {
    "objectID": "30-matrix-factorization.html#exercises",
    "href": "30-matrix-factorization.html#exercises",
    "title": "30  Matrix factorization",
    "section": "30.2 Exercises",
    "text": "30.2 Exercises\nIn this exercise set, we will be covering a topic useful for understanding matrix factorization: the singular value decomposition (SVD). SVD is a mathematical result that is widely used in machine learning, both in practice and to understand the mathematical properties of some algorithms. This is a rather advanced topic and to complete this exercise set you will have to be familiar with linear algebra concepts such as matrix multiplication, orthogonal matrices, and diagonal matrices.\nThe SVD tells us that we can decompose an \\(N\\times p\\) matrix \\(Y\\) with \\(p &lt; N\\) as\n\\[ Y = U D V^{\\top} \\]\nWith \\(U\\) and \\(V\\) orthogonal of dimensions \\(N\\times p\\) and \\(p\\times p\\), respectively, and \\(D\\) a \\(p \\times p\\) diagonal matrix with the values of the diagonal decreasing:\n\\[d_{1,1} \\geq d_{2,2} \\geq \\dots d_{p,p}.\\]\nIn this exercise, we will see one of the ways that this decomposition can be useful. To do this, we will construct a dataset that represents grade scores for 100 students in 24 different subjects. The overall average has been removed so this data represents the percentage point each student received above or below the average test score. So a 0 represents an average grade (C), a 25 is a high grade (A+), and a -25 represents a low grade (F). You can simulate the data like this:\n\nset.seed(1987)\nn &lt;- 100\nk &lt;- 8\nSigma &lt;- 64  * matrix(c(1, .75, .5, .75, 1, .5, .5, .5, 1), 3, 3) \nm &lt;- MASS::mvrnorm(n, rep(0, 3), Sigma)\nm &lt;- m[order(rowMeans(m), decreasing = TRUE),]\ny &lt;- m %x% matrix(rep(1, k), nrow = 1) +\n  matrix(rnorm(matrix(n * k * 3)), n, k * 3)\ncolnames(y) &lt;- c(paste(rep(\"Math\",k), 1:k, sep=\"_\"),\n                 paste(rep(\"Science\",k), 1:k, sep=\"_\"),\n                 paste(rep(\"Arts\",k), 1:k, sep=\"_\"))\n\nOur goal is to describe the student performances as succinctly as possible. For example, we want to know if these test results are all just random independent numbers. Are all students just about as good? Does being good in one subject imply you will be good in another? How does the SVD help with all this? We will go step by step to show that with just three relatively small pairs of vectors we can explain much of the variability in this \\(100 \\times 24\\) dataset.\nYou can visualize the 24 test scores for the 100 students by plotting an image:\n\nmy_image &lt;- function(x, zlim = range(x), ...){\n  colors = rev(RColorBrewer::brewer.pal(9, \"RdBu\"))\n  cols &lt;- 1:ncol(x)\n  rows &lt;- 1:nrow(x)\n  image(cols, rows, t(x[rev(rows),,drop=FALSE]), xaxt = \"n\", yaxt = \"n\",\n        xlab=\"\", ylab=\"\",  col = colors, zlim = zlim, ...)\n  abline(h=rows + 0.5, v = cols + 0.5)\n  axis(side = 1, cols, colnames(x), las = 2)\n}\n\nmy_image(y)\n\n\nHow would you describe the data based on this figure?\n\n\nThe test scores are all independent of each other.\nThe students that test well are at the top of the image and there seem to be three groupings by subject.\nThe students that are good at math are not good at science.\nThe students that are good at math are not good at humanities.\n\n\nYou can examine the correlation between the test scores directly like this:\n\n\nmy_image(cor(y), zlim = c(-1,1))\nrange(cor(y))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nWhich of the following best describes what you see?\n\nThe test scores are independent.\nMath and science are highly correlated but the humanities are not.\nThere is high correlation between tests in the same subject but no correlation across subjects.\nThere is a correlation among all tests, but higher if the tests are in science and math and even higher within each subject."
  },
  {
    "objectID": "31-ml-intro.html#sec-training-test",
    "href": "31-ml-intro.html#sec-training-test",
    "title": "31  Machine learning basic concepts and evaluation metrics",
    "section": "31.1 Training and test sets",
    "text": "31.1 Training and test sets\n\nUltimately, a machine learning algorithm is evaluated on how it performs in the real world with completely new datasets.\nHowever, when developing an algorithm, we usually have a dataset for which we know the outcomes, as we do with the heights: we know the sex of every student in our dataset.\nTherefore, to mimic the ultimate evaluation process, we typically split the data into two parts and act as if we don’t know the outcome for one of these.\nWe stop pretending we don’t know the outcome to evaluate the algorithm, but only after we are done constructing it.\nWe refer to the group for which we know the outcome, and use to develop the algorithm, as the training set.\nWe refer to the group for which we pretend we don’t know the outcome as the test set.\nA standard way of generating the training and test sets is by randomly splitting the data. The caret package includes the function createDataPartition that helps us generates indexes for randomly splitting the data into training and test sets:\n\n\nset.seed(2007)\ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\n\n\nThe argument times is used to define how many random samples of indexes to return, the argument p is used to define what proportion of the data is represented by the index, and the argument list is used to decide if we want the indexes returned as a list or not.\nWe can use the result of the createDataPartition function call to define the training and test sets like this:\n\n\ntest_set &lt;- heights[test_index, ]\ntrain_set &lt;- heights[-test_index, ]\n\n\nWe will now develop an algorithm using only the training set.\nOnce we are done developing the algorithm, we will freeze it and evaluate it using the test set. The simplest way to evaluate the algorithm when the outcomes are categorical is by simply reporting the proportion of cases that were correctly predicted in the test set.\nThis metric is usually referred to as overall accuracy."
  },
  {
    "objectID": "31-ml-intro.html#overall-accuracy",
    "href": "31-ml-intro.html#overall-accuracy",
    "title": "31  Machine learning basic concepts and evaluation metrics",
    "section": "31.2 Overall accuracy",
    "text": "31.2 Overall accuracy\n\nTo demonstrate the use of overall accuracy, we will build two competing algorithms and compare them.\nLet’s start by developing the simplest possible machine algorithm: guessing the outcome.\n\n\ny_hat &lt;- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE)\n\n\nNote that we are completely ignoring the predictor and simply guessing the sex.\nIn machine learning applications, it is useful to use factors to represent the categorical outcomes because R functions developed for machine learning, such as those in the caret package, require or recommend that categorical outcomes be coded as factors. So convert y_hat to factors using the factor function:\n\n\ny_hat &lt;- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE) |&gt;\n  factor(levels = levels(test_set$sex))\n\n\nThe overall accuracy is simply defined as the overall proportion that is predicted correctly:\n\n\nmean(y_hat == test_set$sex)\n\n[1] 0.5104762\n\n\n\nNot surprisingly, our accuracy is about 50%: We are guessing!\n\nCan we do better? We shoudl because males are slightly taller than females:\n\nheights |&gt; group_by(sex) |&gt; summarize(mean(height), sd(height))\n\n# A tibble: 2 × 3\n  sex    `mean(height)` `sd(height)`\n  &lt;fct&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1 Female           64.9         3.76\n2 Male             69.3         3.61\n\n\n\nLet’s try another simple approach: predict Male if height is within two standard deviations from the average male:\n\n\ny_hat &lt;- ifelse(x &gt; 62, \"Male\", \"Female\") |&gt; \n  factor(levels = levels(test_set$sex))\n\nThe accuracy goes up from 0.50 to about 0.80:\n\nmean(y == y_hat)\n\n[1] 0.7933333\n\n\n\nBut can we do even better? In the example above, we used a cutoff of 62, but we can examine the accuracy obtained for other cutoffs and then pick the value that provides the best results.\nBut remember, it is important that we optimize the cutoff using only the training set: the test set is only for evaluation.\nAlthough for this simplistic example it is not much of a problem, later we will learn that evaluating an algorithm on the training set can lead to overfitting, which often results in dangerously over-optimistic assessments.\nHere we examine the accuracy of 10 different cutoffs and pick the one yielding the best result:\n\n\ncutoff &lt;- seq(61, 70)\naccuracy &lt;- map_dbl(cutoff, function(x){\n  y_hat &lt;- ifelse(train_set$height &gt; x, \"Male\", \"Female\") |&gt; \n    factor(levels = levels(test_set$sex))\n  mean(y_hat == train_set$sex)\n})\n\n\nWe can make a plot showing the accuracy obtained on the training set for males and females:\n\n\n\n\n\n\n\nWe see that the maximum value is:\n\n\nmax(accuracy)\n\n[1] 0.8495238\n\n\nwhich is much higher than 0.5. The cutoff resulting in this accuracy is:\n\nbest_cutoff &lt;- cutoff[which.max(accuracy)]\nbest_cutoff\n\n[1] 64\n\n\n\nWe can now test this cutoff on our test set to make sure our accuracy is not overly optimistic:\n\n\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\") |&gt; \n  factor(levels = levels(test_set$sex))\ny_hat &lt;- factor(y_hat)\nmean(y_hat == test_set$sex)\n\n[1] 0.8038095\n\n\n\nWe see that it is a bit lower than the accuracy observed for the training set, but it is still better than guessing.\nAnd by testing on a dataset that we did not train on, we know our result is not due to cherry-picking a good result."
  },
  {
    "objectID": "31-ml-intro.html#the-confusion-matrix",
    "href": "31-ml-intro.html#the-confusion-matrix",
    "title": "31  Machine learning basic concepts and evaluation metrics",
    "section": "31.3 The confusion matrix",
    "text": "31.3 The confusion matrix\n\nThe prediction rule we developed in the previous section predicts Male if the student is taller than 64 inches.\nGiven that the average female is about 64 inches, this prediction rule seems wrong. What happened?\nIf a student is the height of the average female, shouldn’t we predict Female?\nGenerally speaking, overall accuracy can be a deceptive measure.\nTo see this, we will start by constructing what is referred to as the confusion matrix, which basically tabulates each combination of prediction and actual value.\nWe can do this in R using the function table:\n\n\ntable(predicted = y_hat, actual = test_set$sex)\n\n         actual\npredicted Female Male\n   Female     48   32\n   Male       71  374\n\n\n\nIf we study this table closely, it reveals a problem:\n\n\ntest_set |&gt; \n  mutate(y_hat = y_hat) |&gt;\n  group_by(sex) |&gt; \n  summarize(accuracy = mean(y_hat == sex))\n\n# A tibble: 2 × 2\n  sex    accuracy\n  &lt;fct&gt;     &lt;dbl&gt;\n1 Female    0.403\n2 Male      0.921\n\n\n\nThere is an imbalance in the accuracy for males and females: too many females are predicted to be male. We are calling almost half of the females male! How can our overall accuracy be so high then?\n\nThis is because the prevalence of males in this dataset is high.\nThese heights were collected from three data sciences courses, two of which had more males enrolled:\n\n\nprev &lt;- mean(y == \"Male\")\nprev\n\n[1] 0.7733333\n\n\n\nSo when computing overall accuracy, the high percentage of mistakes made for females is outweighed by the gains in correct calls for men.\nThis can actually be a big problem in machine learning.\nIf your training data is biased in some way, you are likely to develop algorithms that are biased as well.\nThe fact that we used a test set does not matter because it is also derived from the original biased dataset.\nThis is one of the reasons we look at metrics other than overall accuracy when evaluating a machine learning algorithm.\nThere are several metrics that we can use to evaluate an algorithm in a way that prevalence does not cloud our assessment, and these can all be derived from the confusion matrix.\nA general improvement to using overall accuracy is to study sensitivity and specificity separately."
  },
  {
    "objectID": "31-ml-intro.html#sensitivity-and-specificity",
    "href": "31-ml-intro.html#sensitivity-and-specificity",
    "title": "31  Machine learning basic concepts and evaluation metrics",
    "section": "31.4 Sensitivity and specificity",
    "text": "31.4 Sensitivity and specificity\n\nTo define sensitivity and specificity, we need a binary outcome.\nWhen the outcomes are categorical, we can define these terms for a specific category.\nIn the digits example, we can ask for the specificity in the case of correctly predicting 2 as opposed to some other digit.\nOnce we specify a category of interest, then we can talk about positive outcomes, \\(Y=1\\), and negative outcomes, \\(Y=0\\).\nIn general, sensitivity is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is positive: \\(\\hat{Y}=1\\) when \\(Y=1\\).\nBecause an algorithm that calls everything positive (\\(\\hat{Y}=1\\) no matter what) has perfect sensitivity, this metric on its own is not enough to judge an algorithm.\nFor this reason, we also examine specificity, which is generally defined as the ability of an algorithm to not predict a positive \\(\\hat{Y}=0\\) when the actual outcome is not a positive \\(Y=0\\).\nWe can summarize in the following way:\n\nHigh sensitivity: \\(Y=1 \\implies \\hat{Y}=1\\)\nHigh specificity: \\(Y=0 \\implies \\hat{Y} = 0\\)\n\nAlthough the above is often considered the definition of specificity, another way to think of specificity is by the proportion of positive calls that are actually positive:\n\nHigh specificity: \\(\\hat{Y}=1 \\implies Y=1\\).\n\nTo provide precise definitions, we name the four entries of the confusion matrix:\n\n\n\n\n\n\n\nActually Positive\nActually Negative\n\n\n\n\nPredicted positive\nTrue positives (TP)\nFalse positives (FP)\n\n\nPredicted negative\nFalse negatives (FN)\nTrue negatives (TN)\n\n\n\n\n\n\n\n\nSensitivity is typically quantified by \\(TP/(TP+FN)\\), the proportion of actual positives (the first column = \\(TP+FN\\)) that are called positives (\\(TP\\)).\nThis quantity is referred to as the true positive rate (TPR) or recall.\nSpecificity is defined as \\(TN/(TN+FP)\\) or the proportion of negatives (the second column = \\(FP+TN\\)) that are called negatives (\\(TN\\)).\nThis quantity is also called the true negative rate (TNR).\nThere is another way of quantifying specificity which is \\(TP/(TP+FP)\\) or the proportion of outcomes called positives (the first row or \\(TP+FP\\)) that are actually positives (\\(TP\\)).\nThis quantity is referred to as positive predictive value (PPV) and also as precision.\nNote that, unlike TPR and TNR, precision depends on prevalence since higher prevalence implies you can get higher precision even when guessing.\nThe multiple names can be confusing, so we include a table to help us remember the terms. The table includes a column that shows the definition if we think of the proportions as probabilities.\n\n\n\n\n\n\n\n\n\n\n\nMeasure of\nName 1\nName 2\nDefinition\nProbability representation\n\n\n\n\nsensitivity\nTPR\nRecall\n\\(\\frac{\\mbox{TP}}{\\mbox{TP} + \\mbox{FN}}\\)\n\\(\\mbox{Pr}(\\hat{Y}=1 \\mid Y=1)\\)\n\n\nspecificity\nTNR\n1-FPR\n\\(\\frac{\\mbox{TN}}{\\mbox{TN}+\\mbox{FP}}\\)\n\\(\\mbox{Pr}(\\hat{Y}=0 \\mid Y=0)\\)\n\n\nspecificity\nPPV\nPrecision\n\\(\\frac{\\mbox{TP}}{\\mbox{TP}+\\mbox{FP}}\\)\n\\(\\mbox{Pr}(Y=1 \\mid \\hat{Y}=1)\\)\n\n\n\n\nHere TPR is True Positive Rate, FPR is False Positive Rate, and PPV is Positive Predictive Value.\nThe caret function confusionMatrix computes all these metrics for us once we define what category “positive” is.\nThe function expects factors as input, and the first level is considered the positive outcome or \\(Y=1\\). In our example, Female is the first level because it comes before Male alphabetically.\nIf you type this into R you will see several metrics including accuracy, sensitivity, specificity, and PPV.\n\n\ncm &lt;- confusionMatrix(data = y_hat, reference = test_set$sex)\n\n\nYou can acceess these directly, for example, like this:\n\n\ncm$overall[\"Accuracy\"]\n\n Accuracy \n0.8038095 \n\ncm$byClass[c(\"Sensitivity\",\"Specificity\", \"Prevalence\")]\n\nSensitivity Specificity  Prevalence \n  0.4033613   0.9211823   0.2266667 \n\n\n\nWe can see that the high overall accuracy is possible despite relatively low sensitivity.\nAs we hinted at above, the reason this happens is because of the low prevalence (0.23): the proportion of females is low.\nBecause prevalence is low, failing to predict actual females as females (low sensitivity) does not lower the accuracy as much as failing to predict actual males as males (low specificity).\nThis is an example of why it is important to examine sensitivity and specificity and not just accuracy.\nBefore applying this algorithm to general datasets, we need to ask ourselves if prevalence will be the same."
  },
  {
    "objectID": "31-ml-intro.html#balanced-accuracy-and-f_1-score",
    "href": "31-ml-intro.html#balanced-accuracy-and-f_1-score",
    "title": "31  Machine learning basic concepts and evaluation metrics",
    "section": "31.5 Balanced accuracy and \\(F_1\\) score",
    "text": "31.5 Balanced accuracy and \\(F_1\\) score\n\nAlthough we usually recommend studying both specificity and sensitivity, very often it is useful to have a one-number summary, for example for optimization purposes.\nOne metric that is preferred over overall accuracy is the average of specificity and sensitivity, referred to as balanced accuracy.\nBecause specificity and sensitivity are rates, it is more appropriate to compute the harmonic average. In fact, the \\(F_1\\)-score, a widely used one-number summary, is the harmonic average of precision and recall:\n\n\\[\n\\frac{1}{\\frac{1}{2}\\left(\\frac{1}{\\mbox{recall}} +\n    \\frac{1}{\\mbox{precision}}\\right) }\n\\]\n\nBecause it is easier to write, you often see this harmonic average rewritten as:\n\n\\[\n2 \\times \\frac{\\mbox{precision} \\cdot \\mbox{recall}}\n{\\mbox{precision} + \\mbox{recall}}\n\\]\nwhen defining \\(F_1\\).\n\nRemember that, depending on the context, some types of errors are more costly than others. For example, in the case of plane safety, it is much more important to maximize sensitivity over specificity: failing to predict a plane will malfunction before it crashes is a much more costly error than grounding a plane when, in fact, the plane is in perfect condition.\nIn a capital murder criminal case, the opposite is true since a false positive can lead to executing an innocent person. The \\(F_1\\)-score can be adapted to weigh specificity and sensitivity differently.\nTo do this, we define \\(\\beta\\) to represent how much more important sensitivity is compared to specificity and consider a weighted harmonic average:\n\n\\[\n\\frac{1}{\\frac{\\beta^2}{1+\\beta^2}\\frac{1}{\\mbox{recall}} +\n    \\frac{1}{1+\\beta^2}\\frac{1}{\\mbox{precision}} }\n\\]\n\nThe F_meas function in the caret package computes this summary with beta defaulting to 1.\nLet’s rebuild our prediction algorithm, but this time maximizing the F-score instead of overall accuracy:\n\n\ncutoff &lt;- seq(61, 70)\nF_1 &lt;- map_dbl(cutoff, function(x){\n  y_hat &lt;- ifelse(train_set$height &gt; x, \"Male\", \"Female\") |&gt; \n    factor(levels = levels(test_set$sex))\n  F_meas(data = y_hat, reference = factor(train_set$sex))\n})\n\n\nAs before, we can plot these \\(F_1\\) measures versus the cutoffs:\n\n\n\n\n\n\n\nWe see that it is maximized at \\(F_1\\) value of:\n\n\nmax(F_1)\n\n[1] 0.6470588\n\n\n\nThis maximum is achieved when we use the following cutoff:\n\n\nbest_cutoff &lt;- cutoff[which.max(F_1)]\nbest_cutoff\n\n[1] 66\n\n\n\nA cutoff of 66 makes more sense than 64. Furthermore, it balances the specificity and sensitivity of our confusion matrix:\n\n\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\") |&gt; \n  factor(levels = levels(test_set$sex))\nsensitivity(data = y_hat, reference = test_set$sex)\n\n[1] 0.6302521\n\nspecificity(data = y_hat, reference = test_set$sex)\n\n[1] 0.8325123\n\n\n\nWe now see that we do much better than guessing, that both sensitivity and specificity are relatively high, and that we have built our first machine learning algorithm.\nIt takes height as a predictor and predicts female if you are 65 inches or shorter."
  },
  {
    "objectID": "31-ml-intro.html#prevalence-matters-in-practice",
    "href": "31-ml-intro.html#prevalence-matters-in-practice",
    "title": "31  Machine learning basic concepts and evaluation metrics",
    "section": "31.6 Prevalence matters in practice",
    "text": "31.6 Prevalence matters in practice\n\nA machine learning algorithm with very high sensitivity and specificity may not be useful in practice when prevalence is close to either 0 or 1.\nTo see this, consider the case of a doctor that specializes in a rare disease and is interested in developing an algorithm for predicting who has the disease.\nThe doctor shares data with you and you then develop an algorithm with very high sensitivity.\nYou explain that this means that if a patient has the disease, the algorithm is very likely to predict correctly.\nYou also tell the doctor that you are also concerned because, based on the dataset you analyzed, 1/2 the patients have the disease: \\(\\mbox{Pr}(\\hat{Y}=1)\\).\nThe doctor is neither concerned nor impressed and explains that what is important is the precision of the test: \\(\\mbox{Pr}(Y=1 | \\hat{Y}=1)\\).\nUsing Bayes theorem, we can connect the two measures:\n\n\\[ \\mbox{Pr}(Y = 1\\mid \\hat{Y}=1) = \\mbox{Pr}(\\hat{Y}=1 \\mid Y=1) \\frac{\\mbox{Pr}(Y=1)}{\\mbox{Pr}(\\hat{Y}=1)}\\]\n\nThe doctor knows that the prevalence of the disease is 5 in 1,000, which implies that \\(\\mbox{Pr}(Y=1) \\, / \\,\\mbox{Pr}(\\hat{Y}=1) = 1/100\\) and therefore the precision of your algorithm is less than 0.01. The doctor does not have much use for your algorithm."
  },
  {
    "objectID": "31-ml-intro.html#roc-and-precision-recall-curves",
    "href": "31-ml-intro.html#roc-and-precision-recall-curves",
    "title": "31  Machine learning basic concepts and evaluation metrics",
    "section": "31.7 ROC and precision-recall curves",
    "text": "31.7 ROC and precision-recall curves\n\nWhen comparing the two methods (guessing versus using a height cutoff), we looked at accuracy and \\(F_1\\). The second method clearly outperformed the first.\nHowever, while we considered several cutoffs for the second method, for the first we only considered one approach: guessing with equal probability.\nNote that guessing Male with higher probability would give us higher accuracy due to the bias in the sample:\n\n\np &lt;- 0.9\nn &lt;- length(test_index)\ny_hat &lt;- sample(c(\"Male\", \"Female\"), n, replace = TRUE, prob = c(p, 1 - p)) |&gt; \n  factor(levels = levels(test_set$sex))\nmean(y_hat == test_set$sex)\n\n[1] 0.7390476\n\n\n\nBut, as described above, this would come at the cost of lower sensitivity. The curves we describe in this section will help us see this.\nRemember that for each of these parameters, we can get a different sensitivity and specificity.\nFor this reason, a very common approach to evaluating methods is to compare them graphically by plotting both.\nA widely used plot that does this is the receiver operating characteristic (ROC) curve. If you are wondering where this name comes from, you can consult the ROC Wikipedia page1.\nThe ROC curve plots sensitivity (TPR) versus 1 - specificity or the false positive rate (FPR). Here we compute the TPR and FPR needed for different probabilities of guessing male:\n\n\nprobs &lt;- seq(0, 1, length.out = 10)\nguessing &lt;- map_df(probs, function(p){\n  y_hat &lt;- \n    sample(c(\"Male\", \"Female\"), n, replace = TRUE, prob = c(p, 1 - p)) |&gt; \n    factor(levels = c(\"Female\", \"Male\"))\n  list(method = \"Guessing\",\n       FPR = 1 - specificity(y_hat, test_set$sex),\n       TPR = sensitivity(y_hat, test_set$sex))\n})\n\n\nWe can use similar code to compute these values for our our second approach. By plotting both curves together, we are able to compare sensitivity for different values of specificity:\n\n\n\n\n\n\n\nWe can see that we obtain higher sensitivity with this approach for all values of specificity, which implies it is in fact a better method.\nNote that ROC curves for guessing always fall on the identiy line.\nAlso note that when making ROC curves, it is often nice to add the cutoff associated with each point.\nThe packages pROC and plotROC are useful for generating these plots.\nROC curves have one weakness and it is that neither of the measures plotted depends on prevalence. In cases in which prevalence matters, we may instead make a precision-recall plot.\nThe idea is similar, but we instead plot precision against recall:\n\n\n\n\n\n\n\nFrom this plot we immediately see that the precision of guessing is not high.\nThis is because the prevalence is low. We also see that if we change positives to mean Male instead of Female, the ROC curve remains the same, but the precision recall plot changes."
  },
  {
    "objectID": "31-ml-intro.html#sec-loss-function",
    "href": "31-ml-intro.html#sec-loss-function",
    "title": "31  Machine learning basic concepts and evaluation metrics",
    "section": "31.8 The loss function",
    "text": "31.8 The loss function\n\nUp to now we have described evaluation metrics that apply exclusively to categorical data.\n\nSpecifically, for binary outcomes, we have described how sensitivity, specificity, accuracy, and \\(F_1\\) can be used as quantification. However, these metrics are not useful for continuous outcomes.  In this section, we describe how the general approach to defining “best” in machine learning is to define a loss function, which can be applied to both categorical and continuous data.\n\nThe most commonly used loss function is the squared loss function. If \\(\\hat{y}\\) is our predictor and \\(y\\) is the observed outcome, the squared loss function is simply:\n\n\\[\n(\\hat{y} - y)^2\n\\]\n\nBecause we often have a test set with many observations, say \\(N\\), we use the mean squared error (MSE):\n\n\\[\n\\mbox{MSE} = \\frac{1}{N} \\mbox{RSS} = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2\n\\]\n\nIn practice, we often report the root mean squared error (RMSE), which is \\(\\sqrt{\\mbox{MSE}}\\), because it is in the same units as the outcomes.\nBut doing the math is often easier with the MSE and it is therefore more commonly used in textbooks, since these usually describe theoretical properties of algorithms.\nIf the outcomes are binary, both RMSE and MSE are equivalent to one minus accuracy, since \\((\\hat{y} - y)^2\\) is 0 if the prediction was correct and 1 otherwise. In general, our goal is to build an algorithm that minimizes the loss so it is as close to 0 as possible.\nBecause our data is usually a random sample, we can think of the MSE as a random variable and the observed MSE can be thought of as an estimate of the expected MSE, which in mathematical notation we write like this:\n\n\\[\n\\mbox{E}\\left\\{ \\frac{1}{N}\\sum_{i=1}^N (\\hat{Y}_i - Y_i)^2 \\right\\}\n\\]\n\nThis is a theoretical concept because in practice we only have one dataset to work with. But in theory, we think of having a very large number of random samples (call it \\(B\\)), apply our algorithm to each, obtain an MSE for each random sample, and think of the expected MSE as:\n\n\\[\n\\frac{1}{B} \\sum_{b=1}^B \\frac{1}{N}\\sum_{i=1}^N \\left(\\hat{y}_i^b - y_i^b\\right)^2\n\\]\nwith \\(y_{i}^b\\) denoting the \\(i\\)th observation in the \\(b\\)th random sample and \\(\\hat{y}_i^b\\) the resulting prediction obtained from applying the exact same algorithm to the \\(b\\)th random sample.\n\nAgain, in practice we only observe one random sample, so the expected MSE is only theoretical. However, later we describe crossvalidation, an approach to estimating the MSE that tries to mimic this theoretical quantity.\nNote that there are loss functions other than the squared loss. For example, the Mean Absolute Error uses absolute values, \\(|\\hat{Y}_i - Y_i|\\) instead of squaring the errors \\((\\hat{Y}_i - Y_i)^2\\). However, in this book we focus on minimizing square loss since it is the most widely used."
  },
  {
    "objectID": "31-ml-intro.html#exercises",
    "href": "31-ml-intro.html#exercises",
    "title": "31  Machine learning basic concepts and evaluation metrics",
    "section": "31.9 Exercises",
    "text": "31.9 Exercises\nThe reported_height and height datasets were collected from three classes taught in the Departments of Computer Science and Biostatistics, as well as remotely through the Extension School. The biostatistics class was taught in 2016 along with an online version offered by the Extension School. On 2016-01-25 at 8:15 AM, during one of the lectures, the instructors asked students to fill in the sex and height questionnaire that populated the reported_height dataset. The online students filled the survey during the next few days, after the lecture was posted online. We can use this insight to define a variable, call it type, to denote the type of student: inclass or online:\n\nlibrary(lubridate)\ndat &lt;- mutate(reported_heights, date_time = ymd_hms(time_stamp)) |&gt;\n  filter(date_time &gt;= make_date(2016, 01, 25) & \n           date_time &lt; make_date(2016, 02, 1)) |&gt;\n  mutate(type = ifelse(day(date_time) == 25 & hour(date_time) == 8 & \n                         between(minute(date_time), 15, 30),\n                       \"inclass\", \"online\")) |&gt; select(sex, type)\nx &lt;- dat$type\ny &lt;- factor(dat$sex, c(\"Female\", \"Male\"))\n\n1. Show summary statistics that indicate that the type is predictive of sex.\n2. Instead of using height to predict sex, use the type variable.\n3. Show the confusion matrix.\n4. Use the confusionMatrix function in the caret package to report accuracy.\n5. Now use the sensitivity and specificity functions to report specificity and sensitivity.\n6. What is the prevalence (% of females) in the dat dataset defined above?"
  },
  {
    "objectID": "31-ml-intro.html#footnotes",
    "href": "31-ml-intro.html#footnotes",
    "title": "31  Machine learning basic concepts and evaluation metrics",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic↩︎"
  },
  {
    "objectID": "32-conditional-expectations.html#conditional-probabilities",
    "href": "32-conditional-expectations.html#conditional-probabilities",
    "title": "32  Conditional probabilities and expectations",
    "section": "32.1 Conditional probabilities",
    "text": "32.1 Conditional probabilities\n\nWe use the notation \\((X_1 = x_1,\\dots,X_p=x_p)\\) to represent the fact that we have observed values \\(x_1,\\dots,x_p\\) for features \\(X_1, \\dots, X_p\\).\nThis does not imply that the outcome \\(Y\\) will take a specific value. Instead, it implies a specific probability.\nWe denote the conditional probabilities for each class \\(k\\) with:\n\n\\[\n\\mbox{Pr}(Y=k \\mid X_1 = x_1,\\dots,X_p=x_p), \\, \\mbox{for}\\,k=1,\\dots,K\n\\]\n\nWe will use the bold letters like this: \\(\\mathbf{X} \\equiv (X_1,\\dots,X_p)^\\top\\) and \\(\\mathbf{x} \\equiv (x_1,\\dots,x_p)^\\top\\).\nWe will also use the following notation for the conditional probability of being class \\(k\\):\n\n\\[\np_k(\\mathbf{x}) = \\mbox{Pr}(Y=k \\mid \\mathbf{X}=\\mathbf{x}), \\, \\mbox{for}\\, k=1,\\dots,K\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWDo not confuse this with the \\(p\\) that represents the number of predictors.\n\n\n\nThese probabilities guide the construction of an algorithm that makes the best prediction: for any given \\(\\mathbf{x}\\), we will predict the class \\(k\\) with the largest probability among \\(p_1(x), p_2(x), \\dots p_K(x)\\).\nIn mathematical notation, we write it like this:\n\n\\[\\hat{Y} = \\max_k p_k(\\mathbf{x})\\]\n\nIn machine learning, we refer to this as Bayes’ Rule.\nBut this is a theoretical rule since, in practice, we don’t know \\(p_k(\\mathbf{x}), k=1,\\dots,K\\).\nEstimating these conditional probabilities can be thought of as the main challenge of machine learning.\nThe better our probability estimates \\(\\hat{p}_k(\\mathbf{x})\\), the better our predictor \\(\\hat{Y}\\).\nSo how well we predict depends on two things:\n\nhow close are the \\(\\max_k p_k(\\mathbf{x})\\) to 1 or 0 (perfect certainty) and\nhow close our estimates \\(\\hat{p}_k(\\mathbf{x})\\) are to \\(p_k(\\mathbf{x})\\).\n\nWe can’t do anything about the first restriction as it is determined by the nature of the problem, so\nour energy goes into finding ways to best estimate conditional probabilities.\nThe first restriction does imply that we have limits as to how well even the best possible algorithm can perform.\nin some challenges we will be able to achieve almost perfect accuracy, with digit readers for example,\nin others our success is restricted by the randomness of the process, with movie recommendations for example.\nIt is important to remember that defining our prediction by maximizing the probability is not always optimal in practice and depends on the context.\nAs discussed above, sensitivity and specificity may differ in importance.\nBut even in these cases, having a good estimate of the \\(p_k(x), k=1,\\dots,K\\) will suffice for us to build optimal prediction models, since we can control the balance between specificity and sensitivity however we wish."
  },
  {
    "objectID": "32-conditional-expectations.html#conditional-expectations",
    "href": "32-conditional-expectations.html#conditional-expectations",
    "title": "32  Conditional probabilities and expectations",
    "section": "32.2 Conditional expectations",
    "text": "32.2 Conditional expectations\n\nFor binary data, you can think of the probability \\(\\mbox{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x})\\) as the proportion of 1s in the stratum of the population for which \\(\\mathbf{X}=\\mathbf{x}\\).\nMany of the algorithms we will learn can be applied to both categorical and continuous data due to the connection between conditional probabilities and conditional expectations.\nBecause the expectation is the average of values \\(y_1,\\dots,y_n\\) in the population, in the case in which the \\(y\\)s are 0 or 1:\n\n\\[\n\\mbox{E}(Y \\mid \\mathbf{X}=\\mathbf{x})=\\mbox{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x}).\n\\]\n\nAs a result, we often only use the expectation to denote both the conditional probability and conditional expectation.\nWe assume that the outcome follows the same conditional distribution."
  },
  {
    "objectID": "32-conditional-expectations.html#conditional-expectations-minimizes-squared-loss-function",
    "href": "32-conditional-expectations.html#conditional-expectations-minimizes-squared-loss-function",
    "title": "32  Conditional probabilities and expectations",
    "section": "32.3 Conditional expectations minimizes squared loss function",
    "text": "32.3 Conditional expectations minimizes squared loss function\n\nWhy do we care about the conditional expectation in machine learning?\nThis is because the expected value has an attractive mathematical property: it minimizes the MSE. Specifically, of all possible predictions \\(\\hat{Y}\\),\n\n\\[\n\\hat{Y} = \\mbox{E}(Y \\mid \\mathbf{X}=\\mathbf{x}) \\, \\mbox{ minimizes } \\, \\mbox{E}\\{ (\\hat{Y} - Y)^2  \\mid  \\mathbf{X}=\\mathbf{x} \\}\n\\]\n\nDue to this property, a succinct description of the main task of machine learning is that we use data to estimate:\n\n\\[\nf(\\mathbf{x}) \\equiv \\mbox{E}( Y  \\mid  \\mathbf{X}=\\mathbf{x} )\n\\]\nfor any set of features \\(\\mathbf{x} = (x_1, \\dots, x_p)^\\top\\).\n\nThis is easier said than done, since this function can take any shape and \\(p\\) can be very large.\nConsider a case in which we only have one predictor \\(x\\). The expectation \\(\\mbox{E}\\{ Y \\mid X=x \\}\\) can be any function of \\(x\\): a line, a parabola, a sine wave, a step function, anything.\nIt gets even more complicated when we consider instances with large \\(p\\), in which case \\(f(\\mathbf{x})\\) is a function of a multidimensional vector \\(\\mathbf{x}\\). For example, in our digit reader example \\(p = 784\\)!\nThe main way in which competing machine learning algorithms differ is in their approach to estimating this conditional expectation."
  },
  {
    "objectID": "33-smoothing.html#sec-two-or-seven",
    "href": "33-smoothing.html#sec-two-or-seven",
    "title": "33  Smoothing",
    "section": "33.1 Simplified MNIST: Is it a 2 or a 7?",
    "text": "33.1 Simplified MNIST: Is it a 2 or a 7?\n\nwe define the challenge as building an algorithm that can determine if a digit is a 2 or 7 from the proportion of dark pixels in the upper left quadrant (\\(X_1\\)) and the lower right quadrant (\\(X_2\\)).\nWe also selected a random sample of 1,000 digits, 500 in the training set and 500 in the test set. We provide this dataset in the dslabs package:\n\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(dslabs)\nmnist_27$train |&gt; ggplot(aes(x_1, x_2, color = y)) + geom_point()\n\n\n\n\n\nWe can immediately see some patterns. For example, if \\(X_1\\) (the upper left panel) is very large, then the digit is probably a 7.\nAlso, for smaller values of \\(X_1\\), the 2s appear to be in the mid range values of \\(X_2\\).\n\nTo illustrate how to interpret \\(X_1\\) and \\(X_2\\), we include four example images.\n\nOn the left are the original images of the two digits with the largest and smallest values for \\(X_1\\) and on the right we have the images corresponding to the largest and smallest values of \\(X_2\\):\n\n\n\n\n\n\n\nWe can start getting a sense for why these predictors are useful, but also why the problem will be somewhat challenging.\nWe haven’t really learned any algorithms yet, so let’s try building an algorithm using multivariable regression.\nThe model is simply:\n\n\\[\np(x_1, x_2) = \\mbox{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2) =\n\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\n\nWe fit it like this:\n\n\nfit &lt;- mnist_27$train |&gt; mutate(y = ifelse(y == 7, 1, 0)) |&gt; lm(y ~ x_1 + x_2, data = _)\n\n\nWe can now build a decision rule based on the estimate of \\(\\hat{p}(x_1, x_2)\\):\n\n\np_hat &lt;- predict(fit, newdata = mnist_27$test)\ny_hat &lt;- factor(ifelse(p_hat &gt; 0.5, 7, 2))\nconfusionMatrix(y_hat, mnist_27$test$y)$overall[[\"Accuracy\"]]\n\n[1] 0.75\n\n\n\nWe get an accuracy well above 50%. Not bad for our first try.\nBut can we do better?\nBecause we constructed the mnist_27 example and we had at our disposal 60,000 digits in just the MNIST dataset, we used this to build the true conditional distribution \\(p(x_1, x_2)\\).\nKeep in mind that in practice we don’t have access to the true conditional distribution.\nWe include it in this educational example because it permits the comparison of \\(\\hat{p}(x_1, x_2)\\) to the true \\(p(x_1, x_2)\\). This comparison teaches us the limitations of different algorithms.\nWe have stored the true \\(p(x_1,x_2)\\) in the mnist_27 and can plot it as an image.\nWe draw a curve that separates pairs \\((x_1,x_2)\\) for which \\(p(x_1,x_2) &gt; 0.5\\) and pairs for which \\(p(x_1,x_2) &lt; 0.5\\):\n\n\n\n\n\n\n\nTo start understanding the limitations of regression, first note that with regression \\(\\hat{p}(x_1,x_2)\\) has to be a plane, and as a result the boundary defined by the decision rule is given by: \\(\\hat{p}(x_1,x_2) = 0.5\\):\n\n\\[\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = 0.5 \\implies\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = 0.5  \\implies\nx_2 = (0.5-\\hat{\\beta}_0)/\\hat{\\beta}_2  -\\hat{\\beta}_1/\\hat{\\beta}_2 x_1\n\\]\n\nThis implies that for the boundary \\(x_2\\) is a linear function of \\(x_1\\).\nThis implies that our regression approach has no chance of capturing the non-linear nature of the true \\(p(x_1,x_2)\\).\nBelow is a visual representation of \\(\\hat{p}(x_1, x_2)\\) which clearly shows how it fails to capture the shape of \\(p(x_1, x_2)\\).\n\n\n\n\n\n\n\nWe need something more flexible: a method that permits estimates with shapes other than a plane.\nSmoothing techniques permit this flexibility.\nWe will start by describing nearest neighbor and kernel approaches.\nTo understand why we cover this topic, remember that the concepts behind smoothing techniques are extremely useful in machine learning because conditional expectations/probabilities can be thought of as trends of unknown shapes that we need to estimate in the presence of uncertainty."
  },
  {
    "objectID": "33-smoothing.html#signal-plus-noise-model",
    "href": "33-smoothing.html#signal-plus-noise-model",
    "title": "33  Smoothing",
    "section": "33.2 Signal plus noise model",
    "text": "33.2 Signal plus noise model\n\nTo explain these concepts, we will focus first on a problem with just one predictor.\nSpecifically, we try to estimate the time trend in the 2008 US popular vote poll margin (difference between Obama and McCain). Later we will learn about methods such as k-nearest neighbors that can be used to smooth with higher dimensions.\n\n\npolls_2008 |&gt; ggplot(aes(day, margin)) + geom_point()\n\n\n\n\n\nFor the purposes of the popular vote example, do not think of it as a forecasting problem.\nInstead, we are simply interested in learning the shape of the trend after the election is over.\nWe assume that for any given day \\(x\\), there is a true preference among the electorate \\(f(x)\\), but due to the uncertainty introduced by the polling, each data point comes with an error \\(\\varepsilon\\). A mathematical model for the observed poll margin \\(Y_i\\) is:\n\n\\[\nY_i = f(x_i) + \\varepsilon_i\n\\]\n\nTo think of this as a machine learning problem, consider that we want to predict \\(Y\\) given a day \\(x\\). If we knew the conditional expectation \\(f(x) = \\mbox{E}(Y \\mid X=x)\\), we would use it.\nBut since we don’t know this conditional expectation, we have to estimate it.\nLet’s use regression, since it is the only method we have learned up to now.\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nThe fitted regression line does not appear to describe the trend very well.\nFor example, on September 4 (day -62), the Republican Convention was held and the data suggest that it gave John McCain a boost in the polls.\nHowever, the regression line does not capture this potential trend.\nTo see the lack of fit more clearly, we note that points above the fitted line (blue) and those below (red) are not evenly distributed across days.\nWe therefore need an alternative, more flexible approach."
  },
  {
    "objectID": "33-smoothing.html#bin-smoothing",
    "href": "33-smoothing.html#bin-smoothing",
    "title": "33  Smoothing",
    "section": "33.3 Bin smoothing",
    "text": "33.3 Bin smoothing\n\nThe general idea of smoothing is to group data points into strata in which the value of \\(f(x)\\) can be assumed to be constant.\nWe can make this assumption when we think \\(f(x)\\) changes slowly and, as a result, \\(f(x)\\) is almost constant in small windows of \\(x\\). An example of this idea for the poll_2008 data is to assume that public opinion remained approximately the same within a week’s time.\nWith this assumption in place, we have several data points with the same expected value.\nIf we fix a day to be in the center of our week, call it \\(x_0\\), then for any other day \\(x\\) such that \\(|x - x_0| \\leq 3.5\\), we assume \\(f(x)\\) is a constant \\(f(x) = \\mu\\). This assumption implies that:\n\n\\[\nE[Y_i | X_i = x_i ] \\approx \\mu \\mbox{   if   }  |x_i - x_0| \\leq 3.5\n\\]\n\nIn smoothing, we call the size of the interval satisfying \\(|x_i - x_0| \\leq 3.5\\) the window size, bandwidth or span. Later we will see that we try to optimize this parameter.\nThis assumption implies that a good estimate for \\(f(x_0)\\) is the average of the \\(Y_i\\) values in the window.\nIf we define \\(A_0\\) as the set of indexes \\(i\\) such that \\(|x_i - x_0| \\leq 3.5\\) and \\(N_0\\) as the number of indexes in \\(A_0\\), then our estimate is:\n\n\\[\n\\hat{f}(x_0) = \\frac{1}{N_0} \\sum_{i \\in A_0}  Y_i\n\\]\n\nWe make this calculation with each value of \\(x\\) as the center.\nIn the poll example, for each day, we would compute the average of the values within a week with that day in the center.\nHere are two examples: \\(x_0 = -125\\) and \\(x_0 = -55\\). The blue segment represents the resulting average.\n\n\n\n\n\n\n\nBy computing this mean for every point, we form an estimate of the underlying curve \\(f(x)\\). Below we show the procedure happening as we move from the -155 up to 0.\nAt each value of \\(x_0\\), we keep the estimate \\(\\hat{f}(x_0)\\) and move on to the next point:\n\n\n\n\n\n\n\nThe final code and resulting estimate look like this:\n\n\nspan &lt;- 7 \nfit &lt;- with(polls_2008, ksmooth(day, margin, kernel = \"box\", bandwidth = span))\n\npolls_2008 |&gt; mutate(smooth = fit$y) |&gt;\n  ggplot(aes(day, margin)) +\n    geom_point(size = 3, alpha = .5, color = \"grey\") + \n  geom_line(aes(day, smooth), color = \"red\")"
  },
  {
    "objectID": "33-smoothing.html#kernels",
    "href": "33-smoothing.html#kernels",
    "title": "33  Smoothing",
    "section": "33.4 Kernels",
    "text": "33.4 Kernels\n\nThe final result from the bin smoother is quite wiggly.\nOne reason for this is that each time the window moves, two points change.\nWe can attenuate this somewhat by taking weighted averages that give the center point more weight than far away points, with the two points at the edges receiving very little weight.\nYou can think of the bin smoother approach as a weighted average:\n\n\\[\n\\hat{f}(x_0) = \\sum_{i=1}^N w_0(x_i) Y_i\n\\]\nin which each point receives a weight of either \\(0\\) or \\(1/N_0\\), with \\(N_0\\) the number of points in the week.\n\nIn the code above, we used the argument kernel=\"box\" in our call to the function ksmooth. This is because the weight function looks like a box.\nThe ksmooth function provides a “smoother” option which uses the normal density to assign weights.\n\n\n\n\n\n\n\nThe final code and resulting plot for the normal kernel look like this:\n\n\nspan &lt;- 7\nfit &lt;- with(polls_2008, ksmooth(day, margin, kernel = \"normal\", bandwidth = span))\n\npolls_2008 |&gt; mutate(smooth = fit$y) |&gt;\n  ggplot(aes(day, margin)) +\n  geom_point(size = 3, alpha = .5, color = \"grey\") + \n  geom_line(aes(day, smooth), color = \"red\")\n\n\n\n\n\nNotice that this version looks smoother.\nThere are several functions in R that implement bin smoothers.\nOne example is ksmooth, shown above.\nIn practice, however, we typically prefer methods that use slightly more complex models than fitting a constant.\nThe final result above, for example, is still somewhat wiggly in parts we don’t expect it to be (between -125 and -75, for example). Methods such as loess, which we explain next, improve on this."
  },
  {
    "objectID": "33-smoothing.html#local-weighted-regression-loess",
    "href": "33-smoothing.html#local-weighted-regression-loess",
    "title": "33  Smoothing",
    "section": "33.5 Local weighted regression (loess)",
    "text": "33.5 Local weighted regression (loess)\n\nA limitation of the bin smoother approach just described is that we need small windows for the approximately constant assumptions to hold.\nAs a result, we end up with a small number of data points to average and obtain imprecise estimates \\(\\hat{f}(x)\\). Here we describe how local weighted regression (loess) permits us to consider larger window sizes.\nTo do this, we will use a mathematical result, referred to as Taylor’s theorem, which tells us that if you look closely enough at any smooth function \\(f(x)\\), it will look like a line.\nTo see why this makes sense, consider the curved edges gardeners make using straight-edged spades:\n\n\n\nInstead of assuming the function is approximately constant in a window, we assume the function is locally linear.\nWe can consider larger window sizes with the linear assumption than with a constant.\nInstead of the one-week window, we consider a larger one in which the trend is approximately linear.\nWe start with a three-week window and later consider and evaluate other options:\n\n\\[\nE[Y_i | X_i = x_i ] = \\beta_0 + \\beta_1 (x_i-x_0) \\mbox{   if   }  |x_i - x_0| \\leq 21\n\\]\n\nFor every point \\(x_0\\), loess defines a window and fits a line within that window.\nHere is an example showing the fits for \\(x_0=-125\\) and \\(x_0 = -55\\):\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nThe fitted value at \\(x_0\\) becomes our estimate \\(\\hat{f}(x_0)\\). Below we show the procedure happening as we move from the -155 up to 0.\n\n\n\n\n\n\n\nThe final result is a smoother fit than the bin smoother since we use larger sample sizes to estimate our local parameters:\n\n\ntotal_days &lt;- diff(range(polls_2008$day))\nspan &lt;- 21/total_days\nfit &lt;- loess(margin ~ day, degree = 1, span = span, data = polls_2008)\npolls_2008 |&gt; mutate(smooth = fit$fitted) |&gt;\n  ggplot(aes(day, margin)) +\n  geom_point(size = 3, alpha = .5, color = \"grey\") +\n  geom_line(aes(day, smooth), color = \"red\")\n\n\n\n\n\nDifferent spans give us different estimates.\nWe can see how different window sizes lead to different estimates:\n\n\n\n\n\n\n\nHere are the final estimates:\n\n\n\n\n\n\n\nThere are three other differences between loess and the typical bin smoother.\n\n1. Rather than keeping the bin size the same, loess keeps the number of points used in the local fit the same.This number is controlled via the span argument, which expects a proportion. For example, if N is the number of data points and span=0.5, then for a given \\(x\\), loess will use the 0.5 * N closest points to \\(x\\) for the fit.\n2. When fitting a line locally, loess uses a weighted approach.Basically, instead of using least squares, we minimize a weighted version:\n\\[\n\\sum_{i=1}^N w_0(x_i) \\left[Y_i - \\left\\{\\beta_0 + \\beta_1 (x_i-x_0)\\right\\}\\right]^2\n\\]\nHowever, instead of the Gaussian kernel, loess uses a function called the Tukey tri-weight:\n\\[\nW(u)= \\left( 1  - |u|^3\\right)^3 \\mbox{ if } |u| \\leq 1 \\mbox{ and } W(u) = 0 \\mbox{ if } |u| &gt; 1\n\\]\nTo define the weights, we denote \\(2h\\) as the window size and define:\n\\[\nw_0(x_i) = W\\left(\\frac{x_i - x_0}{h}\\right)\n\\]\nThis kernel differs from the Gaussian kernel in that more points get values closer to the max:\n\n\n\n\n\n3. loess has the option of fitting the local model robustly. An iterative algorithm is implemented in which, after fitting a model in one iteration, outliers are detected and down-weighted for the next iteration. To use this option, we use the argument family=\"symmetric\".\n\n33.5.1 Fitting parabolas\n\nTaylor’s theorem also tells us that if you look at any mathematical function closely enough, it looks like a parabola.\nThe theorem also states that you don’t have to look as closely when approximating with parabolas as you do when approximating with lines.\nThis means we can make our windows even larger and fit parabolas instead of lines.\n\n\\[\nE[Y_i | X_i = x_i ] = \\beta_0 + \\beta_1 (x_i-x_0) + \\beta_2 (x_i-x_0)^2 \\mbox{   if   }  |x_i - x_0| \\leq h\n\\]\n\nYou may have noticed that when we showed the code for using loess, we set degree = 1. This tells loess to fit polynomials of degree 1, a fancy name for lines.\nIf you read the help page for loess, you will see that the argument degree defaults to 2. By default, loess fits parabolas not lines.\nHere is a comparison of the fitting lines (red dashed) and fitting parabolas (orange solid):\n\n\ntotal_days &lt;- diff(range(polls_2008$day))\nspan &lt;- 28/total_days\nfit_1 &lt;- loess(margin ~ day, degree = 1, span = span, data = polls_2008)\nfit_2 &lt;- loess(margin ~ day, span = span, data = polls_2008)\n\npolls_2008 |&gt; mutate(smooth_1 = fit_1$fitted, smooth_2 = fit_2$fitted) |&gt;\n  ggplot(aes(day, margin)) +\n  geom_point(size = 3, alpha = .5, color = \"grey\") +\n  geom_line(aes(day, smooth_1), color = \"red\", lty = 2) +\n  geom_line(aes(day, smooth_2), color = \"orange\", lty = 1) \n\n\n\n\n\nThe degree = 2 gives us more wiggly results.\nIn general, we actually prefer degree = 1 as it is less prone to this kind of noise.\n\n\n\n33.5.2 Beware of default smoothing parameters\nggplot uses loess in its geom_smooth function:\n\npolls_2008 |&gt; ggplot(aes(day, margin)) +\n  geom_point() + \n  geom_smooth(method = loess)\n\n\n\n\n\nBut be careful with default parameters as they are rarely optimal.\nHowever, you can conveniently change them:\n\n\npolls_2008 |&gt; ggplot(aes(day, margin)) +\n  geom_point() + \n  geom_smooth(method = loess, method.args = list(span = 0.15, degree = 1))"
  },
  {
    "objectID": "33-smoothing.html#sec-smoothing-ml-connection",
    "href": "33-smoothing.html#sec-smoothing-ml-connection",
    "title": "33  Smoothing",
    "section": "33.6 Connecting smoothing to machine learning",
    "text": "33.6 Connecting smoothing to machine learning\n\nTo see how smoothing relates to machine learning with a concrete example, consider again our Section 33.1 example.\nIf we define the outcome \\(Y = 1\\) for digits that are seven and \\(Y=0\\) for digits that are 2, then we are interested in estimating the conditional probability:\n\n\\[\np(x_1, x_2) = \\mbox{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2).\n\\]\n\nwith \\(X_1\\) and \\(X_2\\) the two predictors defined above.\nIn this example, the 0s and 1s we observe are “noisy” because for some regions the probabilities \\(p(x_1, x_2)\\) are not that close to 0 or 1. So we need to estimate \\(p(x_1, x_2)\\).\nSmoothing is an alternative to accomplishing this.\nWe saw that linear regression was not flexible enough to capture the non-linear nature of \\(p(x_1, x_2)\\), thus smoothing approaches provide an improvement.\nIn a future lecture we describe a popular machine learning algorithm, k-nearest neighbors, which is based on bin smoothing."
  },
  {
    "objectID": "33-smoothing.html#case-study-estimating-indirect-effects-of-natural-dissasters",
    "href": "33-smoothing.html#case-study-estimating-indirect-effects-of-natural-dissasters",
    "title": "33  Smoothing",
    "section": "33.7 Case Study: Estimating indirect effects of Natural Dissasters",
    "text": "33.7 Case Study: Estimating indirect effects of Natural Dissasters\n\nhead(pr_death_counts)\n\nExplore the data and notice we should remove missing or incomplete values\n\npr_death_counts |&gt;\n  ggplot(aes(date, deaths)) +\n  geom_point()\n\nWarning: Removed 4 rows containing missing values (`geom_point()`).\n\n\n\n\ndat &lt;- pr_death_counts |&gt; filter(date &lt; make_date(2018, 4, 1))\n\nWe start by fitting a seasonal model. We will fit the model to 2015-2016 and then obtain residulas for 2017-2018\n\ndat &lt;- mutate(dat, tt = as.numeric(date - make_date(2017, 9, 20))) |&gt;\n  filter(!is.na(deaths)) |&gt;\n  mutate(x1 = sin(2*pi*tt/365), x2 = cos(2*pi*tt/365),\n         x3 = sin(4*pi*tt/365), x4 = cos(4*pi*tt/365)) \n\nseasonal_fit &lt;- lm(deaths ~ x1 + x2 + x3 + x4, data = filter(dat, year(date) &lt; 2017))\n\nCheck the fit\n\nwith(dat, plot(tt, deaths))\ns &lt;- predict(seasonal_fit, newdata = dat)\nlines(dat$tt, s, col = 2, lwd = 2)\n\n\n\n\nLet’s now fit smooth curve to the residuals\n\ndat &lt;- mutate(dat, resid = deaths - s)\nfit &lt;- loess(resid ~ tt, span = 1/20, degree = 1, data = dat)\nfit &lt;- predict(fit, se = TRUE)\ndat &lt;- mutate(dat, f = fit$fit) |&gt;\n  mutate(lower = f - qnorm(0.995)*fit$se.fit, upper = f + qnorm(.995)*fit$se.fit)\ndat |&gt; ggplot(aes(x = date)) +\n  geom_point(aes(y = resid)) +\n  geom_line(aes(y = f), color = \"red\")\n\n\n\n\nIs there some evidence of indirect effect after the first week after landfall?\n\ndat |&gt; \n  filter(year(date) &gt;= 2017) |&gt;\n  ggplot(aes(date, f, ymin = lower, ymax = upper)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(color = \"red\") +\n  geom_hline(yintercept = 0) +\n  theme_bw()\n\n\n\n\nNote that this is only an exploratory analysis. A formal analysis would take into account changing demographics and perhaps fit a model with a discontinuity on the day the hurricane made landfall."
  },
  {
    "objectID": "34-cross-validation.html#sec-knn-cv-intro",
    "href": "34-cross-validation.html#sec-knn-cv-intro",
    "title": "34  Cross validation",
    "section": "34.1 Motivation with k-nearest neighbors",
    "text": "34.1 Motivation with k-nearest neighbors\n\nWe are interested in estimating the conditional probability function\n\n\\[\np(\\mathbf{x}) = \\mbox{Pr}(Y = 1 \\mid X_1 = x_1 , X_2 = x_2).\n\\]\nas defined in previously.\n\nWith k-nearest neighbors (kNN) we estimate \\(p(\\mathbf{x})\\) in a similar way to bin smoothing.\nHowever, as we will see, kNN is easier to adapt to multiple dimensions.\nFirst we define the distance between all observations based on the features.\nThen, for any point \\(\\mathbf{x}_0\\) for which we want an estimate of \\(p(\\mathbf{x})\\), we look for the \\(k\\) nearest points to \\(mathbf{x}_0\\) and then take an average of the 0s and 1s associated with these points.\nWe refer to the set of points used to compute the average as the neighborhood.\n\nDue to the connection we described earlier between conditional expectations and conditional probabilities, this gives us a \\(\\hat{p}(\\mathbf{x}_0)\\), just like the bin smoother gave us an estimate of a trend.\n\nAs with bin smoothers, we can control the flexibility of our estimate, in this case through the \\(k\\) parameter: larger \\(k\\)s result in smoother estimates, while smaller \\(k\\)s result in more flexible and more wiggly estimates.\n\nTo implement the algorithm, we can use the knn3 function from the caret package.\n\nLooking at the help file for this package, we see that we can call it in one of two ways.\nWe will use the first in which we specify a formula and a data frame.\nThe data frame contains all the data to be used.\nThe formula has the form outcome ~ predictor_1 + predictor_2 + predictor_3 and so on.\nTherefore, we would type y ~ x_1 + x_2. If we are going to use variables in the data frame, we can use the . like this y ~ ..\n\nFor knn3, we also need to pick a parameter: the number of neighbors to include.\n\nLet’s start with the default \\(k = 5\\). The final call looks like this:\n\n\nlibrary(tidyverse)\nlibrary(dslabs)\nlibrary(caret)\nknn_fit &lt;- knn3(y ~ ., data = mnist_27$train, k = 5)\n\nIn this case, since our dataset is balanced and we care just as much about sensitivity as we do about specificity, we will use accuracy to quantify performance.\nThe predict function for knn produces a probability for each class.\n\nWe keep the probability of being a 7 as the estimate \\(\\hat{p}(\\mathbf{x})\\)\n\n\ny_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn, mnist_27$test$y)$overall[\"Accuracy\"]\n\nAccuracy \n   0.815 \n\n\n\nWe see that kNN, with the default parameter, already beats regression.\nTo see why this is the case, we plot \\(\\hat{p}(\\mathbf{x})\\) and compare it to the true conditional probability \\(p(\\mathbf{x})\\):\n\n\n\n\n\n\n\nWe see that kNN better adapts to the non-linear shape of \\(p(\\mathbf{x})\\). However, our estimate has some islands of blue in the red area, which intuitively does not make much sense.\nThis is due to what we call over-training. We describe over-training in detail below.\nOver-training is the reason that we have higher accuracy in the train set compared to the test set:\n\n\ny_hat_knn &lt;- predict(knn_fit, mnist_27$train, type = \"class\")\nconfusionMatrix(y_hat_knn, mnist_27$train$y)$overall[\"Accuracy\"]\n\nAccuracy \n  0.8825 \n\ny_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn, mnist_27$test$y)$overall[\"Accuracy\"]\n\nAccuracy \n   0.815"
  },
  {
    "objectID": "34-cross-validation.html#over-training",
    "href": "34-cross-validation.html#over-training",
    "title": "34  Cross validation",
    "section": "34.2 Over-training",
    "text": "34.2 Over-training\n\nWith kNN, over-training is at its worst when we set \\(k = 1\\). With \\(k = 1\\), the estimate for each \\(\\mathbf{x}\\) in the training set is obtained with just the \\(y\\) corresponding to that point.\nIn this case, if the \\(x_1\\) and \\(x_2\\) are unique, we will obtain perfect accuracy in the training set because each point is used to predict itself.\nRemember that if the predictors are not unique and have different outcomes for at least one set of predictors, then it is impossible to predict perfectly.\nHere we fit a kNN model with \\(k = 1\\):\n\n\nknn_fit_1 &lt;- knn3(y ~ ., data = mnist_27$train, k = 1)\ny_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$train, type = \"class\")\nconfusionMatrix(y_hat_knn_1, mnist_27$train$y)$overall[[\"Accuracy\"]]\n\n[1] 0.99625\n\n\n\nHowever, the test set accuaracy is actually worse than regression:\n\n\ny_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn_1, mnist_27$test$y)$overall[\"Accuracy\"]\n\nAccuracy \n   0.735 \n\n\n\nWe can see the over-fitting problem in this figure.\n\n\n\n\n\n\n\nThe black curves denote the decision rule boundaries.\nThe estimate \\(\\hat{p}(\\mathbf{x})\\) follows the training data too closely (left). You can see that in the training set, boundaries have been drawn to perfectly surround a single red point in a sea of blue.\nBecause most points \\(\\mathbf{x}\\) are unique, the prediction is either 1 or 0 and the prediction for that point is the associated label.\nHowever, once we introduce the training set (right), we see that many of these small islands now have the opposite color and we end up making several incorrect predictions."
  },
  {
    "objectID": "34-cross-validation.html#over-smoothing",
    "href": "34-cross-validation.html#over-smoothing",
    "title": "34  Cross validation",
    "section": "34.3 Over-smoothing",
    "text": "34.3 Over-smoothing\n\nAlthough not as badly as with \\(k=1\\), we saw that with \\(k = 5\\) we also over-trained.\nHence, we should consider a larger \\(k\\). Let’s try, as an example, a much larger number: \\(k = 401\\).\n\n\nknn_fit_401 &lt;- knn3(y ~ ., data = mnist_27$train, k = 401)\ny_hat_knn_401 &lt;- predict(knn_fit_401, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn_401, mnist_27$test$y)$overall[\"Accuracy\"]\n\nAccuracy \n    0.79 \n\n\n\nThis turns out to be similar to regression:\n\n\n\n\n\n\n\nThis size of \\(k\\) is so large that it does not permit enough flexibility.\nWe call this over-smoothing."
  },
  {
    "objectID": "34-cross-validation.html#picking-the-k-in-knn",
    "href": "34-cross-validation.html#picking-the-k-in-knn",
    "title": "34  Cross validation",
    "section": "34.4 Picking the \\(k\\) in kNN",
    "text": "34.4 Picking the \\(k\\) in kNN\n\nSo how do we pick \\(k\\)? In principle we want to pick the \\(k\\) that maximizes accuracy, or minimizes the expected MSE as defined earlier.\nThe goal of cross validation is to estimate these quantities for any given algorithm and set of tuning parameters such as \\(k\\). To understand why we need a special method to do this let’s repeat what we did above but for different values of \\(k\\):\n\n\nks &lt;- seq(3, 251, 2)\n\n\nWe do this using sapply to repeat the above for each one.\n\n\naccuracy &lt;- sapply(ks, function(k){\n  fit &lt;- knn3(y ~ ., data = mnist_27$train, k = k)\n  \n  y_hat &lt;- predict(fit, mnist_27$train, type = \"class\")\n  cm_train &lt;- confusionMatrix(y_hat, mnist_27$train$y)\n  train_error &lt;- cm_train$overall[[\"Accuracy\"]]\n  \n  y_hat &lt;- predict(fit, mnist_27$test, type = \"class\")\n  cm_test &lt;- confusionMatrix(y_hat, mnist_27$test$y)\n  test_error &lt;- cm_test$overall[[\"Accuracy\"]]\n  \n  c(train = train_error, test = test_error)\n})\n\n\nNote that we estimate accuracy by using both the training set and the test set.\nWe can now plot the accuracy estimates for each value of \\(k\\):\n\n\n\n\n\n\n\nFirst, note that the estimate obtained on the training set is generally lower than the estimate obtained with the test set, with the difference larger for smaller values of \\(k\\). This is due to over-training.\nAlso note that the accuracy versus \\(k\\) plot is quite jagged.\nWe do not expect this because small changes in \\(k\\) should not affect the algorithm’s performance too much.\nThe jaggedness is explained by the fact that the accuracy is computed on a sample and therefore is a random variable.\nThis demonstrates why we prefer to minimize the expected loss rather than the loss we observe with one dataset.\nIf we were to use these estimates to pick the \\(k\\) that maximizes accuracy, we would use the estimates built on the test data:\nAnother reason we need a better estimate of accuracy is that if we use the test set to pick this \\(k\\), we should not expect the accompanying accuracy estimate to extrapolate to the real world.\nThis is because even here we broke a golden rule of machine learning: we selected the \\(k\\) using the test set.\nCross validation also provides an estimate that takes this into account."
  },
  {
    "objectID": "34-cross-validation.html#mathematical-description-of-cross-validation",
    "href": "34-cross-validation.html#mathematical-description-of-cross-validation",
    "title": "34  Cross validation",
    "section": "34.5 Mathematical description of cross validation",
    "text": "34.5 Mathematical description of cross validation\n\nWe previously described that a common goal of machine learning is to find an algorithm that produces predictors \\(\\hat{Y}\\) for an outcome \\(Y\\) that minimizes the MSE:\n\n\\[\n\\mbox{MSE} = \\mbox{E}\\left\\{ \\frac{1}{N}\\sum_{i = 1}^N (\\hat{Y}_i - Y_i)^2 \\right\\}\n\\]\n\nWhen all we have at our disposal is one dataset, we can estimate the MSE with the observed MSE like this:\n\n\\[\n\\hat{\\mbox{MSE}} = \\frac{1}{N}\\sum_{i = 1}^N (\\hat{y}_i - y_i)^2\n\\]\n\nThese two are often referred to as the true error and apparent error, respectively.\nThere are two important characteristics of the apparent error we should always keep in mind:\n\n\nBecause our data is random, the apparent error is a random variable. For example, the dataset we have may be a random sample from a larger population. Thus, a prediction algorithm may have a lower apparent error than another algorithm due to luck.\nIf we train an algorithm on the same dataset that we use to compute the apparent error, we might be overtraining. In general, when we do this, the apparent error will be an underestimate of the true error. We saw an extreme example of this with kNN with \\(k=1\\).\n\n\nCross validation is a technique that permits us to alleviate both these problems.\nTo understand cross validation, it helps to think of the true error, a theoretical quantity, as the average of many apparent errors obtained by applying the algorithm to \\(B\\) new random samples of the data, none of them used to train the algorithm.\nWe think of the true error as:\n\n\\[\n\\frac{1}{B} \\sum_{b = 1}^B \\frac{1}{N}\\sum_{i = 1}^N \\left(\\hat{y}_i^b - y_i^b\\right)^2\n\\]\nwith \\(B\\) a large number that can be thought of as practically infinite.\n\nAs already mentioned, this is a theoretical quantity because we only have available one set of outcomes: \\(y_1, \\dots, y_n\\). Cross validation is based on the idea of imitating the theoretical setup above as best we can with the data we have.\nTo do this, we have to generate a series of different random samples.\nThere are several approaches we can use, but the general idea for all of them is to randomly generate smaller datasets that are not used for training, and instead used to estimate the true error."
  },
  {
    "objectID": "34-cross-validation.html#k-fold-cross-validation",
    "href": "34-cross-validation.html#k-fold-cross-validation",
    "title": "34  Cross validation",
    "section": "34.6 K-fold cross validation",
    "text": "34.6 K-fold cross validation\n\nThe first one we describe is K-fold cross validation. Generally speaking, a machine learning challenge starts with a dataset (blue in the image below).\nWe need to build an algorithm using this dataset that will eventually be used in completely independent datasets (yellow).\n\n\n\nBut we don’t get to see these independent datasets.\n\n\n\nSo to imitate this situation, we carve out a piece of our dataset and pretend it is an independent dataset: we divide the dataset into a training set (blue) and a test set (red).\nWe will train our algorithm exclusively on the training set and use the test set only for evaluation purposes.\nWe usually try to select a small piece of the dataset so that we have as much data as possible to train.\nHowever, we also want the test set to be large so that we obtain a stable estimate of the loss without fitting an impractical number of models.\nTypical choices are to use 10%-20% of the data for testing.\n\n\n\nLet’s reiterate that it is indispensable that we not use the test set at all: not for filtering out rows, not for selecting features, nothing!\nNow this presents a new problem because for most machine learning algorithms we need to select parameters, for example the number of neighbors \\(k\\) in k-nearest neighbors.\nHere, we will refer to the set of parameters as \\(\\lambda\\). We need to optimize algorithm parameters without using our test set and we know that if we optimize and evaluate on the same dataset, we will overtrain.\nThis is where cross validation is most useful.\nFor each set of algorithm parameters being considered, we want an estimate of the MSE and then we will choose the parameters with the smallest MSE.\nCross validation provides this estimate.\nFirst, before we start the cross validation procedure, it is important to fix all the algorithm parameters.\nAlthough we will train the algorithm on the set of training sets, the parameters \\(\\lambda\\) will be the same across all training sets.\nWe will use \\(\\hat{y}_i(\\lambda)\\) to denote the predictors obtained when we use parameters \\(\\lambda\\).\nSo, if we are going to imitate this definition:\n\n\\[\n\\mbox{MSE}(\\lambda) = \\frac{1}{B} \\sum_{b = 1}^B \\frac{1}{N}\\sum_{i = 1}^N \\left(\\hat{y}_i^b(\\lambda) - y_i^b\\right)^2\n\\]\n\nWe want to consider datasets that can be thought of as an independent random sample and we want to do this several times.\nWith K-fold cross validation, we do it \\(K\\) times.\nIn the illustrations, we are showing an example that uses \\(K = 5\\).\nWe will eventually end up with \\(K\\) samples, but let’s start by describing how to construct the first: we simply pick \\(M = N/K\\) observations at random (we round if \\(M\\) is not a round number) and think of these as a random sample \\(y_1^b, \\dots, y_M^b\\), with \\(b = 1\\).\nWe call this the validation set:\n\n\n\nNow we can fit the model in the training set, then compute the apparent error on the independent set:\n\n\\[\n\\hat{\\mbox{MSE}}_b(\\lambda) = \\frac{1}{M}\\sum_{i = 1}^M \\left(\\hat{y}_i^b(\\lambda) - y_i^b\\right)^2\n\\]\n\nNote that this is just one sample and will therefore return a noisy estimate of the true error.\nThis is why we take \\(K\\) samples, not just one.\nIn K-cross validation, we randomly split the observations into \\(K\\) non-overlapping sets:\n\n\n\nNow we repeat the calculation above for each of these sets \\(b = 1,\\dots,K\\) and obtain \\(\\hat{\\mbox{MSE}}_1(\\lambda),\\dots, \\hat{\\mbox{MSE}}_K(\\lambda)\\).\nThen, for our final estimate, we compute the average:\n\n\\[\n\\hat{\\mbox{MSE}}(\\lambda) = \\frac{1}{K} \\sum_{b = 1}^K \\hat{\\mbox{MSE}}_b(\\lambda)\n\\]\nand obtain an estimate of our loss.\n\nA final step would be to select the \\(\\lambda\\) that minimizes the MSE.\nWe have described how to use cross validation to optimize parameters.\nHowever, we now have to take into account the fact that the optimization occurred on the training data and therefore we need an estimate of our final algorithm based on data that was not used to optimize the choice.\nHere is where we use the test set we separated early on:\n\n\n\nWe can do cross validation again:\n\n\nand obtain a final estimate of our expected loss.\n\nHowever, note that this means that our entire compute time gets multiplied by \\(K\\). You will soon learn that performing this task takes time because we are performing many complex computations.\nAs a result, we are always looking for ways to reduce this time.\nFor the final evaluation, we often just use the one test set.\nOnce we are satisfied with this model and want to make it available to others, we could refit the model on the entire dataset, without changing the optimized parameters.\n\n\n\nNow how do we pick the cross validation \\(K\\)? Large values of \\(K\\) are preferable because the training data better imitates the original dataset.\nHowever, larger values of \\(K\\) will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation.\nFor this reason, the choices of \\(K = 5\\) and \\(K = 10\\) are popular.\nOne way we can improve the variance of our final estimate is to take more samples.\nTo do this, we would no longer require the training set to be partitioned into non-overlapping sets.\nInstead, we would just pick \\(K\\) sets of some size at random.\nOne popular version of this technique, at each fold, picks observations at random with replacement (which means the same observation can appear twice). This approach has some advantages (not discussed here) and is generally referred to as the bootstrap.\nIn fact, this is the default approach in the caret package.\nWe describe how to implement cross validation with the caret package in the next chapter.\nIn the next section, we include an explanation of how the bootstrap works in general."
  },
  {
    "objectID": "34-cross-validation.html#bootstrap",
    "href": "34-cross-validation.html#bootstrap",
    "title": "34  Cross validation",
    "section": "34.7 Bootstrap",
    "text": "34.7 Bootstrap\n\nSuppose the income distribution of your population is as follows:\n\n\nset.seed(1995)\nn &lt;- 10^6\nincome &lt;- 10^(rnorm(n, log10(45000), log10(3)))\nhist(log10(income), nlcass = 30)\n\nWarning in plot.window(xlim, ylim, \"\", ...): \"nlcass\" is not a graphical\nparameter\n\n\nWarning in title(main = main, sub = sub, xlab = xlab, ylab = ylab, ...):\n\"nlcass\" is not a graphical parameter\n\n\nWarning in axis(1, ...): \"nlcass\" is not a graphical parameter\n\n\nWarning in axis(2, at = yt, ...): \"nlcass\" is not a graphical parameter\n\n\n\n\n\n\nThe population median is:\n\n\nm &lt;- median(income)\nm\n\n[1] 44938.54\n\n\n\nSuppose we don’t have access to the entire population, but want to estimate the median \\(m\\). We take a sample of 100 and estimate the population median \\(m\\) with the sample median \\(M\\):\n\n\nN &lt;- 100\nx &lt;- sample(income, N)\nmedian(x)\n\n[1] 38461.33\n\n\n\nCan we construct a confidence interval? What is the distribution of \\(M\\) ?\nBecause we are simulating the data, we can use a Monte Carlo simulation to learn the distribution of \\(M\\).\n\n\nlibrary(gridExtra)\nB &lt;- 10^4\nm &lt;- replicate(B, {\n  x &lt;- sample(income, N)\n  median(x)\n})\nhist(m, nclass = 30)\nqqnorm(scale(m)); abline(0,1)\n\n\n\n\n\n\n\nIf we know this distribution, we can construct a confidence interval.\nThe problem here is that, as we have already described, in practice we do not have access to the distribution.\nIn the past, we have used the Central Limit Theorem, but the CLT we studied applies to averages and here we are interested in the median.\nWe can see that the 95% confidence interval based on CLT\n\n\nmedian(x) + 1.96 * sd(x) / sqrt(N) * c(-1, 1)\n\n[1] 21017.93 55904.72\n\n\nis quite different from the confidence interval we would generate if we know the actual distribution of \\(M\\):\n\nquantile(m, c(0.025, 0.975))\n\n    2.5%    97.5% \n34437.72 59049.59 \n\n\n\nThe bootstrap permits us to approximate a Monte Carlo simulation without access to the entire distribution.\nThe general idea is relatively simple.\nWe act as if the observed sample is the population.\nWe then sample (with replacement) datasets, of the same sample size as the original dataset.\nThen we compute the summary statistic, in this case the median, on these bootstrap samples.\nTheory tells us that, in many situations, the distribution of the statistics obtained with bootstrap samples approximate the distribution of our actual statistic.\nThis is how we construct bootstrap samples and an approximate distribution:\n\n\nB &lt;- 10^4\nm_star &lt;- replicate(B, {\n  x_star &lt;- sample(x, N, replace = TRUE)\n  median(x_star)\n})\n\n\nNote a confidence interval constructed with the bootstrap is much closer to one constructed with the theoretical distribution:\n\n\nquantile(m_star, c(0.025, 0.975))\n\n    2.5%    97.5% \n30252.82 56908.62 \n\n\n\nFor more on the Bootstrap, including corrections one can apply to improve these confidence intervals, please consult the book An introduction to the bootstrap by Efron, B., & Tibshirani, R. J.\nWe can use ideas similar to those used in the bootstrap in cross validation: instead of dividing the data into equal partitions, we simply bootstrap many times."
  },
  {
    "objectID": "35-caret.html#the-train-functon",
    "href": "35-caret.html#the-train-functon",
    "title": "35  The caret package",
    "section": "35.1 The train functon",
    "text": "35.1 The train functon\nThe caret train function lets us train different algorithms using similar syntax.\n\nSo, for example, we can type:\n\n\nlibrary(tidyverse)\nlibrary(dslabs)\nlibrary(caret)\ntrain_glm &lt;- train(y ~ ., method = \"glm\", data = mnist_27$train)\ntrain_qda &lt;- train(y ~ ., method = \"qda\", data = mnist_27$train)\ntrain_knn &lt;- train(y ~ ., method = \"knn\", data = mnist_27$train)\n\n\nTo make predictions, we can use the output of this function directly without needing to look at the specifics of predict.glm and predict.knn.\nInstead, we can learn how to obtain predictions from predict.train.\n\nThe code looks the same for both methods:\n\ny_hat_glm &lt;- predict(train_glm, mnist_27$test, type = \"raw\")\ny_hat_qda &lt;- predict(train_qda, mnist_27$test, type = \"raw\")\ny_hat_knn &lt;- predict(train_knn, mnist_27$test, type = \"raw\")\n\n\nThis permits us to quickly compare the algorithms.\nFor example, we can compare the accuracy like this:\n\n\nfits &lt;- list(glm = y_hat_glm, qda = y_hat_qda, knn = y_hat_knn)\nsapply(fits, function(fit) confusionMatrix(fit, mnist_27$test$y)$overall[[\"Accuracy\"]])\n\n glm  qda  knn \n0.75 0.82 0.84"
  },
  {
    "objectID": "35-caret.html#sec-caret-cv",
    "href": "35-caret.html#sec-caret-cv",
    "title": "35  The caret package",
    "section": "35.2 Cross validation",
    "text": "35.2 Cross validation\n\nWhen an algorithm includes a tuning parameter, train automatically uses cross validation to decide among a few default values.\nTo find out what parameter or parameters are optimized, you can read the manual 2 or study the output of:\n\n\ngetModelInfo(\"knn\")\n\n\nWe can also use a quick lookup like this:\n\n\nmodelLookup(\"knn\")\n\n\nIf we run it with default values:\n\n\ntrain_knn &lt;- train(y ~ ., method = \"knn\", data = mnist_27$train)\n\nyou can quickly see the results of the cross validation using the ggplot function.\n\nThe argument highlight highlights the max:\n\n\nggplot(train_knn, highlight = TRUE)\n\n\n\n\n\nBy default, the cross validation is performed by taking 25 bootstrap samples comprised of 25% of the observations.\nFor the kNN method, the default is to try \\(k=5,7,9\\). We change this using the tuneGrid parameter.\nThe grid of values must be supplied by a data frame with the parameter names as specified in the modelLookup output.\nHere, we present an example where we try out 30 values between 9 and 67.\nTo do this with caret, we need to define a column named k, so we use this: data.frame(k = seq(9, 67, 2)).\nNote that when running this code, we are fitting 30 versions of kNN to 25 bootstrapped samples.\nSince we are fitting \\(30 \\times 25 = 750\\) kNN models, running this code will take several seconds.\nWe set the seed because cross validation is a random procedure and we want to make sure the result here is reproducible.\n\n\nset.seed(2008)\ntrain_knn &lt;- train(y ~ ., method = \"knn\", \n                   data = mnist_27$train,\n                   tuneGrid = data.frame(k = seq(9, 71, 2)))\nggplot(train_knn, highlight = TRUE)\n\n\n\n\n\nTo access the parameter that maximized the accuracy, you can use this:\n\n\ntrain_knn$bestTune\n\n    k\n10 27\n\n\nand the best performing model like this:\n\ntrain_knn$finalModel\n\n27-nearest neighbor model\nTraining set outcome distribution:\n\n  2   7 \n379 421 \n\n\n\nThe function predict will use this best performing model.\nHere is the accuracy of the best model when applied to the test set, which we have not used at all yet because the cross validation was done on the training set:\n\n\nconfusionMatrix(predict(train_knn, mnist_27$test, type = \"raw\"),\n                mnist_27$test$y)$overall[\"Accuracy\"]\n\nAccuracy \n   0.835 \n\n\n\nIf we want to change how we perform cross validation, we can use the trainControl function.\nWe can make the code above go a bit faster by using, for example, 10-fold cross validation.\nThis means we have 10 samples using 10% of the observations each.\nWe accomplish this using the following code:\n\n\ncontrol &lt;- trainControl(method = \"cv\", number = 10, p = .9)\ntrain_knn_cv &lt;- train(y ~ ., method = \"knn\", \n                   data = mnist_27$train,\n                   tuneGrid = data.frame(k = seq(9, 71, 2)),\n                   trControl = control)\nggplot(train_knn_cv, highlight = TRUE)\n\n\n\n\n\nWe notice that the accuracy estimates are more variable, which is expected since we changed the number of samples used to estimate accuracy.\nNote that results component of the train output includes several summary statistics related to the variability of the cross validation estimates:\n\n\nnames(train_knn$results)\n\n[1] \"k\"          \"Accuracy\"   \"Kappa\"      \"AccuracySD\" \"KappaSD\"   \n\n\n\nWe have only covered the basics.\nThe caret package manual 3 includes many more details."
  },
  {
    "objectID": "35-caret.html#footnotes",
    "href": "35-caret.html#footnotes",
    "title": "35  The caret package",
    "section": "",
    "text": "https://topepo.github.io/caret/available-models.html↩︎\nhttp://topepo.github.io/caret/available-models.html↩︎\nhttps://topepo.github.io/caret/available-models.html↩︎"
  },
  {
    "objectID": "36-algorithms.html#logistic-regression",
    "href": "36-algorithms.html#logistic-regression",
    "title": "36  Examples of algorithms",
    "section": "36.1 Logistic regression",
    "text": "36.1 Logistic regression\n\nWe previously used linear regression to predict classes by fitting the model\n\n\\[\np(\\mathbf{x}) = \\mbox{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2) =\n\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\] using least squares after assigning numeric values of 0 and 1 to the outcomes \\(y\\), and applied regression as if the data were continuous.\n\nA obvious problem with this approach is that \\(\\hat{p}(\\mathbf{x})\\) can be negative and larger than 1:\n\n\nfit_lm &lt;- lm(y ~ x_1 + x_2, data = mutate(mnist_27$train,y = ifelse(y == 7, 1, 0)))\nrange(fit_lm$fitted)\n\n[1] -0.2196035  1.9220528\n\n\n\nTo avoid this we can apply the approach described in Section 23.3 that is more appropriate for binary data.\nWe write the model like this:\n\n\\[\n\\log \\frac{p(\\mathbf{x})}{1-p(\\mathbf{x})} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\n\nWe can then find the the maximum likelihood estimates (MLE) of the model parameters and predict using the estimate \\(p(\\mathbf{x})\\) to obtain an accuracy of 0.775.\nWe see that logistic regression performs similarly to regression.\nThis is not surprising, given that the estimate of \\(\\hat{p}(\\mathbf{x})\\) looks similar as well:\n\n\n\n\n\n\n\nJust like regression, the decision rule is a line, a fact that can be corroborated mathematically, definint \\(g(x) = \\log \\{x/(1-x)\\}\\), we have:\n\n\\[\ng^{-1}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2) = 0.5 \\implies\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = g(0.5) = 0 \\implies\nx_2 = -\\hat{\\beta}_0/\\hat{\\beta}_2 -\\hat{\\beta}_1/\\hat{\\beta}_2 x_1\n\\]\n\nThus, just like with regression, \\(x_2\\) is a linear function of \\(x_1\\). This implies that our logistic regression approach has no chance of capturing the non-linear nature of the true \\(p(\\mathbf{x})\\).\nWe now described some techniques that estimate the conditional probability in a way that is more flexible.\n\n\n\n\n\n\n\nNote\n\n\n\nYou are ready to do exercises 1 - 11."
  },
  {
    "objectID": "36-algorithms.html#k-nearest-neighbors",
    "href": "36-algorithms.html#k-nearest-neighbors",
    "title": "36  Examples of algorithms",
    "section": "36.2 k-nearest neighbors",
    "text": "36.2 k-nearest neighbors\n\nWe previously introduced the kNN algorithm.\nWe noted that \\(k=31\\) provided the highest accuracy in the test set.\nUsing \\(k=31\\) we obtain an accuracy 0.825, an improvement over regression.\nA plot of the estimated conditional probability shows that the kNN estimate is flexible enough and does indeed capture the shape of the true conditional probability.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou are ready to do exercises 12 - 13."
  },
  {
    "objectID": "36-algorithms.html#generative-models",
    "href": "36-algorithms.html#generative-models",
    "title": "36  Examples of algorithms",
    "section": "36.3 Generative models",
    "text": "36.3 Generative models\n\nWe have described how, when using squared loss, the conditional expectation provide the best approach to developing a decision rule.\nIn a binary case, the smallest true error we can achieve is determined by Bayes’ rule, which is a decision rule based on the true conditional probability:\n\n\\[\np(\\mathbf{x}) = \\mbox{Pr}(Y = 1 \\mid \\mathbf{X}=\\mathbf{x})\n\\]\n\nWe have described several approaches to estimating \\(p(\\mathbf{x})\\).\nIn all these approaches, we estimate the conditional probability directly and do not consider the distribution of the predictors.\nIn machine learning, these are referred to as discriminative approaches.\nHowever, Bayes’ theorem tells us that knowing the distribution of the predictors \\(\\mathbf{X}\\) may be useful.\nMethods that model the joint distribution of \\(Y\\) and \\(\\mathbf{X}\\) are referred to as generative models (we model how the entire data, \\(\\mathbf{X}\\) and \\(Y\\), are generated).\nWe start by describing the most general generative model, Naive Bayes, and then proceed to describe two specific cases, quadratic discriminant analysis (QDA) and linear discriminant analysis (LDA).\n\n\n36.3.1 Naive Bayes\n\nRecall that Bayes rule tells us that we can rewrite \\(p(\\mathbf{x})\\) like this:\n\n\\[\np(\\mathbf{x}) = \\mbox{Pr}(Y = 1|\\mathbf{X}=\\mathbf{x}) = \\frac{f_{\\mathbf{X}|Y = 1}(\\mathbf{x}) \\mbox{Pr}(Y = 1)}\n{ f_{\\mathbf{X}|Y = 0}(\\mathbf{x})\\mbox{Pr}(Y = 0)  + f_{\\mathbf{X}|Y = 1}(\\mathbf{x})\\mbox{Pr}(Y = 1) }\n\\]\nwith \\(f_{\\mathbf{X}|Y = 1}\\) and \\(f_{\\mathbf{X}|Y = 0}\\) representing the distribution functions of the predictor \\(\\mathbf{X}\\) for the two classes \\(Y = 1\\) and \\(Y = 0\\).\n\nThe formula implies that if we can estimate these conditional distributions of the predictors, we can develop a powerful decision rule.\nHowever, this is a big if.\nAs we go forward, we will encounter examples in which \\(\\mathbf{X}\\) has many dimensions and we do not have much information about the distribution.\nIn these cases, Naive Bayes will be practically impossible to implement.\nHowever, there are instances in which we have a small number of predictors (not much more than 2) and many categories in which generative models can be quite powerful.\nWe describe two specific examples and use our previously described case studies to illustrate them.\nLet’s start with a very simple and uninteresting, yet illustrative, case: the example related to predicting sex from height.\n\n\nset.seed(1995)\ny &lt;- heights$height\ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\ntrain_set &lt;- heights |&gt; slice(-test_index)\ntest_set &lt;- heights |&gt; slice(test_index)\n\n\nIn this case, the Naive Bayes approach is particularly appropriate because we know that the normal distribution is a good approximation for the conditional distributions of height given sex for both classes \\(Y = 1\\) (female) and \\(Y = 0\\) (male).\nThis implies that we can approximate the conditional distributions \\(f_{X|Y = 1}\\) and \\(f_{X|Y = 0}\\) by simply estimating averages and standard deviations from the data:\n\n\nparams &lt;- train_set |&gt; group_by(sex) |&gt; summarize(avg = mean(height), sd = sd(height))\nparams\n\n# A tibble: 2 × 3\n  sex      avg    sd\n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Female  64.8  4.14\n2 Male    69.2  3.57\n\n\n\nThe prevalence, which we will denote with \\(\\pi = \\mbox{Pr}(Y = 1)\\), can be estimated from the data with:\n\n\npi &lt;- train_set |&gt; summarize(pi = mean(sex == \"Female\")) |&gt; pull(pi)\npi\n\n[1] 0.2118321\n\n\n\nNow we can use our estimates of average and standard deviation to get an actual rule:\n\n\nx &lt;- test_set$height\nf0 &lt;- dnorm(x, params$avg[2], params$sd[2])\nf1 &lt;- dnorm(x, params$avg[1], params$sd[1])\np_hat_bayes &lt;- f1*pi / (f1*pi + f0*(1 - pi))\n\n\nOur Naive Bayes estimate \\(\\hat{p}(x)\\) looks a lot like a logistic regression estimate:\n\n\n\n\n\n\n\nIn fact, we can show that the Naive Bayes approach is similar to the logistic regression prediction mathematically.\nHowever, we leave the demonstration to a more advanced text, such as the Elements of Statistical Learning1. We can see that they are similar empirically by comparing the two resulting curves.\n\n\n\n36.3.2 Controlling prevalence\n\nOne useful feature of the Naive Bayes approach is that it includes a parameter to account for differences in prevalence.\nUsing our sample, we estimated \\(f_{X|Y = 1}\\), \\(f_{X|Y = 0}\\) and \\(\\pi\\). If we use hats to denote the estimates, we can write \\(\\hat{p}(x)\\) as:\n\n\\[\n\\hat{p}(x)= \\frac{\\hat{f}_{X|Y = 1}(x) \\hat{\\pi}}\n{ \\hat{f}_{X|Y = 0}(x)(1-\\hat{\\pi}) + \\hat{f}_{X|Y = 1}(x)\\hat{\\pi} }\n\\]\n\nAs we discussed earlier, our sample has a much lower prevalence, 0.21, than the general population.\nSo if we use the rule \\(\\hat{p}(x) &gt; 0.5\\) to predict females, our accuracy will be affected due to the low sensitivity:\n\n\ny_hat_bayes &lt;- ifelse(p_hat_bayes &gt; 0.5, \"Female\", \"Male\")\nsensitivity(data = factor(y_hat_bayes), reference = factor(test_set$sex))\n\n[1] 0.2125984\n\n\n\nAgain, this is because the algorithm gives more weight to specificity to account for the low prevalence:\n\n\nspecificity(data = factor(y_hat_bayes), reference = factor(test_set$sex))\n\n[1] 0.9674185\n\n\n\nThis is due mainly to the fact that \\(\\hat{\\pi}\\) is substantially less than 0.5, so we tend to predict Male more often.\nIt makes sense for a machine learning algorithm to do this in our sample because we do have a higher percentage of males.\nBut if we were to extrapolate this to a general population, our overall accuracy would be affected by the low sensitivity.\nThe Naive Bayes approach gives us a direct way to correct this since we can simply force \\(\\hat{\\pi}\\) to be whatever value we want it to be.\nSo to balance specificity and sensitivity, instead of changing the cutoff in the decision rule, we could simply change \\(\\hat{\\pi}\\) to 0.5 like this:\n\n\np_hat_bayes_unbiased &lt;- f1 * 0.5 / (f1 * 0.5 + f0 * (1 - 0.5)) \ny_hat_bayes_unbiased &lt;- ifelse(p_hat_bayes_unbiased &gt; 0.5, \"Female\", \"Male\")\n\n\nNote the difference in sensitivity with a better balance:\n\n\nsensitivity(factor(y_hat_bayes_unbiased), factor(test_set$sex))\n\n[1] 0.6929134\n\nspecificity(factor(y_hat_bayes_unbiased), factor(test_set$sex))\n\n[1] 0.8320802\n\n\n\nThe new rule also gives us a very intuitive cutoff between 66-67, which is about the middle of the female and male average heights:\n\n\nplot(x, p_hat_bayes_unbiased)\nabline(h = 0.5, lty = 2) + \nabline(v = 67, lty = 2)\n\n\n\n\ninteger(0)\n\n\n\n\n36.3.3 Quadratic discriminant analysis\n\nQuadratic Discriminant Analysis (QDA) is a version of Naive Bayes in which we assume that the distributions \\(p_{\\mathbf{X}|Y = 1}(x)\\) and \\(p_{\\mathbf{X}|Y = 0}(\\mathbf{x})\\) are multivariate normal.\nThe simple example we described in the previous section is actually QDA. Let’s now look at a slightly more complicated case: the 2 or 7 example.\nIn this case, we have two predictors so we assume each one is bivariate normal.\nThis implies that we need to estimate two averages, two standard deviations, and a correlation for each case \\(Y = 1\\) and \\(Y = 0\\).\nOnce we have these, we can approximate the distributions \\(f_{X_1,X_2|Y = 1}\\) and \\(f_{X_1, X_2|Y = 0}\\).\nWe can easily estimate parameters from the data:\n\n\nparams &lt;- mnist_27$train |&gt; \n  group_by(y) |&gt; \n  summarize(avg_1 = mean(x_1), avg_2 = mean(x_2), \n            sd_1= sd(x_1), sd_2 = sd(x_2), \n            r = cor(x_1, x_2))\nparams\n\n# A tibble: 2 × 6\n  y     avg_1 avg_2   sd_1   sd_2     r\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 2     0.136 0.287 0.0670 0.0600 0.415\n2 7     0.238 0.290 0.0745 0.104  0.468\n\n\n\nWith these estimates in place, all we need are the prevalence \\(\\pi\\) to to compute\n\n\\[\n\\hat{p}(\\mathbf{x})= \\frac{\\hat{f}_{\\mathbf{X}|Y = 1}(\\mathbf{x}) \\hat{\\pi}}\n{ \\hat{f}_{\\mathbf{X}|Y = 0}(x)(1-\\hat{\\pi}) + \\hat{f}_{\\mathbf{X}|Y = 1}(\\mathbf{x})\\hat{\\pi} }\n\\]\n\nNote that the densities \\(f\\) are bivariate normal distributions.\nHere we provide a visual way of showing the approach.\nWe plot the data and use contour plots to give an idea of what the two estimated normal densities look like (we show the curve representing a region that includes 95% of the points):\n\n\n\n\n\n\n\nWe can fit QDA using the qda function the MASS package:\n\n\nlibrary(MASS)\ntrain_qda &lt;- qda(y ~ ., data = mnist_27$train)\ny_hat &lt;- predict(train_qda, mnist_27$test)$class\n\n\nWe see that we obtain relatively good accuracy\n\n\nconfusionMatrix(y_hat, mnist_27$test$y)$overall[\"Accuracy\"] \n\nAccuracy \n   0.815 \n\n\n\nThe conditional probability looks relatively good, although it does not fit as well as the kernel smoothers:\n\n\n\n\n\n\n\nOne reason QDA does not work as well as the kernel methods is because the assumption of normality do not quite hold.\nAlthough for the 2s it seems reasonable, for the 7s it does seem to be off.\nNotice the slight curvature in the points for the 7s:\n\n\nmnist_27$train |&gt; mutate(y = factor(y)) |&gt; \n  ggplot(aes(x_1, x_2, fill = y, color = y)) + \n  geom_point(show.legend = FALSE) + \n  stat_ellipse(type = \"norm\") +\n  facet_wrap(~y)\n\n\n\n\n\nQDA can work well here, but it becomes harder to use as the number of predictors increases.\nHere we have 2 predictors and had to compute 4 means, 4 SDs, and 2 correlations.\nNotice that if we have 10 predictors, we have 45 correlations for each class.\nIn general, the formula is \\(K\\times p(p-1)/2\\), which gets big fast.\nOnce the number of parameters approaches the size of our data, the method becomes impractical due to overfitting.\n\n\n\n36.3.4 Linear discriminant analysis\n\nA relatively simple solution to QDA’s problem of having too many parameters is to assume that the correlation structure is the same for all classes, which reduces the number of parameters we need to estimate.\nIn this case the the distributions looks like this:\n\n\n\n\n\n\n\nWe can LDA using the MASS lda function:\n\n\nNow the size of the ellipses as well as the angles are the same.\nThis is because they are assumed to have the same standard deviations and correlations.\nAlthough this added constrain lowers the number of parameters, the rigidity lowers our accuracy to:\n\n\nconfusionMatrix(y_hat, mnist_27$test$y)$overall[\"Accuracy\"]\n\nAccuracy \n   0.775 \n\n\n\nWhen we force this assumption, we can show mathematically that the boundary is a line, just as with logistic regression.\nFor this reason, we call the method linear discriminant analysis (LDA). Similarly, for QDA, we can show that the boundary must be a quadratic function.\n\n\n\n\n\n\n\nIn the case of LDA, the lack of flexibility does not permit us to capture the non-linearity in the true conditional probability function.\n\n\n\n36.3.5 Connection to distance\n\nThe normal density is:\n\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left\\{ - \\frac{(x-\\mu)^2}{\\sigma^2}\\right\\}\n\\]\n\nIf we remove the constant \\(1/(\\sqrt{2\\pi} \\sigma)\\) and then take the log, we get:\n\n\\[\n- \\frac{(x-\\mu)^2}{\\sigma^2}\n\\]\nwhich is the negative of a distance squared scaled by the standard deviation.\n\nFor higher dimensions, the same is true except the scaling is more complex and involves correlations.\n\n\n\n\n\n\n\nNote\n\n\n\nYou are now ready to do exercises 14-22."
  },
  {
    "objectID": "36-algorithms.html#sec-trees",
    "href": "36-algorithms.html#sec-trees",
    "title": "36  Examples of algorithms",
    "section": "36.4 Classification and regression trees (CART)",
    "text": "36.4 Classification and regression trees (CART)\n\n36.4.1 The curse of dimensionality\n\nWe described how methods such as LDA and QDA are not meant to be used with many predictors \\(p\\) because the number of parameters that we need to estimate becomes too large.\nFor example, with the digits example \\(p = 784\\), we would have over 600,000 parameters with LDA, and we would multiply that by the number of classes for QDA.\nKernel methods such as kNN or local regression do not have model parameters to estimate.\nHowever, they also face a challenge when multiple predictors are used due to what is referred to as the curse of dimensionality. The dimension here refers to the fact that when we have \\(p\\) predictors, the distance between two observations is computed in \\(p\\)-dimensional space.\nA useful way of understanding the curse of dimensionality is by considering how large we have to make a span/neighborhood/window to include a given percentage of the data.\nRemember that with larger neighborhoods, our methods lose flexibility, and to be flexible we need to keep the neighborhoods small.\nTo see how this becomes an issue for higher dimensions, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want to create windows that include 1/10th of data.\nThen it’s easy to see that our windows have to be of size 0.1:\n\n\n\n\n\n\n\nNow, for two predictors, if we decide to keep the neighborhood just as small, 10% for each dimension, we include only 1 point.\nIf we want to include 10% of the data, then we need to increase the size of each side of the square to \\(\\sqrt{.10} \\approx .316\\):\n\n\n\n\n\n\n\nUsing the same logic, if we want to include 10% of the data in a three-dimensional space, then the side of each cube is \\(\\sqrt[3]{.10} \\approx 0.464\\). In general, to include 10% of the data in a case with \\(p\\) dimensions, we need an interval with each side of size \\(\\sqrt[p]{.10}\\) of the total.\nThis proportion gets close to 1 quickly, and if the proportion is 1 it means we include all the data and are no longer smoothing.\n\n\n\n\n\n\n\nBy the time we reach 100 predictors, the neighborhood is no longer very local, as each side covers almost the entire dataset.\nHere we look at a set of elegant and versatile methods that adapt to higher dimensions and also allow these regions to take more complex shapes while still producing models that are interpretable.\nThese are very popular, well-known and studied methods.\nWe will concentrate on regression and decision trees and their extension to random forests.\n\n\n\n36.4.2 CART motivation\n\nTo motivate this section, we will use a new dataset that includes the breakdown of the composition of olive oil into 8 fatty acids:\n\n\nnames(olive)\n\n [1] \"region\"      \"area\"        \"palmitic\"    \"palmitoleic\" \"stearic\"    \n [6] \"oleic\"       \"linoleic\"    \"linolenic\"   \"arachidic\"   \"eicosenoic\" \n\n\n\nFor illustrative purposes, we will try to predict the region using the fatty acid composition values as predictors.\n\n\ntable(olive$region)\n\n\nNorthern Italy       Sardinia Southern Italy \n           151             98            323 \n\n\n\nWe remove the area column because we won’t use it as a predictor.\n\n\nolive &lt;- dplyr::select(olive, -area)\n\n\nUsing kNN we can achieve a test set accuracy of 0.9717332. However, a bit of data exploration reveals that we should be able to do even better.\nFor example, if we look at the distribution of each predictor stratified by region we see that eicosenoic is only present in Southern Italy and that linoleic separates Northern Italy from Sardinia.\n\n\n\n\n\n\n\nThis implies that we should be able to build an algorithm that predicts perfectly!\nWe can see this clearly by plotting the values for eicosenoic and linoleic.\n\n\n\n\n\n\n\nWe previously defined predictor spaces.\nThe predictor space here consists of eight-dimensional points with values between 0 and 100.\nIn the plot above, we show the space defined by the two predictors eicosenoic and linoleic, and, by eye, we can construct a prediction rule that partitions the predictor space so that each partition contains only outcomes of a one category.\nThis in turn can be used to define an algorithm with perfect accuracy.\nSpecifically, we define the following decision rule: if eicosenoic is larger than 0.065, predict Southern Italy.\nIf not, then if linoleic is larger than \\(10.535\\), predict Sardinia, and if lower, predict Northern Italy.\nWe can draw this decision tree like this:\n\n\n\n\n\n\n\nDecision trees like this are often used in practice.\nFor example, to decide on a person’s risk of poor outcome after having a heart attack, doctors use the following:\n\n\n(Source: Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-1842.)\n\nA tree is basically a flow chart of yes or no questions.\nThe general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as nodes. Regression and decision trees operate by predicting an outcome variable \\(y\\) by partitioning the predictors.\n\n\n\n36.4.3 Regression trees\n\nWhen using trees, and the outcome is continuous, we call the approach a regression tree.\nTo introduce regression trees, we will use the 2008 poll data used in previous sections to describe the basic idea of how we build these algorithms.\nAs with other machine learning algorithms, we will try to estimate the conditional expectation \\(f(x) = \\mbox{E}(Y | X = x)\\) with \\(Y\\) the poll margin and \\(x\\) the day.\n\n\n\n\n\n\n( The general idea here is to build a decision tree and, at the end of each node, obtain a predictor \\(\\hat{y}\\).\n\nA mathematical way to describe this is: we are partitioning the predictor space into \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\), and then for any predictor \\(x\\) that falls within region \\(R_j\\), estimate \\(f(x)\\) with the average of the training observations \\(y_i\\) for which the associated predictor \\(x_i\\) is also in \\(R_j\\).\nBut how do we decide on the partition \\(R_1, R_2, \\ldots, R_J\\) and how do we choose \\(J\\)? Here is where the algorithm gets a bit complicated.\nRegression trees create partitions recursively.\nWe start the algorithm with one partition, the entire predictor space.\nIn our simple first example, this space is the interval [-155, 1]. But after the first step we will have two partitions.\nAfter the second step we will split one of these partitions into two and will have three partitions, then four, and so on.\nWe describe how we pick the partition to further partition, and when to stop, later.\nFor each existing partition, we find a predictor \\(j\\) and value \\(s\\) that define two new partitions, which we will call \\(R_1(j,s)\\) and \\(R_2(j,s)\\), that split our observations in the current partition by asking if \\(x_j\\) is bigger than \\(s\\):\n\n\\[\nR_1(j,s) = \\{\\mathbf{x} \\mid x_j &lt; s\\} \\mbox{  and  } R_2(j,s) = \\{\\mathbf{x} \\mid x_j \\geq s\\}\n\\]\n\nIn our current example we only have one predictor, so we will always choose \\(j = 1\\), but in general this will not be the case.\nNow, after we define the new partitions \\(R_1\\) and \\(R_2\\), and we decide to stop the partitioning, we compute predictors by taking the average of all the observations \\(y\\) for which the associated \\(\\mathbf{x}\\) is in \\(R_1\\) and \\(R_2\\).\nWe refer to these two as \\(\\hat{y}_{R_1}\\) and \\(\\hat{y}_{R_2}\\) respectively.\nBut how do we pick \\(j\\) and \\(s\\)? Basically we find the pair that minimizes the residual sum of squares (RSS):\n\n\\[\n\\sum_{i:\\, x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 +\n\\sum_{i:\\, x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\n\\]\n\nThis is then applied recursively to the new regions \\(R_1\\) and \\(R_2\\). We describe how we stop later, but once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region.\nLet’s take a look at what this algorithm does on the 2008 presidential election poll data.\nWe will use the rpart function in the rpart package.\n\n\nlibrary(rpart)\nfit &lt;- rpart(margin ~ ., data = polls_2008)\n\n\nHere, there is only one predictor.\nThus we do not have to decide which predictor \\(j\\) to split by, we simply have to decide what value \\(s\\) we use to split.\nWe can visually see where the splits were made:\n\n\nplot(fit, margin = 0.1)\ntext(fit, cex = 0.75)\n\n\n\n\n\n\n\nThe first split is made on day 39.5.\nOne of those regions is then split at day 86.5.\nThe two resulting new partitions are split on days 49.5 and 117.5, respectively, and so on.\nWe end up with 8 partitions.\nThe final estimate \\(\\hat{f}(x)\\) looks like this:\n\n\npolls_2008 |&gt; \n  mutate(y_hat = predict(fit)) |&gt; \n  ggplot() +\n  geom_point(aes(day, margin)) +\n  geom_step(aes(day, y_hat), col = \"red\")\n\n\n\n\n\nNote that the algorithm stopped partitioning at 8.\nThe decision is made based on a measure referred to as complexity parameter (cp). Every time we split and define two new partitions, our training set RSS decreases.\nThis is because with more partitions, our model has more flexibility to adapt to the training data.\nIn fact, if you split until every point is its own partition, then RSS goes all the way down to 0 since the average of one value is that same value.\nTo avoid this, the algorithm sets a minimum for how much the RSS must improve for another partition to be added.\nThis parameter is referred to as the complexity parameter (cp). The RSS must improve by a factor of cp for the new partition to be added.\nLarge values of cp will therefore force the algorithm to stop earlier which results in fewer nodes.\n\nHowever, cp is not the only parameter used to decide if we should partition a current partition or not.\n\nAnother common parameter is the minimum number of observations required in a partition before partitioning it further.\nThe argument used in the rpart function is minsplit and the default is 20.\nThe rpart implementation of regression trees also permits users to determine a minimum number of observations in each node.\nThe argument is minbucket and defaults to round(minsplit/3).\nAs expected, if we set cp = 0 and minsplit = 2, then our prediction is as flexible as possible and our predictor is our original data:\n\n\n\n\n\n\n\nIntuitively we know that this is not a good approach as it will generally result in over-training.\nThese cp, minsplit, and minbucket, three parameters can be used to control the variability of the final predictors.\nThe larger these values are the more data is averaged to compute a predictor and thus reduce variability.\nThe drawback is that it restricts flexibility.\n\n\nSo how do we pick these parameters? We can use cross validation, just like with any tuning parameter.\nHere is the resulting tree when we use cross validation to choose cp:\n\n\n\n\n\n\n\nNote that if we already have a tree and want to apply a higher cp value, we can use the prune function.\nWe call this pruning a tree because we are snipping off partitions that do not meet a cp criterion.\nHere is an example where we create a tree that used a cp = 0 and then we prune it back:\n\n\nfit &lt;- rpart(margin ~ ., data = polls_2008, control = rpart.control(cp = 0))\npruned_fit &lt;- prune(fit, cp = 0.01)\n\n\n\n36.4.4 Classification (decision) trees\n\nClassification trees, or decision trees, are used in prediction problems where the outcome is categorical.\nWe use the same partitioning principle with some differences to account for the fact that we are now working with a categorical outcome.\nThe first difference is that we form predictions by calculating which class is the most common among the training set observations within the partition, rather than taking the average in each partition (as we can’t take the average of categories).\nThe second is that we can no longer use RSS to choose the partition.\nWhile we could use the naive approach of looking for partitions that minimize training error, better performing approaches use more sophisticated metrics.\nTwo of the more popular ones are the Gini Index and Entropy.\nIn a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy.\nThe Gini Index is going to be 0 in this scenario, and become larger the more we deviate from this scenario.\nTo define the Gini Index, we define \\(\\hat{p}_{j,k}\\) as the proportion of observations in partition \\(j\\) that are of class \\(k\\). The Gini Index is defined as\n\n\\[\n\\mbox{Gini}(j) = \\sum_{k = 1}^K \\hat{p}_{j,k}(1-\\hat{p}_{j,k})\n\\]\n\nIf you study the formula carefully you will see that it is in fact 0 in the perfect scenario described above.\n\nEntropy is a very similar quantity, defined as\n\\[\n\\mbox{entropy}(j) = -\\sum_{k = 1}^K \\hat{p}_{j,k}\\log(\\hat{p}_{j,k}), \\mbox{ with } 0 \\times \\log(0) \\mbox{ defined as }0\n\\]\n\nIf we use classification tree performs on the 2 or 7 example we achieve an accuracy of 0.81 which is better than regression, but is not as good as what we achieved with kernel methods.\nThe plot of the estimated conditional probability shows us the limitations of classification trees:\n\n\n\n\n\n\n\nNote that with decision trees, it is difficult to make the boundaries smooth since each partition creates a discontinuity.\nClassification trees have certain advantages that make them very useful.\nThey are highly interpretable, even more so than linear models.\nThey are easy to visualize (if small enough). Finally, they can model human decision processes and don’t require use of dummy predictors for categorical variables.\nOn the other hand, the approach via recursive partitioning can easily over-train and is therefore a bit harder to train than, for example, linear regression or kNN.\nFurthermore, in terms of accuracy, it is rarely the best performing method since it is not very flexible and is highly unstable to changes in training data.\nRandom forests, explained next, improve on several of these shortcomings."
  },
  {
    "objectID": "36-algorithms.html#random-forests",
    "href": "36-algorithms.html#random-forests",
    "title": "36  Examples of algorithms",
    "section": "36.5 Random forests",
    "text": "36.5 Random forests\n\nRandom forests are a very popular machine learning approach that addresses the shortcomings of decision trees using a clever idea.\nThe goal is to improve prediction performance and reduce instability by averaging multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.\nThe first step is bootstrap aggregation or bagging. The general idea is to generate many predictors, each using regression or classification trees, and then forming a final prediction based on the average prediction of all these trees.\nTo assure that the individual trees are not the same, we use the bootstrap to induce randomness.\nThese two features combined explain the name: the bootstrap makes the individual trees randomly different, and the combination of trees is the forest. The specific steps are as follows.\n\n1. Build \\(B\\) decision trees using the training set. We refer to the fitted models as \\(T_1, T_2, \\dots, T_B\\).\n2. For every observation in the test set, form a prediction \\(\\hat{y}_j\\) using tree \\(T_j\\).\n3. For continuous outcomes, form a final prediction with the average \\(\\hat{y} = \\frac{1}{B} \\sum_{j = 1}^B \\hat{y}_j\\). For categorical data classification, predict \\(\\hat{y}\\) with majority vote (most frequent class among \\(\\hat{y}_1, \\dots, \\hat{y}_T\\)).\n\nSo how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below.\nLet \\(N\\) be the number of observations in the training set.\nTo create \\(T_j, \\, j = 1,\\ldots,B\\) from the training set we do the following:\n\n1. Create a bootstrap training set by sampling \\(N\\) observations from the training set with replacement. This is the first way to induce randomness.\n2. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy.\n\nTo illustrate how the first steps can result in smoother estimates we will demonstrate by fitting a random forest to the 2008 polls data.\n\n*We will use the randomForest function in the randomForest package:\n\nlibrary(randomForest)\nfit &lt;- randomForest(margin ~ ., data = polls_2008) \n\n\nNote that if we apply the function plot to the resulting object, stored in fit, we see how the error rate of our algorithm changes as we add trees.\n\n\nrafalib::mypar()\nplot(fit)\n\n\n\n\n\n\n\nWe can see that in this case, the accuracy improves as we add more trees until about 30 trees where accuracy stabilizes.\nThe resulting estimate for this random forest can be seen like this:\n\n\n\n\n\n\n\nNotice that the random forest estimate is much smoother than what we achieved with the regression tree in the previous section.\nThis is possible because the average of many step functions can be smooth.\nWe can see this by visually examining how the estimate changes as we add more trees.\nIn the following figure you see each of the bootstrap samples for several values of \\(b\\) and for each one we see the tree that is fitted in grey, the previous trees that were fitted in lighter grey, and the result of averaging all the trees estimated up to that point.\n\n\n\nlibrary(randomForest)\ntrain_rf &lt;- randomForest(y ~ ., data = mnist_27$train)\n\n\nThe accuracy for the random forest fit for our 2 or 7 example is confusionMatrix(predict(train_rf, mnist_27$test), mnist_27$test$y)$overall[\"Accuracy\".\nHere is what the conditional probabilities look like:\n\n\n\n\n\n\n\nVisualizing the estimate shows that, although we obtain high accuracy, it appears that there is room for improvement by making the estimate smoother.\nThis could be achieved by changing the parameter that controls the minimum number of data points in the nodes of the tree.\nThe larger this minimum, the smoother the final estimate will be.\nIf we use a nodeize of 31, the number of neighbors we used with knn, we get an accuracy of confusionMatrix(predict(train_rf_2, mnist_27$test), mnist_27$test$y)$overall[\"Accuracy\"]. The selected model improves accuracy and provides a smoother estimate:\n\n\n\n\n\n\n\nRandom forest performs better than trees in all the examples we have considered.\nHowever, a disadvantage of random forests is that we lose interpretability.\nAn approach that helps with interpretability is to examine variable importance. To define variable importance we count how often a predictor is used in the individual trees.\nYou can learn more about variable importance in an advanced machine learning book. The caret package includes the function varImp that extracts variable importance from any model in which the calculation is implemented.\nWe give an example on how we use variable importance in the next section."
  },
  {
    "objectID": "36-algorithms.html#footnotes",
    "href": "36-algorithms.html#footnotes",
    "title": "36  Examples of algorithms",
    "section": "",
    "text": "https://web.stanford.edu/~hastie/Papers/ESLII.pdf↩︎\nhttps://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid = 1759289&mirid = 1&type = 2↩︎"
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "BIOL B215: Biostatistics with R",
    "section": "",
    "text": "Instructor: Bárbara D. Bitarello (bbitarello [at] brynmawr.edu)\nTA: Nicole Cavalieri (ncavalieri [at] brynmawr.edu)\nLecture: Monday & Wednesday 11:40 AM (Park 264)\nLab: Wednesday 1:10 - 4 PM\nRemember to read the syllabus, listen to Snoop Dog.\nLecture notes:\nPiazza (for Qs)\nMoodle\nRstudio (Posit) Cloud\nRemember to read the Course Info, listen to SD.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "BIOL B215: Biostatistics with R",
    "section": "Lectures",
    "text": "Lectures\nLecture slides, class notes, and problem sets are linked below. New material is added approximately on a weekly basis.\n\n\n\nDates\nTopic\nSlides\nReading",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#problem-sets",
    "href": "index.html#problem-sets",
    "title": "BIOL B215: Biostatistics with R",
    "section": "Problem sets",
    "text": "Problem sets\n\n\n\nProblem set\nTopic\nDue Date\nDifficulty",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#office-hour-times",
    "href": "index.html#office-hour-times",
    "title": "BIOL B215: Biostatistics with R",
    "section": "Office hour times",
    "text": "Office hour times\n\n\n\nMeeting\nTime\nLocation",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "BIOL B215: Biostatistics with R",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nWe thank Maria Tackett and Mine Çetinkaya-Rundel for sharing their web page template which we used in creating this website, and Rafael Irizarry sharing this template. We also thank Yaniv Brandvain for sharing awesome teaching materials.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Course Info",
    "section": "",
    "text": "BIOL B 260 Biostatistics with R\nLecture: Monday & Wednesday 11:40 AM (Park 264)\nLab: Wednesday 1:10 - 4 PM (Park 128)\nLecture notes:\nPiazza (for Qs): https://piazza.com/brynmawr/fall2025/bmcbiolb215001f25\nMoodle: https://moodle.brynmawr.edu/course/view.php?id=8224\nRstudio (Posit) Cloud: https://posit.cloud/spaces/678147/join?access_code=hl0WMhTu8LNwvW1hn-uIdm_lx1Xm8sCjIUMwCW24",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#general-information",
    "href": "syllabus.html#general-information",
    "title": "Course Info",
    "section": "",
    "text": "BIOL B 260 Biostatistics with R\nLecture: Monday & Wednesday 11:40 AM (Park 264)\nLab: Wednesday 1:10 - 4 PM (Park 128)\nLecture notes:\nPiazza (for Qs): https://piazza.com/brynmawr/fall2025/bmcbiolb215001f25\nMoodle: https://moodle.brynmawr.edu/course/view.php?id=8224\nRstudio (Posit) Cloud: https://posit.cloud/spaces/678147/join?access_code=hl0WMhTu8LNwvW1hn-uIdm_lx1Xm8sCjIUMwCW24",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Course Info",
    "section": "Prerequisites",
    "text": "Prerequisites\nNo prior experience with programming is required. Ideally the student has taken BIOL B110 or BIOL B111.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\n\nIntroduction to Data Science: Data Wrangling and Visualization with R\nIntroduction to Data Science: Statistics and Prediction Algorithms Through Case Studies",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Course Info",
    "section": "Course Description",
    "text": "Course Description\nStatistics came from biology. For 15+ years, a solid foundation in statistics and R has been the standard tool for performing statistics in biology, health studies, data science, and many other fields. Because of this, B215 is neither just a biostatistics course nor simply an R programming course: it is a biostatistics course where R is directly integrated into the learning process. We will focus mainly on classic statistics (so-called frequentist ) and probability theory, but we will also have an introduction to the increasingly relevant Bayesian statistics approach.\nThroughout the course, we use motivating case studies and data analysis problem sets and R labs based on similar challenges to those one finds in scientific research.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#weekly-course-structure",
    "href": "syllabus.html#weekly-course-structure",
    "title": "Course Info",
    "section": "Weekly Course Structure",
    "text": "Weekly Course Structure\n\nMonday and Wednesday lectures: Focus on concepts and (manual) problem solving. In-class activities will occur often and count towards your grade.\nTuesday labs: R computer lab (bring your laptop) with guided tutorial. Activity is due by the end of the day.\n\nPlease ensure that you read the chapters listed in the syllabus before each Monday. The lectures are designed with the assumption that you have completed the readings, enabling us to dive deeper into nuances, applications, and questions.\nLectures will not be recorded.\nWe have Piazza set up for you to ask questions during and after class. Professor and TA will check this often and it is the preferred method to get our help. Also, see office hours schedule.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grade-distribution",
    "href": "syllabus.html#grade-distribution",
    "title": "Course Info",
    "section": "Grade Distribution",
    "text": "Grade Distribution\n\n\n\nComponent\nWeight\n\n\n\n\nHomework (several)\n12%\n\n\nProblem sets (3)\n18 %\n\n\nLab Assignments (3)\n30%\n\n\nMidterm 1\n25%\n\n\nFinal exam\n25%\n\n\n\nFinal grades will likely be assigned as follows (if curving is performed, it could increase but not decrease your grade):\n\n\n\nMin %\nBMC scale\n\n\n\n\n93\n4\n\n\n90\n3.7\n\n\n87\n3.3\n\n\n83\n3.0\n\n\n80\n2.7\n\n\n77\n2.3\n\n\n70\n2.0",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#problem-sets",
    "href": "syllabus.html#problem-sets",
    "title": "Course Info",
    "section": "Problem Sets",
    "text": "Problem Sets\nProblem sets will be due every week or every other week, depending on difficulty. They will be due at 11:59 PM on the day denoted on the Problem Sets page.\nSome problem sets include open ended questions that will be difficult to answer on your own. We will be working on these together during Wednesday labs. We also offer office hours where you can get help with unanswered questions.\nProblem sets must be submitted via GitHub. Students are required to have a GitHub account and create a repository for the course. We will be providing further instructions during the first lab.\n10% of the total points for the problem sets will be deducted for every late day. Students can have a total of 4 late days without penalty during the entire semester. No need to provide a written excuse. Providing an excuse does not give you more days unless an accommodation is requested and approved by the Office of Student Affairs (this includes COVID).\nProblem set submissions need to be completely reproducible Quarto documents. If your Quarto file does not compile it will be considered a late day, and you will be notified and will need to resubmit a Quarto file that does compile. You will be deducted further late days for every day it takes for you to turn in a Quarto file that does knit. You are required to check emails that come through the Canvas system, as this the only way we will communicate problems with your problem sets.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#midterm-policy",
    "href": "syllabus.html#midterm-policy",
    "title": "Course Info",
    "section": "Midterm Policy",
    "text": "Midterm Policy\nBoth midterms are closed book, no internet, and in-class. You are expected to complete them in 1 hour.\nQuestions will be drawn mostly or entirely from the problem sets.\nPlease make sure you can come to class on the midterm dates provided in the Key Dates table below. If you miss the exam, you will need approval from the Office of Student Affairs to receive a makeup. All make-up exams will be completely different from the in-class ones.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#final-project",
    "href": "syllabus.html#final-project",
    "title": "Course Info",
    "section": "Final Project",
    "text": "Final Project\nFor your final project we ask that you turn in a 4-6 page report using data to answer a public health related question. You can chose from one of the following:\n\nBased on state-level data, how effective where vaccines against SARS-CoV-2 reported cases and COVID-19 hospitalizations and deaths, and vaccination rates.\nWhat was the excess mortality after Hurricane María in Puerto Rico? Where different age groups affected differently?\n\nOptionally, you can select a question that align with your ongoing research. This way, it can be directly beneficial to your work. This will require prior approval from the instructor by October 25.\nYet another option is to build a interactive webpage with poll-driven predictions for the 2024 US elections. Note this will be more challenging as we will not cover tools for interactive webpages until the last week of class (time permitting).\nNote: You should start working on your project after the first midterm. Do not wait until the last week. Teaching staff will be available during office hours.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#chatgpt-policy",
    "href": "syllabus.html#chatgpt-policy",
    "title": "Course Info",
    "section": "ChatGPT Policy",
    "text": "ChatGPT Policy\nYou can use ChatGPT however you want. Do remember you won’t be able to use it during the midterms.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#key-dates",
    "href": "syllabus.html#key-dates",
    "title": "Course Info",
    "section": "Key Dates",
    "text": "Key Dates\n\n\n\n\n\n\n\nDate\nEvent\n\n\n\n\nSep 10\nPset 1 due\n\n\nSep 13\nPset 2 due\n\n\nOct 14\nNo class: Indigenous Peoples’ Day\n\n\nOct 16\nMidterm 1: covers material from Sep 04-Oct 11\n\n\nOct 23\nStart final project. Obtain approval if you want to do a personal project instead.\n\n\nNov 11\nNo class: Veterans’ Day\n\n\nNov 25\nMidterm 2: cover material from Sep 04-Nov 22\n\n\nNov 27\nNo class: Thanksgiving Recess Begins\n\n\nDec 20\nFinal Project due",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbook",
    "href": "syllabus.html#textbook",
    "title": "Course Info",
    "section": "Textbook",
    "text": "Textbook\n\nThe Analysis of Biological Data (3rd or 2nd edition), by Whitlock & Schluter.\nPlease make sure you also bring a laptop to labs. You may borrow one from the bio department. Please email if this is your case.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#topics-we-aim-to-cover",
    "href": "syllabus.html#topics-we-aim-to-cover",
    "title": "Course Info",
    "section": "Topics we (aim to) cover",
    "text": "Topics we (aim to) cover\n\nSampling & Experimental Design\nDescriptive statistics\nData visualization\nProbability Theory\nEstimation & Hypothesis Testing\nThe Normal Distribution\nComparing samples & Correlations\nBootstrapping and permutation\nBayesian vs Likelihood approaches\nBootstrapping & Permutation Tests Likelihood & Bayesian Statistics",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#lab-assignments",
    "href": "index.html#lab-assignments",
    "title": "BIOL B215: Biostatistics with R",
    "section": "Lab Assignments",
    "text": "Lab Assignments\n\n\n\nProblem set\nTopic\nDue Date\nDifficulty",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "psets.html",
    "href": "psets.html",
    "title": "Problem Sets",
    "section": "",
    "text": "Topic\n\n\nDue date (at 11:59 PM)\n\n\n\n\n\n\nProblem set 1\n\n\nWed, Sep 11\n\n\n\n\nProblem set 2\n\n\nThu, Sep 19\n\n\n\n\nProblem set 3\n\n\nFri, Sep 27\n\n\n\n\nProblem set 4\n\n\nFri, Oct 04\n\n\n\n\nProblem set 5\n\n\nFri, Oct 11\n\n\n\n\nProblem set 6\n\n\nFri, Oct 25\n\n\n\n\nProblem set 7\n\n\nMon, Nov 04\n\n\n\n\nProblem set 8\n\n\nFri, Nov 15\n\n\n\n\nProblem set 9\n\n\nFri, Nov 22\n\n\n\n\nProblem set 10\n\n\nMon, Dec 16\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Problem Sets"
    ]
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Topic\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nFirst day\n\n\nIntroduction\n\n\nWed, Sep 04\n\n\n\n\nProductivity Tools\n\n\nUnix\n\n\nWed, Sep 04\n\n\n\n\nProductivity Tools\n\n\nRStudio\n\n\nWed, Sep 04\n\n\n\n\nProductivity Tools\n\n\nQuarto\n\n\nMon, Sep 09\n\n\n\n\nProductivity Tools\n\n\nGit and GitHub\n\n\nMon, Sep 09\n\n\n\n\nR\n\n\nR Basics\n\n\nMon, Sep 16\n\n\n\n\nR\n\n\nVectorization\n\n\nWed, Sep 18\n\n\n\n\nR\n\n\nTidyverse\n\n\nMon, Sep 23\n\n\n\n\nR\n\n\nggplot2\n\n\nMon, Sep 23\n\n\n\n\nR\n\n\nTidying data\n\n\nWed, Sep 25\n\n\n\n\nWrangling\n\n\nIntroduction to Wrangling\n\n\nMon, Sep 30\n\n\n\n\nWrangling\n\n\nImporting files\n\n\nMon, Sep 30\n\n\n\n\nWrangling\n\n\nDates And Times\n\n\nMon, Sep 30\n\n\n\n\nWrangling\n\n\nLocales\n\n\nMon, Sep 30\n\n\n\n\nWrangling\n\n\nData APIs\n\n\nMon, Sep 30\n\n\n\n\nWrangling\n\n\nWeb Scraping\n\n\nMon, Sep 30\n\n\n\n\nWrangling\n\n\nJoining Tables\n\n\nMon, Sep 30\n\n\n\n\nData Visualization\n\n\nData Visualization Principles\n\n\nMon, Oct 07\n\n\n\n\nData Visualization\n\n\nDistributions\n\n\nWed, Oct 09\n\n\n\n\nData Visualization\n\n\nDataviz In Practice\n\n\nWed, Oct 09\n\n\n\n\nProbability\n\n\nIntroduction to Probability\n\n\nMon, Oct 21\n\n\n\n\nProbability\n\n\nFoundations of Statistical Inference\n\n\nMon, Oct 21\n\n\n\n\nInference\n\n\nIntroduction to Statistical Inference and Models\n\n\nWed, Oct 23\n\n\n\n\nInference\n\n\nParameters and Estimates\n\n\nWed, Oct 23\n\n\n\n\nInference\n\n\nConfidence Intervals\n\n\nWed, Oct 23\n\n\n\n\nInference\n\n\nData-driven models\n\n\nMon, Oct 28\n\n\n\n\nInference\n\n\nBayesian Models\n\n\nWed, Oct 30\n\n\n\n\nInference\n\n\nHierarchical Models\n\n\nWed, Oct 30\n\n\n\n\nLinear Models\n\n\nIntroduction\n\n\nTue, Nov 05\n\n\n\n\nLinear Models\n\n\nRegression\n\n\nTue, Nov 05\n\n\n\n\nLinear Models\n\n\nMultivariate Regression\n\n\nTue, Nov 12\n\n\n\n\nLinear Models\n\n\nTreatment Effect Models\n\n\nTue, Nov 12\n\n\n\n\nHigh dimensional data\n\n\nIntroduction to Linear Algebra\n\n\nTue, Nov 12\n\n\n\n\nHigh dimensional data\n\n\nMatrices In R\n\n\nTue, Nov 12\n\n\n\n\nHigh dimensional data\n\n\nDistance\n\n\nMon, Dec 02\n\n\n\n\nHigh dimensional data\n\n\nDimension Reduction\n\n\nMon, Dec 02\n\n\n\n\nMachine Learning\n\n\nIntroduction\n\n\nMon, Dec 02\n\n\n\n\nMachine Learning\n\n\nEvaluation Metrics\n\n\nWed, Dec 04\n\n\n\n\nMachine Learning\n\n\nConditionals\n\n\nWed, Dec 04\n\n\n\n\nMachine Learning\n\n\nSmoothing\n\n\nWed, Dec 04\n\n\n\n\nMachine Learning\n\n\nk-nearest neighbors (knn)\n\n\nTue, Dec 10\n\n\n\n\nMachine Learning\n\n\nResampling Methods\n\n\nMon, Dec 09\n\n\n\n\nMachine Learning\n\n\nThe caret package\n\n\nMon, Dec 09\n\n\n\n\nMachine Learning\n\n\nAlgorithms\n\n\nWed, Dec 11\n\n\n\n\nMachine Learning\n\n\nML In Practice\n\n\nMon, Dec 09\n\n\n\n\nOther topics\n\n\nAssociation Not Causation\n\n\nMon, Dec 16\n\n\n\n\nOther topics\n\n\nIntroduction to Shiny\n\n\nMon, Dec 16\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Slides"
    ]
  }
]